* Installation and Verification
** Logging in

#+begin_src bash
ssh -l lab-user bastion.munich-e7ab.sandbox1596.opentlc.com -o ServerAliveInterval=120
#+end_src

r3dh4t1!

#+begin_src bash
export GUID=`hostname | cut -d. -f2`
#+end_src

** Login to OpenShift

#+begin_src bash
ls -al ~/cluster-$GUID
#+end_src

#+begin_example
# total 2824
# drwxrwxr-x.  4 ec2-user ec2-user     209 Feb 19 18:02 ./
# drwx------. 15 lab-user users       4096 Feb 20 16:01 ../
# drwxr-xr-x.  2 ec2-user ec2-user      50 Feb 19 17:44 auth/
# -rw-r--r--.  1 ec2-user ec2-user     292 Feb 19 17:44 metadata.json
# -rw-r--r--.  1 ec2-user ec2-user  129762 Feb 19 18:08 .openshift_install.log
# -rw-r--r--.  1 ec2-user ec2-user 2294231 Feb 19 17:50 .openshift_install_state.json
# -rw-r--r--.  1 ec2-user ec2-user     772 Feb 19 17:44 terraform.aws.auto.tfvars.json
# -rw-r--r--.  1 ec2-user ec2-user  130217 Feb 19 18:02 terraform.tfstate
# -rw-r--r--.  1 ec2-user ec2-user  315708 Feb 19 17:44 terraform.tfvars.json
# drwxr-xr-x.  2 ec2-user ec2-user      62 Feb 19 17:44 tls/
#+end_example

#+begin_src bash
tail -n5 ~/cluster-$GUID/.openshift_install.log
#+end_src

#+begin_example
# time="2020-02-19T18:08:42Z" level=debug msg="OpenShift console route is created"
# time="2020-02-19T18:08:42Z" level=info msg="Install complete!"
# time="2020-02-19T18:08:42Z" level=info msg="To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/ec2-user/cluster-munich-e7ab/auth/kubeconfig'"
# time="2020-02-19T18:08:42Z" level=info msg="Access the OpenShift web-console here: https://console-openshift-console.apps.cluster-munich-e7ab.sandbox1596.opentlc.com"
# time="2020-02-19T18:08:42Z" level=info msg="Login to the console with user: kubeadmin, password: RmEAm-FN5EH-XhpAH-YYbf8"
#+end_example

#+begin_src bash
oc whoami
#+end_src

#+begin_example
# kube:admin
#+end_example

** Examine the Cluster Version

#+begin_src bash
oc get clusterversion
#+end_src

#+begin_example
# NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
# version   4.2.7     True        False         45h     Cluster version is 4.2.7
#+end_example

#+begin_src bash
oc describe clusterversion
#+end_src

#+begin_example
# Name:         version
# Namespace:    
# Labels:       <none>
# Annotations:  <none>
# API Version:  config.openshift.io/v1
# Kind:         ClusterVersion
# Metadata:
#   Creation Timestamp:  2020-02-19T17:56:01Z
#   Generation:          1
#   Resource Version:    918522
#   Self Link:           /apis/config.openshift.io/v1/clusterversions/version
#   UID:                 170b699c-5341-11ea-8707-027cbd288fd4
# Spec:
#   Channel:     stable-4.2
#   Cluster ID:  52701483-cceb-4ad4-8560-67c2e725de2a
#   Upstream:    https://api.openshift.com/api/upgrades_info/v1/graph
# Status:
#   Available Updates:
#     Force:    false
#     Image:    quay.io/openshift-release-dev/ocp-release@sha256:4bf307b98beba4d42da3316464013eac120c6e5a398646863ef92b0e2c621230
#     Version:  4.2.8
#     Force:    false
#     Image:    quay.io/openshift-release-dev/ocp-release@sha256:782b41750f3284f3c8ee2c1f8cb896896da074e362cf8a472846356d1617752d
#     Version:  4.2.13
#     Force:    false
#     Image:    quay.io/openshift-release-dev/ocp-release@sha256:e5a6e348721c38a78d9299284fbb5c60fb340135a86b674b038500bf190ad514
#     Version:  4.2.16
#     Force:    false
#     Image:    quay.io/openshift-release-dev/ocp-release@sha256:dc2e38fb00085d6b7f722475f8b7b758a0cb3a02ba42d9acf8a8298a6d510d9c
#     Version:  4.2.10
#     Force:    false
#     Image:    quay.io/openshift-release-dev/ocp-release@sha256:77ade34c373062c6a6c869e0e56ef93b2faaa373adadaac1430b29484a24d843
#     Version:  4.2.12
#     Force:    false
#     Image:    quay.io/openshift-release-dev/ocp-release@sha256:3fabe939da31f9a31f509251b9f73d321e367aba2d09ff392c2f452f6433a95a
#     Version:  4.2.14
#     Force:    false
#     Image:    quay.io/openshift-release-dev/ocp-release@sha256:f28cbabd1227352fe704a00df796a4511880174042dece96233036a10ac61639
#     Version:  4.2.9
#   Conditions:
#     Last Transition Time:  2020-02-19T18:08:42Z
#     Message:               Done applying 4.2.7
#     Status:                True
#     Type:                  Available
#     Last Transition Time:  2020-02-20T17:48:12Z
#     Status:                False
#     Type:                  Failing
#     Last Transition Time:  2020-02-19T18:08:42Z
#     Message:               Cluster version is 4.2.7
#     Status:                False
#     Type:                  Progressing
#     Last Transition Time:  2020-02-19T17:56:14Z
#     Status:                True
#     Type:                  RetrievedUpdates
#   Desired:
#     Force:    false
#     Image:    quay.io/openshift-release-dev/ocp-release@sha256:bac62983757570b9b8f8bc84c740782984a255c16372b3e30cfc8b52c0a187b9
#     Version:  4.2.7
#   History:
#     Completion Time:    2020-02-19T18:08:42Z
#     Image:              quay.io/openshift-release-dev/ocp-release@sha256:bac62983757570b9b8f8bc84c740782984a255c16372b3e30cfc8b52c0a187b9
#     Started Time:       2020-02-19T17:56:14Z
#     State:              Completed
#     Verified:           false
#     Version:            4.2.7
#   Observed Generation:  1
#   Version Hash:         SmE5nyYNooU=
# Events:                 <none>
#+end_example

** Look at the Nodes

#+begin_src bash
oc get nodes
#+end_src

#+begin_example
# NAME                                         STATUS   ROLES          AGE     VERSION
# ip-10-0-131-190.us-east-2.compute.internal   Ready    worker         7h29m   v1.14.6+9fb2d5cf9
# ip-10-0-131-54.us-east-2.compute.internal    Ready    infra,worker   25h     v1.14.6+9fb2d5cf9
# ip-10-0-131-55.us-east-2.compute.internal    Ready    master         46h     v1.14.6+9fb2d5cf9
# ip-10-0-133-255.us-east-2.compute.internal   Ready    worker         45h     v1.14.6+9fb2d5cf9
# ip-10-0-137-100.us-east-2.compute.internal   Ready    infra,worker   25h     v1.14.6+9fb2d5cf9
# ip-10-0-137-236.us-east-2.compute.internal   Ready    infra,worker   25h     v1.14.6+9fb2d5cf9
# ip-10-0-143-103.us-east-2.compute.internal   Ready    worker         4h37m   v1.14.6+9fb2d5cf9
# ip-10-0-147-203.us-east-2.compute.internal   Ready    worker         7h29m   v1.14.6+9fb2d5cf9
# ip-10-0-150-228.us-east-2.compute.internal   Ready    master         46h     v1.14.6+9fb2d5cf9
# ip-10-0-153-85.us-east-2.compute.internal    Ready    worker         4h36m   v1.14.6+9fb2d5cf9
# ip-10-0-156-47.us-east-2.compute.internal    Ready    worker         45h     v1.14.6+9fb2d5cf9
# ip-10-0-163-36.us-east-2.compute.internal    Ready    worker         4h36m   v1.14.6+9fb2d5cf9
# ip-10-0-172-64.us-east-2.compute.internal    Ready    worker         28h     v1.14.6+9fb2d5cf9
# ip-10-0-173-76.us-east-2.compute.internal    Ready    master         46h     v1.14.6+9fb2d5cf9
# ip-10-0-174-204.us-east-2.compute.internal   Ready    worker         7h30m   v1.14.6+9fb2d5cf9
#+end_example

** Check the Web Console

https://console-openshift-console.apps.cluster-munich-e7ab.sandbox1596.opentlc.com

** You will now exit the ssh session

#+begin_src bash
exit
#+end_src

* Application Management Basics
** Projects

#+begin_src bash
oc new-project app-management
#+end_src

#+begin_example
# Now using project "app-management" on server "https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443".
# 
# You can add applications to this project with the 'new-app' command. For example, try:
# 
#     oc new-app django-psql-example
# 
# to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:
# 
#     kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
# 
#+end_example

** Deploy a Sample Application

#+begin_src bash
oc new-app quay.io/thoraxe/mapit
#+end_src

#+begin_example
# --> Found container image 7ce7ade (2 years old) from quay.io for "quay.io/thoraxe/mapit"
# 
#     * An image stream tag will be created as "mapit:latest" that will track this image
#     * This image will be deployed in deployment config "mapit"
#     * Ports 8080/tcp, 8778/tcp, 9779/tcp will be load balanced by service "mapit"
#       * Other containers can access this service through the hostname "mapit"
# 
# --> Creating resources ...
#     imagestream.image.openshift.io "mapit" created
#     deploymentconfig.apps.openshift.io "mapit" created
#     service "mapit" created
# --> Success
#     Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
#      'oc expose svc/mapit' 
#     Run 'oc status' to view your app.
#+end_example

** Pods

#+begin_src bash
oc get pods
#+end_src

#+begin_example
# NAME             READY   STATUS              RESTARTS   AGE
# mapit-1-deploy   0/1     ContainerCreating   0          3s
#+end_example

#+begin_src bash
oc get pods
#+end_src

#+begin_example
# NAME             READY   STATUS              RESTARTS   AGE
# mapit-1-deploy   1/1     Running             0          26s
# mapit-1-dt5k7    0/1     ContainerCreating   0          18s
#+end_example

#+begin_src bash
oc describe pod $(oc get pods | grep "Running" | awk '{ print $1 }')
#+end_src

#+begin_example
# Name:               mapit-1-dt5k7
# Namespace:          app-management
# Priority:           0
# PriorityClassName:  <none>
# Node:               ip-10-0-172-64.us-east-2.compute.internal/10.0.172.64
# Start Time:         Fri, 21 Feb 2020 16:10:57 +0000
# Labels:             app=mapit
#                     deployment=mapit-1
#                     deploymentconfig=mapit
# Annotations:        k8s.v1.cni.cncf.io/networks-status:
#                       [{
#                           "name": "openshift-sdn",
#                           "interface": "eth0",
#                           "ips": [
#                               "10.130.2.13"
#                           ],
#                           "default": true,
#                           "dns": {}
#                       }]
#                     kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container mapit; cpu, memory limit for container mapit
#                     openshift.io/deployment-config.latest-version: 1
#                     openshift.io/deployment-config.name: mapit
#                     openshift.io/deployment.name: mapit-1
#                     openshift.io/generated-by: OpenShiftNewApp
#                     openshift.io/scc: restricted
# Status:             Running
# IP:                 10.130.2.13
# Controlled By:      ReplicationController/mapit-1
# Containers:
#   mapit:
#     Container ID:   cri-o://5c6adb041ba82010938d70def3b37f6bbb25db27c87108b675698ae6db9bc553
#     Image:          quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
#     Image ID:       quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
#     Ports:          8080/TCP, 8778/TCP, 9779/TCP
#     Host Ports:     0/TCP, 0/TCP, 0/TCP
#     State:          Running
#       Started:      Fri, 21 Feb 2020 16:11:14 +0000
#     Ready:          True
#     Restart Count:  0
#     Limits:
#       cpu:     4
#       memory:  1Gi
#     Requests:
#       cpu:        100m
#       memory:     512Mi
#     Environment:  <none>
#     Mounts:
#       /var/run/secrets/kubernetes.io/serviceaccount from default-token-w6rh2 (ro)
# Conditions:
#   Type              Status
#   Initialized       True 
#   Ready             True 
#   ContainersReady   True 
#   PodScheduled      True 
# Volumes:
#   default-token-w6rh2:
#     Type:        Secret (a volume populated by a Secret)
#     SecretName:  default-token-w6rh2
#     Optional:    false
# QoS Class:       Burstable
# Node-Selectors:  <none>
# Tolerations:     node.kubernetes.io/memory-pressure:NoSchedule
#                  node.kubernetes.io/not-ready:NoExecute for 300s
#                  node.kubernetes.io/unreachable:NoExecute for 300s
# Events:
#   Type    Reason     Age    From                                                Message
#   ----    ------     ----   ----                                                -------
#   Normal  Scheduled  4m5s   default-scheduler                                   Successfully assigned app-management/mapit-1-dt5k7 to ip-10-0-172-64.us-east-2.compute.internal
#   Normal  Pulling    3m58s  kubelet, ip-10-0-172-64.us-east-2.compute.internal  Pulling image "quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d"
#   Normal  Pulled     3m48s  kubelet, ip-10-0-172-64.us-east-2.compute.internal  Successfully pulled image "quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d"
#   Normal  Created    3m48s  kubelet, ip-10-0-172-64.us-east-2.compute.internal  Created container mapit
#   Normal  Started    3m48s  kubelet, ip-10-0-172-64.us-east-2.compute.internal  Started container mapit
#+end_example

** Services

#+begin_src bash
oc get services
#+end_src

#+begin_example
# NAME    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
# mapit   ClusterIP   172.30.67.133   <none>        8080/TCP,8778/TCP,9779/TCP   4m48s
#+end_example

#+begin_src bash
oc describe service mapit
#+end_src

#+begin_example
# Name:              mapit
# Namespace:         app-management
# Labels:            app=mapit
# Annotations:       openshift.io/generated-by: OpenShiftNewApp
# Selector:          app=mapit,deploymentconfig=mapit
# Type:              ClusterIP
# IP:                172.30.67.133
# Port:              8080-tcp  8080/TCP
# TargetPort:        8080/TCP
# Endpoints:         10.130.2.13:8080
# Port:              8778-tcp  8778/TCP
# TargetPort:        8778/TCP
# Endpoints:         10.130.2.13:8778
# Port:              9779-tcp  9779/TCP
# TargetPort:        9779/TCP
# Endpoints:         10.130.2.13:9779
# Session Affinity:  None
# Events:            <none>
#+end_example

#+begin_src bash
oc get service mapit -o yaml
#+end_src

#+begin_example
# apiVersion: v1
# kind: Service
# metadata:
#   annotations:
#     openshift.io/generated-by: OpenShiftNewApp
#   creationTimestamp: "2020-02-21T16:10:46Z"
#   labels:
#     app: mapit
#   name: mapit
#   namespace: app-management
#   resourceVersion: "928455"
#   selfLink: /api/v1/namespaces/app-management/services/mapit
#   uid: b7acba15-54c4-11ea-9d6d-06399a073c9e
# spec:
#   clusterIP: 172.30.67.133
#   ports:
#   - name: 8080-tcp
#     port: 8080
#     protocol: TCP
#     targetPort: 8080
#   - name: 8778-tcp
#     port: 8778
#     protocol: TCP
#     targetPort: 8778
#   - name: 9779-tcp
#     port: 9779
#     protocol: TCP
#     targetPort: 9779
#   selector:
#     app: mapit
#     deploymentconfig: mapit
#   sessionAffinity: None
#   type: ClusterIP
# status:
#   loadBalancer: {}
#+end_example

#+begin_src bash
oc get pod $(oc get pods | grep "Running" | awk '{ print $1 }') -o yaml
#+end_src

#+begin_example
# apiVersion: v1
# kind: Pod
# metadata:
#   annotations:
#     k8s.v1.cni.cncf.io/networks-status: |-
#       [{
#           "name": "openshift-sdn",
#           "interface": "eth0",
#           "ips": [
#               "10.130.2.13"
#           ],
#           "default": true,
#           "dns": {}
#       }]
#     kubernetes.io/limit-ranger: 'LimitRanger plugin set: cpu, memory request for container
#       mapit; cpu, memory limit for container mapit'
#     openshift.io/deployment-config.latest-version: "1"
#     openshift.io/deployment-config.name: mapit
#     openshift.io/deployment.name: mapit-1
#     openshift.io/generated-by: OpenShiftNewApp
#     openshift.io/scc: restricted
#   creationTimestamp: "2020-02-21T16:10:57Z"
#   generateName: mapit-1-
#   labels:
#     app: mapit
#     deployment: mapit-1
#     deploymentconfig: mapit
#   name: mapit-1-dt5k7
#   namespace: app-management
#   ownerReferences:
#   - apiVersion: v1
#     blockOwnerDeletion: true
#     controller: true
#     kind: ReplicationController
#     name: mapit-1
#     uid: b96b188f-54c4-11ea-8e4e-029a3b7b53fa
#   resourceVersion: "928735"
#   selfLink: /api/v1/namespaces/app-management/pods/mapit-1-dt5k7
#   uid: be5766ef-54c4-11ea-9d6d-06399a073c9e
# spec:
#   containers:
#   - image: quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
#     imagePullPolicy: Always
#     name: mapit
#     ports:
#     - containerPort: 8080
#       protocol: TCP
#     - containerPort: 8778
#       protocol: TCP
#     - containerPort: 9779
#       protocol: TCP
#     resources:
#       limits:
#         cpu: "4"
#         memory: 1Gi
#       requests:
#         cpu: 100m
#         memory: 512Mi
#     securityContext:
#       capabilities:
#         drop:
#         - KILL
#         - MKNOD
#         - SETGID
#         - SETUID
#       runAsUser: 1000890000
#     terminationMessagePath: /dev/termination-log
#     terminationMessagePolicy: File
#     volumeMounts:
#     - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
#       name: default-token-w6rh2
#       readOnly: true
#   dnsPolicy: ClusterFirst
#   enableServiceLinks: true
#   imagePullSecrets:
#   - name: default-dockercfg-sgc84
#   nodeName: ip-10-0-172-64.us-east-2.compute.internal
#   priority: 0
#   restartPolicy: Always
#   schedulerName: default-scheduler
#   securityContext:
#     fsGroup: 1000890000
#     seLinuxOptions:
#       level: s0:c30,c10
#   serviceAccount: default
#   serviceAccountName: default
#   terminationGracePeriodSeconds: 30
#   tolerations:
#   - effect: NoSchedule
#     key: node.kubernetes.io/memory-pressure
#     operator: Exists
#   - effect: NoExecute
#     key: node.kubernetes.io/not-ready
#     operator: Exists
#     tolerationSeconds: 300
#   - effect: NoExecute
#     key: node.kubernetes.io/unreachable
#     operator: Exists
#     tolerationSeconds: 300
#   volumes:
#   - name: default-token-w6rh2
#     secret:
#       defaultMode: 420
#       secretName: default-token-w6rh2
# status:
#   conditions:
#   - lastProbeTime: null
#     lastTransitionTime: "2020-02-21T16:10:57Z"
#     status: "True"
#     type: Initialized
#   - lastProbeTime: null
#     lastTransitionTime: "2020-02-21T16:11:15Z"
#     status: "True"
#     type: Ready
#   - lastProbeTime: null
#     lastTransitionTime: "2020-02-21T16:11:15Z"
#     status: "True"
#     type: ContainersReady
#   - lastProbeTime: null
#     lastTransitionTime: "2020-02-21T16:10:57Z"
#     status: "True"
#     type: PodScheduled
#   containerStatuses:
#   - containerID: cri-o://5c6adb041ba82010938d70def3b37f6bbb25db27c87108b675698ae6db9bc553
#     image: quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
#     imageID: quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
#     lastState: {}
#     name: mapit
#     ready: true
#     restartCount: 0
#     state:
#       running:
#         startedAt: "2020-02-21T16:11:14Z"
#   hostIP: 10.0.172.64
#   phase: Running
#   podIP: 10.130.2.13
#   qosClass: Burstable
#   startTime: "2020-02-21T16:10:57Z"
#+end_example

** Exploring Deployment-related Objects

#+begin_src bash
oc get dc
#+end_src

#+begin_example
# NAME    REVISION   DESIRED   CURRENT   TRIGGERED BY
# mapit   1          1         1         config,image(mapit:latest)
#+end_example

#+begin_src bash
oc get rc
#+end_src

#+begin_example
# NAME      DESIRED   CURRENT   READY   AGE
# mapit-1   1         1         1       7m57s
#+end_example

** Scaling the Application

#+begin_src bash
oc scale --replicas=2 dc/mapit
#+end_src

#+begin_example
# deploymentconfig.apps.openshift.io/mapit scaled
#+end_example

#+begin_src bash
oc get rc
#+end_src

#+begin_example
# NAME      DESIRED   CURRENT   READY   AGE
# mapit-1   2         2         2       8m19s
#+end_example

#+begin_src bash
oc get pods
#+end_src

#+begin_example
# NAME             READY   STATUS      RESTARTS   AGE
# mapit-1-deploy   0/1     Completed   0          8m39s
# mapit-1-dt5k7    1/1     Running     0          8m31s
# mapit-1-zwxg6    1/1     Running     0          40s
#+end_example

#+begin_src bash
oc describe svc mapit
#+end_src

#+begin_example
# Name:              mapit
# Namespace:         app-management
# Labels:            app=mapit
# Annotations:       openshift.io/generated-by: OpenShiftNewApp
# Selector:          app=mapit,deploymentconfig=mapit
# Type:              ClusterIP
# IP:                172.30.67.133
# Port:              8080-tcp  8080/TCP
# TargetPort:        8080/TCP
# Endpoints:         10.129.6.18:8080,10.130.2.13:8080
# Port:              8778-tcp  8778/TCP
# TargetPort:        8778/TCP
# Endpoints:         10.129.6.18:8778,10.130.2.13:8778
# Port:              9779-tcp  9779/TCP
# TargetPort:        9779/TCP
# Endpoints:         10.129.6.18:9779,10.130.2.13:9779
# Session Affinity:  None
# Events:            <none>
#+end_example

#+begin_src bash
oc get endpoints mapit
#+end_src

#+begin_example
# NAME    ENDPOINTS                                                        AGE
# mapit   10.129.6.18:8080,10.130.2.13:8080,10.129.6.18:9779 + 3 more...   9m2s
#+end_example

** Application "Self Healing"

#+begin_src bash
oc get pods
#+end_src

#+begin_example
# NAME             READY   STATUS      RESTARTS   AGE
# mapit-1-deploy   0/1     Completed   0          10m
# mapit-1-dt5k7    1/1     Running     0          10m
# mapit-1-zwxg6    1/1     Running     0          2m34s
#+end_example

#+begin_src bash
oc delete pod mapit-1-zwxg6
#+end_src

#+begin_example
# pod "mapit-1-zwxg6" deleted
#+end_example

#+begin_src bash
oc get pods
#+end_src

#+begin_example
# NAME             READY   STATUS      RESTARTS   AGE
# mapit-1-deploy   0/1     Completed   0          11m
# mapit-1-dt5k7    1/1     Running     0          11m
# mapit-1-wcsfm    1/1     Running     0          17s
#+end_example

** Background: Routes

#+begin_src bash
oc describe deployment router-default -n openshift-ingress
#+end_src

#+begin_example
# Name:                   router-default
# Namespace:              openshift-ingress
# CreationTimestamp:      Wed, 19 Feb 2020 18:02:18 +0000
# Labels:                 ingresscontroller.operator.openshift.io/owning-ingresscontroller=default
# Annotations:            deployment.kubernetes.io/revision: 2
# Selector:               ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
# Replicas:               2 desired | 2 updated | 2 total | 2 available | 0 unavailable
# StrategyType:           RollingUpdate
# MinReadySeconds:        0
# RollingUpdateStrategy:  25% max unavailable, 0 max surge
# Pod Template:
#   Labels:           ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
#   Service Account:  router
#   Containers:
#    router:
#     Image:       quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:7476ec12a318b52bd7a8f83eb9fec6f2fdd87e3f594562aeea2c29d5af536690
#     Ports:       80/TCP, 443/TCP, 1936/TCP
#     Host Ports:  0/TCP, 0/TCP, 0/TCP
#     Requests:
#       cpu:      100m
#       memory:   256Mi
#     Liveness:   http-get http://:1936/healthz delay=10s timeout=1s period=10s #success=1 #failure=3
#     Readiness:  http-get http://:1936/healthz/ready delay=10s timeout=1s period=10s #success=1 #failure=3
#     Environment:
#       STATS_PORT:                    1936
#       ROUTER_SERVICE_NAMESPACE:      openshift-ingress
#       DEFAULT_CERTIFICATE_DIR:       /etc/pki/tls/private
#       ROUTER_SERVICE_NAME:           default
#       STATS_USERNAME:                <set to the key 'statsUsername' in secret 'router-stats-default'>  Optional: false
#       STATS_PASSWORD:                <set to the key 'statsPassword' in secret 'router-stats-default'>  Optional: false
#       ROUTER_METRICS_TYPE:           haproxy
#       ROUTER_METRICS_TLS_CERT_FILE:  /etc/pki/tls/metrics-certs/tls.crt
#       ROUTER_METRICS_TLS_KEY_FILE:   /etc/pki/tls/metrics-certs/tls.key
#       ROUTER_CANONICAL_HOSTNAME:     apps.cluster-munich-e7ab.sandbox1596.opentlc.com
#       ROUTER_USE_PROXY_PROTOCOL:     true
#       ROUTER_THREADS:                4
#     Mounts:
#       /etc/pki/tls/metrics-certs from metrics-certs (ro)
#       /etc/pki/tls/private from default-certificate (ro)
#   Volumes:
#    default-certificate:
#     Type:        Secret (a volume populated by a Secret)
#     SecretName:  router-certs-default
#     Optional:    false
#    metrics-certs:
#     Type:        Secret (a volume populated by a Secret)
#     SecretName:  router-metrics-certs-default
#     Optional:    false
# Conditions:
#   Type           Status  Reason
#   ----           ------  ------
#   Available      True    MinimumReplicasAvailable
#   Progressing    True    NewReplicaSetAvailable
# OldReplicaSets:  router-default-5d5f9f8649 (2/2 replicas created)
# NewReplicaSet:   <none>
# Events:          <none>
#+end_example

** Creating a Route

#+begin_src bash
oc expose service mapit
#+end_src

#+begin_example
# route.route.openshift.io/mapit exposed
#+end_example

#+begin_src bash
oc get route
#+end_src

#+begin_example
# NAME    HOST/PORT                                                               PATH   SERVICES   PORT       TERMINATION   WILDCARD
# mapit   mapit-app-management.apps.cluster-munich-e7ab.sandbox1596.opentlc.com          mapit      8080-tcp                 None
#+end_example

** Scale Down

#+begin_src bash
oc scale --replicas=1 dc/mapit
#+end_src

#+begin_example
# deploymentconfig.apps.openshift.io/mapit scaled
#+end_example

** Add Probes to the Application

#+begin_src bash
curl mapit-app-management.apps.cluster-munich-e7ab.sandbox1596.opentlc.com/health
#+end_src

#+begin_example
# 
# 
#+end_example

#+begin_src bash
oc set probe dc/mapit --liveness --get-url=http://:8080/health --initial-delay-seconds=30
#+end_src

#+begin_example
# deploymentconfig.apps.openshift.io/mapit probes updated
#+end_example

#+begin_src bash
oc describe dc mapit
#+end_src

#+begin_example
# Name:		mapit
# Namespace:	app-management
# Created:	12 minutes ago
# Labels:		app=mapit
# Annotations:	openshift.io/generated-by=OpenShiftNewApp
# Latest Version:	2
# Selector:	app=mapit,deploymentconfig=mapit
# Replicas:	1
# Triggers:	Config, Image(mapit@latest, auto=true)
# Strategy:	Rolling
# Template:
# Pod Template:
#   Labels:	app=mapit
# 		deploymentconfig=mapit
#   Annotations:	openshift.io/generated-by: OpenShiftNewApp
#   Containers:
#    mapit:
#     Image:		quay.io/thoraxe/mapit@sha256:8c7e0349b6a016e3436416f3c54debda4594ba09fd34b8a0dee0c4497102590d
#     Ports:		8080/TCP, 8778/TCP, 9779/TCP
#     Host Ports:		0/TCP, 0/TCP, 0/TCP
#     Liveness:		http-get http://:8080/health delay=30s timeout=1s period=10s #success=1 #failure=3
#     Environment:	<none>
#     Mounts:		<none>
#   Volumes:		<none>
# 
# Deployment #2 (latest):
# 	Created:	3 seconds ago
# 	Status:		Pending
# 	Replicas:	0 current / 0 desired
# Deployment #1:
# 	Name:		mapit-1
# 	Created:	12 minutes ago
# 	Status:		Complete
# 	Replicas:	1 current / 1 desired
# 	Selector:	app=mapit,deployment=mapit-1,deploymentconfig=mapit
# 	Labels:		app=mapit,openshift.io/deployment-config.name=mapit
# 	Pods Status:	1 Running / 0 Waiting / 0 Succeeded / 0 Failed
# 
# Events:
#   Type		Reason				Age	From				Message
#   ----		------				----	----				-------
#   Normal	DeploymentCreated		12m	deploymentconfig-controller	Created new replication controller "mapit-1" for version 1
#   Normal	ReplicationControllerScaled	4m4s	deploymentconfig-controller	Scaled replication controller "mapit-1" from 1 to 2
#   Normal	ReplicationControllerScaled	16s	deploymentconfig-controller	Scaled replication controller "mapit-1" from 2 to 1
#   Normal	DeploymentCreated		3s	deploymentconfig-controller	Created new replication controller "mapit-2" for version 2
#+end_example

#+begin_src bash
oc set probe dc/mapit --readiness --get-url=http://:8080/health --initial-delay-seconds=30
#+end_src

#+begin_example
# deploymentconfig.apps.openshift.io/mapit probes updated
#+end_example

** Examining DeploymentConfigs and ReplicationControllers

#+begin_src bash
oc get pods
#+end_src

#+begin_example
# NAME             READY   STATUS              RESTARTS   AGE
# mapit-1-deploy   0/1     Completed           0          12m
# mapit-2-bwrhg    1/1     Running             0          37s
# mapit-2-deploy   0/1     Completed           0          46s
# mapit-3-deploy   0/1     ContainerCreating   0          7s
#+end_example

#+begin_src bash
oc get pods
#+end_src

#+begin_example
# NAME             READY   STATUS      RESTARTS   AGE
# mapit-1-deploy   0/1     Completed   0          13m
# mapit-2-bwrhg    1/1     Running     0          65s
# mapit-2-deploy   0/1     Completed   0          74s
# mapit-3-deploy   1/1     Running     0          35s
# mapit-3-rbjmb    0/1     Running     0          26s
#+end_example

#+begin_src bash
oc get deploymentconfigs
#+end_src

#+begin_example
# NAME    REVISION   DESIRED   CURRENT   TRIGGERED BY
# mapit   3          1         1         config,image(mapit:latest)
#+end_example

#+begin_src bash
oc get replicationcontrollers
#+end_src

#+begin_example
# NAME      DESIRED   CURRENT   READY   AGE
# mapit-1   0         0         0       13m
# mapit-2   1         1         1       85s
# mapit-3   1         1         0       46s
#+end_example

* Application Storage Basics

* MachineSets, Machines, and Nodes

* Infrastructure Nodes and Operators

* Cluster Admin Authentication

#+begin_src bash
oc explain Role
#+end_src

#+begin_example
# KIND:     Role
# VERSION:  rbac.authorization.k8s.io/v1
# 
# DESCRIPTION:
#      Role is a namespaced, logical grouping of PolicyRules that can be
#      referenced as a unit by a RoleBinding.
# 
# FIELDS:
#    apiVersion	<string>
#      APIVersion defines the versioned schema of this representation of an
#      object. Servers should convert recognized schemas to the latest internal
#      value, and may reject unrecognized values. More info:
#      https://git.k8s.io/community/contributors/devel/api-conventions.md#resources
# 
#    kind	<string>
#      Kind is a string value representing the REST resource this object
#      represents. Servers may infer this from the endpoint the client submits
#      requests to. Cannot be updated. In CamelCase. More info:
#      https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds
# 
#    metadata	<Object>
#      Standard object's metadata.
# 
#    rules	<[]Object>
#      Rules holds all the PolicyRules for this Role
#+end_example
#+begin_src bash
oc explain ClusterRole
#+end_src

#+begin_example
# KIND:     ClusterRole
# VERSION:  rbac.authorization.k8s.io/v1
# 
# DESCRIPTION:
#      ClusterRole is a cluster level, logical grouping of PolicyRules that can be
#      referenced as a unit by a RoleBinding or ClusterRoleBinding.
# 
# FIELDS:
#    aggregationRule	<Object>
#      AggregationRule is an optional field that describes how to build the Rules
#      for this ClusterRole. If AggregationRule is set, then the Rules are
#      controller managed and direct changes to Rules will be stomped by the
#      controller.
# 
#    apiVersion	<string>
#      APIVersion defines the versioned schema of this representation of an
#      object. Servers should convert recognized schemas to the latest internal
#      value, and may reject unrecognized values. More info:
#      https://git.k8s.io/community/contributors/devel/api-conventions.md#resources
# 
#    kind	<string>
#      Kind is a string value representing the REST resource this object
#      represents. Servers may infer this from the endpoint the client submits
#      requests to. Cannot be updated. In CamelCase. More info:
#      https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds
# 
#    metadata	<Object>
#      Standard object's metadata.
# 
#    rules	<[]Object>
#      Rules holds all the PolicyRules for this ClusterRole
# 
#+end_example
#+begin_src bash
oc explain RoleBinding

#+begin_example
# KIND:     RoleBinding
# VERSION:  rbac.authorization.k8s.io/v1
# 
# DESCRIPTION:
#      RoleBinding references a role, but does not contain it. It can reference a
#      Role in the same namespace or a ClusterRole in the global namespace. It
#      adds who information via Subjects and namespace information by which
#      namespace it exists in. RoleBindings in a given namespace only have effect
#      in that namespace.
# 
# FIELDS:
#    apiVersion	<string>
#      APIVersion defines the versioned schema of this representation of an
#      object. Servers should convert recognized schemas to the latest internal
#      value, and may reject unrecognized values. More info:
#      https://git.k8s.io/community/contributors/devel/api-conventions.md#resources
# 
#    kind	<string>
#      Kind is a string value representing the REST resource this object
#      represents. Servers may infer this from the endpoint the client submits
#      requests to. Cannot be updated. In CamelCase. More info:
#      https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds
# 
#    metadata	<Object>
#      Standard object's metadata.
# 
#    roleRef	<Object> -required-
#      RoleRef can reference a Role in the current namespace or a ClusterRole in
#      the global namespace. If the RoleRef cannot be resolved, the Authorizer
#      must return an error.
# 
#    subjects	<[]Object>
#      Subjects holds references to the objects the role applies to.
# 
#+end_example

* Machineset

#+begin_src bash
cat /opt/app-root/src/support/infra-nodes.sh
#+end_src

* Deploying and Managing OpenShift Container Storage

#+begin_src bash
oc get nodes -l node-role.kubernetes.io/worker -l '!node-role.kubernetes.io/infra','!node-role.kubernetes.io/master'
#+end_src

#+begin_example
# NAME                                         STATUS   ROLES    AGE    VERSION
# ip-10-0-133-255.us-east-2.compute.internal   Ready    worker   22h    v1.14.6+9fb2d5cf9
# ip-10-0-156-47.us-east-2.compute.internal    Ready    worker   22h    v1.14.6+9fb2d5cf9
# ip-10-0-172-64.us-east-2.compute.internal    Ready    worker   5h7m   v1.14.6+9fb2d5cf9
#+end_example

#+begin_src bash
oc get machinesets -n openshift-machine-api | grep -v infra
#+end_src

#+begin_example
# NAME                                          DESIRED   CURRENT   READY   AVAILABLE   AGE
# cluster-munich-e7ab-lqhqg-worker-us-east-2a   1         1         1       1           22h
# cluster-munich-e7ab-lqhqg-worker-us-east-2b   1         1         1       1           22h
# cluster-munich-e7ab-lqhqg-worker-us-east-2c   1         1         1       1           22h
#+end_example

#+begin_src bash
CLUSTERID=$(oc get machineset -n openshift-machine-api -o jsonpath='{.items[0].metadata.labels.machine\.openshift\.io/cluster-api-cluster}')
echo $CLUSTERID
#+end_src

#+begin_example
cluster-munich-e7ab-lqhqg
#+end_example

#+begin_src bash
cat ~/test/support/ocslab_cluster-workerocs.yaml
#+end_src

#+begin_example
# ---
# apiVersion: machine.openshift.io/v1beta1
# kind: MachineSet
# metadata:
#   labels:
#     machine.openshift.io/cluster-api-cluster: CLUSTERID
#     machine.openshift.io/cluster-api-machine-role: workerocs
#     machine.openshift.io/cluster-api-machine-type: workerocs
#   name: CLUSTERID-workerocs-us-east-2a
#   namespace: openshift-machine-api
# spec:
#   replicas: 1
#   selector:
#     matchLabels:
#       machine.openshift.io/cluster-api-cluster: CLUSTERID
#       machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-2a
#   template:
#     metadata:
#       creationTimestamp: null
#       labels:
#         machine.openshift.io/cluster-api-cluster: CLUSTERID
#         machine.openshift.io/cluster-api-machine-role: workerocs
#         machine.openshift.io/cluster-api-machine-type: workerocs
#         machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-2a
#     spec:
#       metadata:
#         creationTimestamp: null
#         labels:
#           role: storage-node
#           node-role.kubernetes.io/worker: ""
#       providerSpec:
#         value:
#           ami:
#             id: ami-0bc59aaa7363b805d
#           apiVersion: awsproviderconfig.openshift.io/v1beta1
#           blockDevices:
#           - ebs:
#               iops: 0
#               volumeSize: 120
#               volumeType: gp2
#           credentialsSecret:
#             name: aws-cloud-credentials
#           deviceIndex: 0
#           iamInstanceProfile:
#             id: CLUSTERID-worker-profile
#           instanceType: m5.4xlarge
#           kind: AWSMachineProviderConfig
#           metadata:
#             creationTimestamp: null
#           placement:
#             availabilityZone: us-east-2a
#             region: us-east-2
#           publicIp: null
#           securityGroups:
#           - filters:
#             - name: tag:Name
#               values:
#               - CLUSTERID-worker-sg
#           subnet:
#             filters:
#             - name: tag:Name
#               values:
#               - CLUSTERID-private-us-east-2a
#           tags:
#           - name: kubernetes.io/cluster/CLUSTERID
#             value: owned
#           userDataSecret:
#             name: worker-user-data
#       versions:
#         kubelet: ""
# ---
# apiVersion: machine.openshift.io/v1beta1
# kind: MachineSet
# metadata:
#   labels:
#     machine.openshift.io/cluster-api-cluster: CLUSTERID
#     machine.openshift.io/cluster-api-machine-role: workerocs
#     machine.openshift.io/cluster-api-machine-type: workerocs
#   name: CLUSTERID-workerocs-us-east-2b
#   namespace: openshift-machine-api
# spec:
#   replicas: 1
#   selector:
#     matchLabels:
#       machine.openshift.io/cluster-api-cluster: CLUSTERID
#       machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-2b
#   template:
#     metadata:
#       creationTimestamp: null
#       labels:
#         machine.openshift.io/cluster-api-cluster: CLUSTERID
#         machine.openshift.io/cluster-api-machine-role: workerocs
#         machine.openshift.io/cluster-api-machine-type: workerocs
#         machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-2b
#     spec:
#       metadata:
#         creationTimestamp: null
#         labels:
#           role: storage-node
#           node-role.kubernetes.io/worker: ""
#       providerSpec:
#         value:
#           ami:
#             id: ami-0bc59aaa7363b805d
#           apiVersion: awsproviderconfig.openshift.io/v1beta1
#           blockDevices:
#           - ebs:
#               iops: 0
#               volumeSize: 120
#               volumeType: gp2
#           credentialsSecret:
#             name: aws-cloud-credentials
#           deviceIndex: 0
#           iamInstanceProfile:
#             id: CLUSTERID-worker-profile
#           instanceType: m5.4xlarge
#           kind: AWSMachineProviderConfig
#           metadata:
#             creationTimestamp: null
#           placement:
#             availabilityZone: us-east-2b
#             region: us-east-2
#           publicIp: null
#           securityGroups:
#           - filters:
#             - name: tag:Name
#               values:
#               - CLUSTERID-worker-sg
#           subnet:
#             filters:
#             - name: tag:Name
#               values:
#               - CLUSTERID-private-us-east-2b
#           tags:
#           - name: kubernetes.io/cluster/CLUSTERID
#             value: owned
#           userDataSecret:
#             name: worker-user-data
#       versions:
#         kubelet: ""
# ---
# apiVersion: machine.openshift.io/v1beta1
# kind: MachineSet
# metadata:
#   labels:
#     machine.openshift.io/cluster-api-cluster: CLUSTERID
#     machine.openshift.io/cluster-api-machine-role: workerocs
#     machine.openshift.io/cluster-api-machine-type: workerocs
#   name: CLUSTERID-workerocs-us-east-2c
#   namespace: openshift-machine-api
# spec:
#   replicas: 1
#   selector:
#     matchLabels:
#       machine.openshift.io/cluster-api-cluster: CLUSTERID
#       machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-2c
#   template:
#     metadata:
#       creationTimestamp: null
#       labels:
#         machine.openshift.io/cluster-api-cluster: CLUSTERID
#         machine.openshift.io/cluster-api-machine-role: workerocs
#         machine.openshift.io/cluster-api-machine-type: workerocs
#         machine.openshift.io/cluster-api-machineset: CLUSTERID-workerocs-us-east-2c
#     spec:
#       metadata:
#         creationTimestamp: null
#         labels:
#           role: storage-node
#           node-role.kubernetes.io/worker: ""
#       providerSpec:
#         value:
#           ami:
#             id: ami-0bc59aaa7363b805d
#           apiVersion: awsproviderconfig.openshift.io/v1beta1
#           blockDevices:
#           - ebs:
#               iops: 0
#               volumeSize: 120
#               volumeType: gp2
#           credentialsSecret:
#             name: aws-cloud-credentials
#           deviceIndex: 0
#           iamInstanceProfile:
#             id: CLUSTERID-worker-profile
#           instanceType: m5.4xlarge
#           kind: AWSMachineProviderConfig
#           metadata:
#             creationTimestamp: null
#           placement:
#             availabilityZone: us-east-2c
#             region: us-east-2
#           publicIp: null
#           securityGroups:
#           - filters:
#             - name: tag:Name
#               values:
#               - CLUSTERID-worker-sg
#           subnet:
#             filters:
#             - name: tag:Name
#               values:
#               - CLUSTERID-private-us-east-2c
#           tags:
#           - name: kubernetes.io/cluster/CLUSTERID
#             value: owned
#           userDataSecret:
#             name: worker-user-data
#       versions:
#         kubelet: ""
# ---
#+end_example

#+begin_src bash
cat ~/test/support/ocslab_cluster-workerocs.yaml | sed "s/CLUSTERID/$CLUSTERID/g" | oc apply -f -
#+end_src

#+begin_src bash
oc get machines -n openshift-machine-api | egrep 'NAME|workerocs'
#+end_src

#+begin_example
# NAME                                                   STATE     TYPE         REGION      ZONE         AGE
# cluster-munich-e7ab-lqhqg-workerocs-us-east-2a-ncm2v   running   m5.4xlarge   us-east-2   us-east-2a   79s
# cluster-munich-e7ab-lqhqg-workerocs-us-east-2b-zf88g   running   m5.4xlarge   us-east-2   us-east-2b   78s
# cluster-munich-e7ab-lqhqg-workerocs-us-east-2c-75plw   running   m5.4xlarge   us-east-2   us-east-2c   78s
#+end_example

#+begin_src bash
oc get nodes -l node-role.kubernetes.io/worker -l '!node-role.kubernetes.io/infra','!node-role.kubernetes.io/master'
#+end_src

#+begin_example
# [lab-user@clientvm 0 ~/test master|✚1…2]$ NAME                                         STATUS   ROLES    AGE   VERSION
# ip-10-0-131-190.us-east-2.compute.internal   Ready    worker   22m   v1.14.6+9fb2d5cf9
# ip-10-0-133-255.us-east-2.compute.internal   Ready    worker   38h   v1.14.6+9fb2d5cf9
# ip-10-0-147-203.us-east-2.compute.internal   Ready    worker   22m   v1.14.6+9fb2d5cf9
# ip-10-0-156-47.us-east-2.compute.internal    Ready    worker   38h   v1.14.6+9fb2d5cf9
# ip-10-0-172-64.us-east-2.compute.internal    Ready    worker   21h   v1.14.6+9fb2d5cf9
# ip-10-0-174-204.us-east-2.compute.internal   Ready    worker   22m   v1.14.6+9fb2d5cf9
#+end_example

#+begin_src bash
oc get nodes -l role=storage-node -o name
#+end_src

#+begin_example
# node/ip-10-0-131-190.us-east-2.compute.internal
# node/ip-10-0-147-203.us-east-2.compute.internal
# node/ip-10-0-174-204.us-east-2.compute.internal
#+end_example

#+begin_src bash
oc get nodes -l role=storage-node -o name | xargs -n1 -t -I {} oc label {} cluster.ocs.openshift.io/openshift-storage=""
#+end_src

#+begin_example
# oc label node/ip-10-0-131-190.us-east-2.compute.internal cluster.ocs.openshift.io/openshift-storage= 
# node/ip-10-0-131-190.us-east-2.compute.internal labeled
# oc label node/ip-10-0-147-203.us-east-2.compute.internal cluster.ocs.openshift.io/openshift-storage= 
# node/ip-10-0-147-203.us-east-2.compute.internal labeled
# oc label node/ip-10-0-174-204.us-east-2.compute.internal cluster.ocs.openshift.io/openshift-storage= 
# node/ip-10-0-174-204.us-east-2.compute.internal labeled
#+end_example

#+begin_src bash
oc get nodes -l cluster.ocs.openshift.io/openshift-storage=
#+end_src

#+begin_example
# NAME                                         STATUS   ROLES    AGE   VERSION
# ip-10-0-131-190.us-east-2.compute.internal   Ready    worker   26m   v1.14.6+9fb2d5cf9
# ip-10-0-147-203.us-east-2.compute.internal   Ready    worker   26m   v1.14.6+9fb2d5cf9
# ip-10-0-174-204.us-east-2.compute.internal   Ready    worker   26m   v1.14.6+9fb2d5cf9
#+end_example

#+begin_src bash
oc create namespace openshift-storage
#+end_src

#+begin_example
# namespace/openshift-storage created
#+end_example

#+begin_src bash
oc label namespace openshift-storage "openshift.io/cluster-monitoring=true"
#+end_src

#+begin_example
# namespace/openshift-storage labeled
#+end_example

#+begin_src bash
oc get -n openshift-console route console
#+end_src

#+begin_example
# NAME      HOST/PORT                                                                    PATH   SERVICES   PORT    TERMINATION          WILDCARD
# console   console-openshift-console.apps.cluster-munich-e7ab.sandbox1596.opentlc.com          console    https   reencrypt/Redirect   None
#+end_example

#+begin_src bash
oc -n openshift-storage get pods
#+end_src

#+begin_example
# NAME                                 READY   STATUS    RESTARTS   AGE
# noobaa-operator-7697b7b488-w7fqk     1/1     Running   0          2m23s
# ocs-operator-55b5dd4d79-j9z75        1/1     Running   0          2m23s
# rook-ceph-operator-7c6c4fd77-k76jl   1/1     Running   0          2m23s
#+end_example

#+begin_src bash
oc get nodes --show-labels | grep ocs | cut -d' ' -f1
#+end_src

#+begin_example
# ip-10-0-131-190.us-east-2.compute.internal
# ip-10-0-147-203.us-east-2.compute.internal
# ip-10-0-174-204.us-east-2.compute.internal
#+end_example

#+begin_src bash
oc -n openshift-storage get pods
#+end_src

#+begin_example
# NAME                                 READY   STATUS    RESTARTS   AGE
# noobaa-operator-7697b7b488-w7fqk     1/1     Running   0          6m22s
# ocs-operator-55b5dd4d79-j9z75        1/1     Running   0          6m22s
# rook-ceph-operator-7c6c4fd77-k76jl   1/1     Running   0          6m22s
#+end_example
Click create in OpenShift console

#+begin_src bash
oc -n openshift-storage get pods
#+end_src

#+begin_example
# NAME                                            READY   STATUS              RESTARTS   AGE
# csi-cephfsplugin-c9k9s                          0/3     ContainerCreating   0          4s
# csi-cephfsplugin-f9hcs                          0/3     ContainerCreating   0          5s
# csi-cephfsplugin-j4fkc                          0/3     ContainerCreating   0          4s
# csi-cephfsplugin-kv2nw                          0/3     ContainerCreating   0          4s
# csi-cephfsplugin-lbjj6                          0/3     ContainerCreating   0          4s
# csi-cephfsplugin-provisioner-647cd6996c-jdsf4   0/4     ContainerCreating   0          4s
# csi-cephfsplugin-provisioner-647cd6996c-q45l9   0/4     ContainerCreating   0          4s
# csi-cephfsplugin-q52sm                          0/3     ContainerCreating   0          4s
# csi-cephfsplugin-rkh44                          0/3     ContainerCreating   0          4s
# csi-cephfsplugin-xbb8b                          0/3     ContainerCreating   0          4s
# csi-cephfsplugin-xwmrc                          0/3     ContainerCreating   0          4s
# csi-rbdplugin-2f7zj                             0/3     ContainerCreating   0          4s
# csi-rbdplugin-6tr5t                             0/3     ContainerCreating   0          5s
# csi-rbdplugin-9tgfg                             0/3     ContainerCreating   0          5s
# csi-rbdplugin-b8sbd                             0/3     ContainerCreating   0          4s
# csi-rbdplugin-fh6xz                             0/3     ContainerCreating   0          4s
# csi-rbdplugin-grd57                             0/3     ContainerCreating   0          4s
# csi-rbdplugin-provisioner-6b8ff67dc4-4xjql      0/4     ContainerCreating   0          5s
# csi-rbdplugin-provisioner-6b8ff67dc4-5z97f      0/4     ContainerCreating   0          4s
# csi-rbdplugin-qwx8d                             0/3     ContainerCreating   0          4s
# csi-rbdplugin-wljmn                             0/3     ContainerCreating   0          5s
# csi-rbdplugin-xvr92                             0/3     ContainerCreating   0          4s
# noobaa-operator-7697b7b488-w7fqk                1/1     Running             0          6m55s
# ocs-operator-55b5dd4d79-j9z75                   1/1     Running             0          6m55s
# rook-ceph-operator-7c6c4fd77-k76jl              1/1     Running             0          6m55s
#+end_example

#+begin_src bash
oc -n openshift-storage get pods
#+end_src

#+begin_example
# NAME                                            READY   STATUS              RESTARTS   AGE
# csi-cephfsplugin-c9k9s                          0/3     ContainerCreating   0          36s
# csi-cephfsplugin-f9hcs                          3/3     Running             0          37s
# csi-cephfsplugin-j4fkc                          0/3     ContainerCreating   0          36s
# csi-cephfsplugin-kv2nw                          3/3     Running             0          36s
# csi-cephfsplugin-lbjj6                          0/3     ContainerCreating   0          36s
# csi-cephfsplugin-provisioner-647cd6996c-jdsf4   0/4     ContainerCreating   0          36s
# csi-cephfsplugin-provisioner-647cd6996c-q45l9   4/4     Running             0          36s
# csi-cephfsplugin-q52sm                          0/3     ContainerCreating   0          36s
# csi-cephfsplugin-rkh44                          3/3     Running             0          36s
# csi-cephfsplugin-xbb8b                          0/3     ContainerCreating   0          36s
# csi-cephfsplugin-xwmrc                          3/3     Running             0          36s
# csi-rbdplugin-2f7zj                             0/3     ContainerCreating   0          36s
# csi-rbdplugin-6tr5t                             0/3     ContainerCreating   0          37s
# csi-rbdplugin-9tgfg                             3/3     Running             0          37s
# csi-rbdplugin-b8sbd                             0/3     ContainerCreating   0          36s
# csi-rbdplugin-fh6xz                             3/3     Running             0          36s
# csi-rbdplugin-grd57                             3/3     Running             0          36s
# csi-rbdplugin-provisioner-6b8ff67dc4-4xjql      4/4     Running             0          37s
# csi-rbdplugin-provisioner-6b8ff67dc4-5z97f      0/4     ContainerCreating   0          36s
# csi-rbdplugin-qwx8d                             0/3     ContainerCreating   0          36s
# csi-rbdplugin-wljmn                             3/3     Running             0          37s
# csi-rbdplugin-xvr92                             0/3     ContainerCreating   0          36s
# noobaa-operator-7697b7b488-w7fqk                1/1     Running             0          7m27s
# ocs-operator-55b5dd4d79-j9z75                   0/1     Running             0          7m27s
# rook-ceph-detect-version-mlrgr                  0/1     PodInitializing     0          30s
# rook-ceph-operator-7c6c4fd77-k76jl              1/1     Running             0          7m27s
#+end_example

#+begin_src bash
oc -n openshift-storage get pods
#+end_src

#+begin_example
# NAME                                            READY   STATUS     RESTARTS   AGE
# csi-cephfsplugin-c9k9s                          3/3     Running    0          2m10s
# csi-cephfsplugin-f9hcs                          3/3     Running    0          2m11s
# csi-cephfsplugin-j4fkc                          3/3     Running    0          2m10s
# csi-cephfsplugin-kv2nw                          3/3     Running    0          2m10s
# csi-cephfsplugin-lbjj6                          3/3     Running    0          2m10s
# csi-cephfsplugin-provisioner-647cd6996c-jdsf4   4/4     Running    0          2m10s
# csi-cephfsplugin-provisioner-647cd6996c-q45l9   4/4     Running    0          2m10s
# csi-cephfsplugin-q52sm                          3/3     Running    0          2m10s
# csi-cephfsplugin-rkh44                          3/3     Running    0          2m10s
# csi-cephfsplugin-xbb8b                          3/3     Running    0          2m10s
# csi-cephfsplugin-xwmrc                          3/3     Running    0          2m10s
# csi-rbdplugin-2f7zj                             3/3     Running    0          2m10s
# csi-rbdplugin-6tr5t                             3/3     Running    0          2m11s
# csi-rbdplugin-9tgfg                             3/3     Running    0          2m11s
# csi-rbdplugin-b8sbd                             3/3     Running    0          2m10s
# csi-rbdplugin-fh6xz                             3/3     Running    0          2m10s
# csi-rbdplugin-grd57                             3/3     Running    0          2m10s
# csi-rbdplugin-provisioner-6b8ff67dc4-4xjql      4/4     Running    0          2m11s
# csi-rbdplugin-provisioner-6b8ff67dc4-5z97f      4/4     Running    0          2m10s
# csi-rbdplugin-qwx8d                             3/3     Running    0          2m10s
# csi-rbdplugin-wljmn                             3/3     Running    0          2m11s
# csi-rbdplugin-xvr92                             3/3     Running    0          2m10s
# noobaa-operator-7697b7b488-w7fqk                1/1     Running    0          9m1s
# ocs-operator-55b5dd4d79-j9z75                   0/1     Running    0          9m1s
# rook-ceph-mon-a-6c7d8bbc5b-fv6g5                1/1     Running    0          61s
# rook-ceph-mon-b-657d966977-ds9pb                1/1     Running    0          38s
# rook-ceph-mon-c-58c5f47974-bdm2g                0/1     Init:0/2   0          4s
# rook-ceph-operator-7c6c4fd77-k76jl              1/1     Running    0          9m1s
#+end_example

#+begin_src bash
oc -n openshift-storage get pods
#+end_src

#+begin_example
# NAME                                                              READY   STATUS      RESTARTS   AGE
# csi-cephfsplugin-c9k9s                                            3/3     Running     0          5m37s
# csi-cephfsplugin-f9hcs                                            3/3     Running     0          5m38s
# csi-cephfsplugin-j4fkc                                            3/3     Running     0          5m37s
# csi-cephfsplugin-kv2nw                                            3/3     Running     0          5m37s
# csi-cephfsplugin-lbjj6                                            3/3     Running     0          5m37s
# csi-cephfsplugin-provisioner-647cd6996c-jdsf4                     4/4     Running     0          5m37s
# csi-cephfsplugin-provisioner-647cd6996c-q45l9                     4/4     Running     0          5m37s
# csi-cephfsplugin-q52sm                                            3/3     Running     0          5m37s
# csi-cephfsplugin-rkh44                                            3/3     Running     0          5m37s
# csi-cephfsplugin-xbb8b                                            3/3     Running     0          5m37s
# csi-cephfsplugin-xwmrc                                            3/3     Running     0          5m37s
# csi-rbdplugin-2f7zj                                               3/3     Running     0          5m37s
# csi-rbdplugin-6tr5t                                               3/3     Running     0          5m38s
# csi-rbdplugin-9tgfg                                               3/3     Running     0          5m38s
# csi-rbdplugin-b8sbd                                               3/3     Running     0          5m37s
# csi-rbdplugin-fh6xz                                               3/3     Running     0          5m37s
# csi-rbdplugin-grd57                                               3/3     Running     0          5m37s
# csi-rbdplugin-provisioner-6b8ff67dc4-4xjql                        4/4     Running     0          5m38s
# csi-rbdplugin-provisioner-6b8ff67dc4-5z97f                        4/4     Running     0          5m37s
# csi-rbdplugin-qwx8d                                               3/3     Running     0          5m37s
# csi-rbdplugin-wljmn                                               3/3     Running     0          5m38s
# csi-rbdplugin-xvr92                                               3/3     Running     0          5m37s
# noobaa-core-0                                                     2/2     Running     0          113s
# noobaa-operator-7697b7b488-w7fqk                                  1/1     Running     0          12m
# ocs-operator-55b5dd4d79-j9z75                                     0/1     Running     0          12m
# rook-ceph-drain-canary-394b22bae7457d3ad4d30d2cc3859f93-84mqgzx   1/1     Running     0          119s
# rook-ceph-drain-canary-b09e5f36ba368752334f340d47e24fee-96r2wmt   1/1     Running     0          2m
# rook-ceph-drain-canary-b3da251966d747eebc14ed058dcd1838-59j7bnq   1/1     Running     0          2m8s
# rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg   1/1     Running     0          108s
# rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk   1/1     Running     0          107s
# rook-ceph-mgr-a-5bcdb86666-gxw5f                                  1/1     Running     0          2m54s
# rook-ceph-mon-a-6c7d8bbc5b-fv6g5                                  1/1     Running     0          4m28s
# rook-ceph-mon-b-657d966977-ds9pb                                  1/1     Running     0          4m5s
# rook-ceph-mon-c-58c5f47974-bdm2g                                  1/1     Running     0          3m31s
# rook-ceph-operator-7c6c4fd77-k76jl                                1/1     Running     0          12m
# rook-ceph-osd-0-54d86bc475-kbjdb                                  1/1     Running     0          2m8s
# rook-ceph-osd-1-5f56fcff97-cnbrg                                  1/1     Running     0          2m
# rook-ceph-osd-2-5d6b876dd7-hw29d                                  1/1     Running     0          119s
# rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8               0/1     Completed   0          2m33s
# rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g               0/1     Completed   0          2m33s
# rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl               0/1     Completed   0          2m32s
#+end_example

#+begin_src bash
oc -n openshift-storage get sc
#+end_src

#+begin_example
# NAME                          PROVISIONER                             AGE
# gp2 (default)                 kubernetes.io/aws-ebs                   39h
# ocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      13m
# ocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   13m
# openshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         7m55s
#+end_example
#+begin_src bash
cat ~/test/support/ocslab_toolbox.yaml
#+end_src

#+begin_example
# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   name: rook-ceph-tools
#   namespace: openshift-storage
#   labels:
#     app: rook-ceph-tools
# spec:
#   replicas: 1
#   selector:
#     matchLabels:
#       app: rook-ceph-tools
#   template:
#     metadata:
#       labels:
#         app: rook-ceph-tools
#     spec:
#       dnsPolicy: ClusterFirstWithHostNet
#       containers:
#       - name: rook-ceph-tools
#         image: rook/ceph:v1.1.8
#         command: ["/tini"]
#         args: ["-g", "--", "/usr/local/bin/toolbox.sh"]
#         imagePullPolicy: IfNotPresent
#         env:
#           - name: ROOK_ADMIN_SECRET
#             valueFrom:
#               secretKeyRef:
#                 name: rook-ceph-mon
#                 key: admin-secret
#         securityContext:
#           privileged: true
#         volumeMounts:
#           - mountPath: /dev
#             name: dev
#           - mountPath: /sys/bus
#             name: sysbus
#           - mountPath: /lib/modules
#             name: libmodules
#           - name: mon-endpoint-volume
#             mountPath: /etc/rook
#       # if hostNetwork: false, the "rbd map" command hangs, see https://github.com/rook/rook/issues/2021
#       hostNetwork: true
#       volumes:
#         - name: dev
#           hostPath:
#             path: /dev
#         - name: sysbus
#           hostPath:
#             path: /sys/bus
#         - name: libmodules
#           hostPath:
#             path: /lib/modules
#         - name: mon-endpoint-volume
#           configMap:
#             name: rook-ceph-mon-endpoints
#             items:
#             - key: data
#               path: mon-endpoints
#+end_example

#+begin_src bash
oc apply -f ~/test/support/ocslab_toolbox.yaml
#+end_src

#+begin_example
# deployment.apps/rook-ceph-tools created
#+end_example

#+begin_src bash
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
#+end_src

#+begin_example
# [lab-user@clientvm 0 ~/test master|✚1…2]$ sh-4.2# 
# 
#+end_example

#+begin_src bash
ceph status
#+end_src

#+begin_example
# ceph status
#   cluster:
#     id:     cede185f-f780-4daa-a4cf-96fe25d736a4
#     health: HEALTH_OK
#  
#   services:
#     mon: 3 daemons, quorum a,b,c (age 15m)
#     mgr: a(active, since 15m)
#     mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-a=up:active} 1 up:standby-replay
#     osd: 3 osds: 3 up (since 14m), 3 in (since 14m)
#  
#   data:
#     pools:   3 pools, 24 pgs
#     objects: 89 objects, 78 MiB
#     usage:   3.1 GiB used, 3.0 TiB / 3.0 TiB avail
#     pgs:     24 active+clean
#  
#   io:
#     client:   853 B/s rd, 8.0 KiB/s wr, 1 op/s rd, 0 op/s wr
#  
#+end_example

#+begin_src bash
ceph osd status
#+end_src

#+begin_example
# ceph osd status
# +----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
# | id |                    host                    |  used | avail | wr ops | wr data | rd ops | rd data |   state   |
# +----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
# | 0  | ip-10-0-174-204.us-east-2.compute.internal | 1045M | 1021G |    0   |  4096   |    0   |     0   | exists,up |
# | 1  | ip-10-0-131-190.us-east-2.compute.internal | 1045M | 1021G |    1   |  4915   |    2   |   106   | exists,up |
# | 2  | ip-10-0-147-203.us-east-2.compute.internal | 1045M | 1021G |    0   |  2457   |    0   |     0   | exists,up |
# +----+--------------------------------------------+-------+-------+--------+---------+--------+---------+-----------+
#+end_example

#+begin_src bash
ceph osd tree
#+end_src

#+begin_example
# ceph osd tree
# ID  CLASS WEIGHT  TYPE NAME                                STATUS REWEIGHT PRI-AFF 
#  -1       2.99698 root default                                                     
#  -5       2.99698     region us-east-2                                             
# -10       0.99899         zone us-east-2a                                          
#  -9       0.99899             host ocs-deviceset-1-0-x7hnz                         
#   1   ssd 0.99899                 osd.1                        up  1.00000 1.00000 
# -14       0.99899         zone us-east-2b                                          
# -13       0.99899             host ocs-deviceset-2-0-rxlj7                         
#   2   ssd 0.99899                 osd.2                        up  1.00000 1.00000 
#  -4       0.99899         zone us-east-2c                                          
#  -3       0.99899             host ocs-deviceset-0-0-qdn6p                         
#   0   ssd 0.99899                 osd.0                        up  1.00000 1.00000 
#+end_example

#+begin_src bash
ceph df
#+end_src

#+begin_example
# ceph df
# RAW STORAGE:
#     CLASS     SIZE        AVAIL       USED       RAW USED     %RAW USED 
#     ssd       3.0 TiB     3.0 TiB     67 MiB      3.1 GiB          0.10 
#     TOTAL     3.0 TiB     3.0 TiB     67 MiB      3.1 GiB          0.10 
#  
# POOLS:
#     POOL                                           ID     STORED      OBJECTS     USED        %USED     MAX AVAIL 
#     ocs-storagecluster-cephblockpool                1      21 MiB          67      64 MiB         0       971 GiB 
#     ocs-storagecluster-cephfilesystem-metadata      2     2.2 KiB          22     384 KiB         0       971 GiB 
#     ocs-storagecluster-cephfilesystem-data0         3         0 B           0         0 B         0       971 GiB 
#+end_example

#+begin_src bash
rados df
#+end_src

#+begin_example
# rados df
# POOL_NAME                                     USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS      RD WR_OPS     WR USED COMPR UNDER COMPR 
# ocs-storagecluster-cephblockpool            65 MiB      67      0    201                  0       0        0    118 1.2 MiB   2444 29 MiB        0 B         0 B 
# ocs-storagecluster-cephfilesystem-data0        0 B       0      0      0                  0       0        0      0     0 B      0    0 B        0 B         0 B 
# ocs-storagecluster-cephfilesystem-metadata 384 KiB      22      0     66                  0       0        0   1938 973 KiB     45 13 KiB        0 B         0 B 
# 
# total_objects    89
# total_used       3.1 GiB
# total_avail      3.0 TiB
# total_space      3.0 TiB
#+end_example

#+begin_src bash
ceph versions
#+end_src

#+begin_example
# ceph versions
# {
#     "mon": {
#         "ceph version 14.2.4-69.el8cp (8d72f97ca776c758a7ce0009959ca3044cd0b9c2) nautilus (stable)": 3
#     },
#     "mgr": {
#         "ceph version 14.2.4-69.el8cp (8d72f97ca776c758a7ce0009959ca3044cd0b9c2) nautilus (stable)": 1
#     },
#     "osd": {
#         "ceph version 14.2.4-69.el8cp (8d72f97ca776c758a7ce0009959ca3044cd0b9c2) nautilus (stable)": 3
#     },
#     "mds": {
#         "ceph version 14.2.4-69.el8cp (8d72f97ca776c758a7ce0009959ca3044cd0b9c2) nautilus (stable)": 2
#     },
#     "overall": {
#         "ceph version 14.2.4-69.el8cp (8d72f97ca776c758a7ce0009959ca3044cd0b9c2) nautilus (stable)": 9
#     }
# }
#+end_example

#+begin_src bash
exit
#+end_src

** Create a new OCP application deployment using Ceph RBD volume

#+begin_src bash
curl https://raw.githubusercontent.com/red-hat-storage/ocs-training/master/ocp4ocs4/configurable-rails-app.yaml
#+end_src

#+begin_example
# apiVersion: template.openshift.io/v1
# kind: Template
# labels:
#   app: rails-pgsql-persistent-storageclass
#   template: rails-pgsql-persistent-storageclass
# message: |-
#   The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.
# 
#   For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/rails-ex/blob/master/README.md.
# metadata:
#   annotations:
#     description: An example Rails application with a PostgreSQL database. For more
#       information about using this template, including OpenShift considerations, see
#       https://github.com/sclorg/rails-ex/blob/master/README.md.
#     iconClass: icon-ruby
#     openshift.io/display-name: Rails + PostgreSQL + Congigigurable StorageClass
#     openshift.io/documentation-url: https://github.com/sclorg/rails-ex
#     openshift.io/long-description: This template defines resources needed to develop
#       a Rails application, including a build configuration, application deployment
#       configuration, and database deployment configuration.
#     openshift.io/provider-display-name: Red Hat, Inc.
#     openshift.io/support-url: https://access.redhat.com
#     samples.operator.openshift.io/version: 4.2.4
#     tags: quickstart,ruby,rails
#     template.openshift.io/bindable: "false"
#   labels:
#     samples.operator.openshift.io/managed: "true"
#   name: rails-pgsql-persistent-storageclass
# objects:
# - apiVersion: v1
#   kind: Secret
#   metadata:
#     name: ${NAME}
#   stringData:
#     application-password: ${APPLICATION_PASSWORD}
#     application-user: ${APPLICATION_USER}
#     database-password: ${DATABASE_PASSWORD}
#     database-user: ${DATABASE_USER}
#     keybase: ${SECRET_KEY_BASE}
# - apiVersion: v1
#   kind: Service
#   metadata:
#     annotations:
#       description: Exposes and load balances the application pods
#       service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
#         "kind": "Service"}]'
#     name: ${NAME}
#   spec:
#     ports:
#     - name: web
#       port: 8080
#       targetPort: 8080
#     selector:
#       name: ${NAME}
# - apiVersion: v1
#   kind: Route
#   metadata:
#     name: ${NAME}
#   spec:
#     host: ${APPLICATION_DOMAIN}
#     to:
#       kind: Service
#       name: ${NAME}
# - apiVersion: v1
#   kind: ImageStream
#   metadata:
#     annotations:
#       description: Keeps track of changes in the application image
#     name: ${NAME}
# - apiVersion: v1
#   kind: BuildConfig
#   metadata:
#     annotations:
#       description: Defines how to build the application
#       template.alpha.openshift.io/wait-for-ready: "true"
#     name: ${NAME}
#   spec:
#     output:
#       to:
#         kind: ImageStreamTag
#         name: ${NAME}:latest
#     postCommit:
#       script: bundle exec rake test
#     source:
#       contextDir: ${CONTEXT_DIR}
#       git:
#         ref: ${SOURCE_REPOSITORY_REF}
#         uri: ${SOURCE_REPOSITORY_URL}
#       type: Git
#     strategy:
#       sourceStrategy:
#         env:
#         - name: RUBYGEM_MIRROR
#           value: ${RUBYGEM_MIRROR}
#         from:
#           kind: ImageStreamTag
#           name: ruby:2.5
#           namespace: ${NAMESPACE}
#       type: Source
#     triggers:
#     - type: ImageChange
#     - type: ConfigChange
#     - github:
#         secret: ${GITHUB_WEBHOOK_SECRET}
#       type: GitHub
# - apiVersion: v1
#   kind: DeploymentConfig
#   metadata:
#     annotations:
#       description: Defines how to deploy the application server
#       template.alpha.openshift.io/wait-for-ready: "true"
#     name: ${NAME}
#   spec:
#     replicas: 1
#     selector:
#       name: ${NAME}
#     strategy:
#       recreateParams:
#         pre:
#           execNewPod:
#             command:
#             - ./migrate-database.sh
#             containerName: ${NAME}
#           failurePolicy: Abort
#       type: Recreate
#     template:
#       metadata:
#         labels:
#           name: ${NAME}
#         name: ${NAME}
#       spec:
#         containers:
#         - env:
#           - name: DATABASE_SERVICE_NAME
#             value: ${DATABASE_SERVICE_NAME}
#           - name: POSTGRESQL_USER
#             valueFrom:
#               secretKeyRef:
#                 key: database-user
#                 name: ${NAME}
#           - name: POSTGRESQL_PASSWORD
#             valueFrom:
#               secretKeyRef:
#                 key: database-password
#                 name: ${NAME}
#           - name: SECRET_KEY_BASE
#             valueFrom:
#               secretKeyRef:
#                 key: keybase
#                 name: ${NAME}
#           - name: POSTGRESQL_DATABASE
#             value: ${DATABASE_NAME}
#           - name: POSTGRESQL_MAX_CONNECTIONS
#             value: ${POSTGRESQL_MAX_CONNECTIONS}
#           - name: POSTGRESQL_SHARED_BUFFERS
#             value: ${POSTGRESQL_SHARED_BUFFERS}
#           - name: APPLICATION_DOMAIN
#             value: ${APPLICATION_DOMAIN}
#           - name: APPLICATION_USER
#             valueFrom:
#               secretKeyRef:
#                 key: application-user
#                 name: ${NAME}
#           - name: APPLICATION_PASSWORD
#             valueFrom:
#               secretKeyRef:
#                 key: application-password
#                 name: ${NAME}
#           - name: RAILS_ENV
#             value: ${RAILS_ENV}
#           image: ' '
#           livenessProbe:
#             httpGet:
#               path: /articles
#               port: 8080
#             initialDelaySeconds: 10
#             timeoutSeconds: 3
#           name: ${NAME}
#           ports:
#           - containerPort: 8080
#           readinessProbe:
#             httpGet:
#               path: /articles
#               port: 8080
#             initialDelaySeconds: 5
#             timeoutSeconds: 3
#           resources:
#             limits:
#               memory: ${MEMORY_LIMIT}
#     triggers:
#     - imageChangeParams:
#         automatic: true
#         containerNames:
#         - ${NAME}
#         from:
#           kind: ImageStreamTag
#           name: ${NAME}:latest
#       type: ImageChange
#     - type: ConfigChange
# - apiVersion: v1
#   kind: PersistentVolumeClaim
#   metadata:
#     name: ${DATABASE_SERVICE_NAME}
#   spec:
#     accessModes:
#     - ReadWriteOnce
#     storageClassName: ${STORAGE_CLASS}
#     resources:
#       requests:
#         storage: ${VOLUME_CAPACITY}
# - apiVersion: v1
#   kind: Service
#   metadata:
#     annotations:
#       description: Exposes the database server
#     name: ${DATABASE_SERVICE_NAME}
#   spec:
#     ports:
#     - name: postgresql
#       port: 5432
#       targetPort: 5432
#     selector:
#       name: ${DATABASE_SERVICE_NAME}
# - apiVersion: v1
#   kind: DeploymentConfig
#   metadata:
#     annotations:
#       description: Defines how to deploy the database
#       template.alpha.openshift.io/wait-for-ready: "true"
#     name: ${DATABASE_SERVICE_NAME}
#   spec:
#     replicas: 1
#     selector:
#       name: ${DATABASE_SERVICE_NAME}
#     strategy:
#       type: Recreate
#     template:
#       metadata:
#         labels:
#           name: ${DATABASE_SERVICE_NAME}
#         name: ${DATABASE_SERVICE_NAME}
#       spec:
#         containers:
#         - env:
#           - name: POSTGRESQL_USER
#             valueFrom:
#               secretKeyRef:
#                 key: database-user
#                 name: ${NAME}
#           - name: POSTGRESQL_PASSWORD
#             valueFrom:
#               secretKeyRef:
#                 key: database-password
#                 name: ${NAME}
#           - name: POSTGRESQL_DATABASE
#             value: ${DATABASE_NAME}
#           - name: POSTGRESQL_MAX_CONNECTIONS
#             value: ${POSTGRESQL_MAX_CONNECTIONS}
#           - name: POSTGRESQL_SHARED_BUFFERS
#             value: ${POSTGRESQL_SHARED_BUFFERS}
#           image: ' '
#           livenessProbe:
#             exec:
#               command:
#               - /usr/libexec/check-container
#               - --live
#             initialDelaySeconds: 120
#             timeoutSeconds: 10
#           name: postgresql
#           ports:
#           - containerPort: 5432
#           readinessProbe:
#             exec:
#               command:
#               - /usr/libexec/check-container
#             initialDelaySeconds: 5
#             timeoutSeconds: 1
#           resources:
#             limits:
#               memory: ${MEMORY_POSTGRESQL_LIMIT}
#           volumeMounts:
#           - mountPath: /var/lib/pgsql/data
#             name: ${DATABASE_SERVICE_NAME}-data
#         volumes:
#         - name: ${DATABASE_SERVICE_NAME}-data
#           persistentVolumeClaim:
#             claimName: ${DATABASE_SERVICE_NAME}
#     triggers:
#     - imageChangeParams:
#         automatic: true
#         containerNames:
#         - postgresql
#         from:
#           kind: ImageStreamTag
#           name: postgresql:10
#           namespace: ${NAMESPACE}
#       type: ImageChange
#     - type: ConfigChange
# parameters:
# - description: The name assigned to all of the frontend objects defined in this template.
#   displayName: Name
#   name: NAME
#   required: true
#   value: rails-pgsql-persistent
# - description: The OpenShift Namespace where the ImageStream resides.
#   displayName: Namespace
#   name: NAMESPACE
#   required: true
#   value: openshift
# - description: Maximum amount of memory the Rails container can use.
#   displayName: Memory Limit
#   name: MEMORY_LIMIT
#   required: true
#   value: 512Mi
# - description: Maximum amount of memory the PostgreSQL container can use.
#   displayName: Memory Limit (PostgreSQL)
#   name: MEMORY_POSTGRESQL_LIMIT
#   required: true
#   value: 512Mi
# - description: Volume space available for data, e.g. 512Mi, 2Gi
#   displayName: Volume Capacity
#   name: VOLUME_CAPACITY
#   required: true
#   value: 1Gi
# - description: Storage Class used to provision Persistent Volume
#   displayName: Volume Storage Class
#   name: STORAGE_CLASS
#   required: true
#   value: gp2
# - description: The URL of the repository with your application source code.
#   displayName: Git Repository URL
#   name: SOURCE_REPOSITORY_URL
#   required: true
#   value: https://github.com/sclorg/rails-ex.git
# - description: Set this to a branch name, tag or other ref of your repository if you
#     are not using the default branch.
#   displayName: Git Reference
#   name: SOURCE_REPOSITORY_REF
# - description: Set this to the relative path to your project if it is not in the root
#     of your repository.
#   displayName: Context Directory
#   name: CONTEXT_DIR
# - description: The exposed hostname that will route to the Rails service, if left
#     blank a value will be defaulted.
#   displayName: Application Hostname
#   name: APPLICATION_DOMAIN
# - description: Github trigger secret.  A difficult to guess string encoded as part
#     of the webhook URL.  Not encrypted.
#   displayName: GitHub Webhook Secret
#   from: '[a-zA-Z0-9]{40}'
#   generate: expression
#   name: GITHUB_WEBHOOK_SECRET
# - description: Your secret key for verifying the integrity of signed cookies.
#   displayName: Secret Key
#   from: '[a-z0-9]{127}'
#   generate: expression
#   name: SECRET_KEY_BASE
# - description: The application user that is used within the sample application to
#     authorize access on pages.
#   displayName: Application Username
#   name: APPLICATION_USER
#   required: true
#   value: openshift
# - description: The application password that is used within the sample application
#     to authorize access on pages.
#   displayName: Application Password
#   name: APPLICATION_PASSWORD
#   required: true
#   value: secret
# - description: Environment under which the sample application will run. Could be set
#     to production, development or test.
#   displayName: Rails Environment
#   name: RAILS_ENV
#   required: true
#   value: production
# - displayName: Database Service Name
#   name: DATABASE_SERVICE_NAME
#   required: true
#   value: postgresql
# - displayName: Database Username
#   from: user[A-Z0-9]{3}
#   generate: expression
#   name: DATABASE_USER
# - displayName: Database Password
#   from: '[a-zA-Z0-9]{8}'
#   generate: expression
#   name: DATABASE_PASSWORD
# - displayName: Database Name
#   name: DATABASE_NAME
#   required: true
#   value: root
# - displayName: Maximum Database Connections
#   name: POSTGRESQL_MAX_CONNECTIONS
#   value: "100"
# - displayName: Shared Buffer Amount
#   name: POSTGRESQL_SHARED_BUFFERS
#   value: 12MB
# - description: The custom RubyGems mirror URL
#   displayName: Custom RubyGems Mirror URL
#   name: RUBYGEM_MIRROR
#+end_example

#+begin_src bash
oc new-project my-database-app
#+end_src

#+begin_example
# Now using project "my-database-app" on server "https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443".
# 
# You can add applications to this project with the 'new-app' command. For example, try:
# 
#     oc new-app django-psql-example
# 
# to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:
# 
#     kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
# 
#+end_example

#+begin_src bash
cat ~/test/support/ocslab_rails-app.yaml
#+end_src

#+begin_example
# apiVersion: template.openshift.io/v1
# kind: Template
# labels:
#   app: rails-pgsql-persistent-storageclass
#   template: rails-pgsql-persistent-storageclass
# message: |-
#   The following service(s) have been created in your project: ${NAME}, ${DATABASE_SERVICE_NAME}.
# 
#   For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/rails-ex/blob/master/README.md.
# metadata:
#   annotations:
#     description: An example Rails application with a PostgreSQL database. For more
#       information about using this template, including OpenShift considerations, see
#       https://github.com/sclorg/rails-ex/blob/master/README.md.
#     iconClass: icon-ruby
#     openshift.io/display-name: Rails + PostgreSQL + Congigigurable StorageClass
#     openshift.io/documentation-url: https://github.com/sclorg/rails-ex
#     openshift.io/long-description: This template defines resources needed to develop
#       a Rails application, including a build configuration, application deployment
#       configuration, and database deployment configuration.
#     openshift.io/provider-display-name: Red Hat, Inc.
#     openshift.io/support-url: https://access.redhat.com
#     samples.operator.openshift.io/version: 4.2.4
#     tags: quickstart,ruby,rails
#     template.openshift.io/bindable: "false"
#   labels:
#     samples.operator.openshift.io/managed: "true"
#   name: rails-pgsql-persistent-storageclass
# objects:
# - apiVersion: v1
#   kind: Secret
#   metadata:
#     name: ${NAME}
#   stringData:
#     application-password: ${APPLICATION_PASSWORD}
#     application-user: ${APPLICATION_USER}
#     database-password: ${DATABASE_PASSWORD}
#     database-user: ${DATABASE_USER}
#     keybase: ${SECRET_KEY_BASE}
# - apiVersion: v1
#   kind: Service
#   metadata:
#     annotations:
#       description: Exposes and load balances the application pods
#       service.alpha.openshift.io/dependencies: '[{"name": "${DATABASE_SERVICE_NAME}",
#         "kind": "Service"}]'
#     name: ${NAME}
#   spec:
#     ports:
#     - name: web
#       port: 8080
#       targetPort: 8080
#     selector:
#       name: ${NAME}
# - apiVersion: v1
#   kind: Route
#   metadata:
#     name: ${NAME}
#   spec:
#     host: ${APPLICATION_DOMAIN}
#     to:
#       kind: Service
#       name: ${NAME}
# - apiVersion: v1
#   kind: ImageStream
#   metadata:
#     annotations:
#       description: Keeps track of changes in the application image
#     name: ${NAME}
# - apiVersion: v1
#   kind: BuildConfig
#   metadata:
#     annotations:
#       description: Defines how to build the application
#       template.alpha.openshift.io/wait-for-ready: "true"
#     name: ${NAME}
#   spec:
#     output:
#       to:
#         kind: ImageStreamTag
#         name: ${NAME}:latest
#     postCommit:
#       script: bundle exec rake test
#     source:
#       contextDir: ${CONTEXT_DIR}
#       git:
#         ref: ${SOURCE_REPOSITORY_REF}
#         uri: ${SOURCE_REPOSITORY_URL}
#       type: Git
#     strategy:
#       sourceStrategy:
#         env:
#         - name: RUBYGEM_MIRROR
#           value: ${RUBYGEM_MIRROR}
#         from:
#           kind: ImageStreamTag
#           name: ruby:2.5
#           namespace: ${NAMESPACE}
#       type: Source
#     triggers:
#     - type: ImageChange
#     - type: ConfigChange
#     - github:
#         secret: ${GITHUB_WEBHOOK_SECRET}
#       type: GitHub
# - apiVersion: v1
#   kind: DeploymentConfig
#   metadata:
#     annotations:
#       description: Defines how to deploy the application server
#       template.alpha.openshift.io/wait-for-ready: "true"
#     name: ${NAME}
#   spec:
#     replicas: 1
#     selector:
#       name: ${NAME}
#     strategy:
#       recreateParams:
#         pre:
#           execNewPod:
#             command:
#             - ./migrate-database.sh
#             containerName: ${NAME}
#           failurePolicy: Abort
#       type: Recreate
#     template:
#       metadata:
#         labels:
#           name: ${NAME}
#         name: ${NAME}
#       spec:
#         containers:
#         - env:
#           - name: DATABASE_SERVICE_NAME
#             value: ${DATABASE_SERVICE_NAME}
#           - name: POSTGRESQL_USER
#             valueFrom:
#               secretKeyRef:
#                 key: database-user
#                 name: ${NAME}
#           - name: POSTGRESQL_PASSWORD
#             valueFrom:
#               secretKeyRef:
#                 key: database-password
#                 name: ${NAME}
#           - name: SECRET_KEY_BASE
#             valueFrom:
#               secretKeyRef:
#                 key: keybase
#                 name: ${NAME}
#           - name: POSTGRESQL_DATABASE
#             value: ${DATABASE_NAME}
#           - name: POSTGRESQL_MAX_CONNECTIONS
#             value: ${POSTGRESQL_MAX_CONNECTIONS}
#           - name: POSTGRESQL_SHARED_BUFFERS
#             value: ${POSTGRESQL_SHARED_BUFFERS}
#           - name: APPLICATION_DOMAIN
#             value: ${APPLICATION_DOMAIN}
#           - name: APPLICATION_USER
#             valueFrom:
#               secretKeyRef:
#                 key: application-user
#                 name: ${NAME}
#           - name: APPLICATION_PASSWORD
#             valueFrom:
#               secretKeyRef:
#                 key: application-password
#                 name: ${NAME}
#           - name: RAILS_ENV
#             value: ${RAILS_ENV}
#           image: ' '
#           livenessProbe:
#             httpGet:
#               path: /articles
#               port: 8080
#             initialDelaySeconds: 10
#             timeoutSeconds: 3
#           name: ${NAME}
#           ports:
#           - containerPort: 8080
#           readinessProbe:
#             httpGet:
#               path: /articles
#               port: 8080
#             initialDelaySeconds: 5
#             timeoutSeconds: 3
#           resources:
#             limits:
#               memory: ${MEMORY_LIMIT}
#     triggers:
#     - imageChangeParams:
#         automatic: true
#         containerNames:
#         - ${NAME}
#         from:
#           kind: ImageStreamTag
#           name: ${NAME}:latest
#       type: ImageChange
#     - type: ConfigChange
# - apiVersion: v1
#   kind: PersistentVolumeClaim
#   metadata:
#     name: ${DATABASE_SERVICE_NAME}
#   spec:
#     accessModes:
#     - ReadWriteOnce
#     storageClassName: ${STORAGE_CLASS}
#     resources:
#       requests:
#         storage: ${VOLUME_CAPACITY}
# - apiVersion: v1
#   kind: Service
#   metadata:
#     annotations:
#       description: Exposes the database server
#     name: ${DATABASE_SERVICE_NAME}
#   spec:
#     ports:
#     - name: postgresql
#       port: 5432
#       targetPort: 5432
#     selector:
#       name: ${DATABASE_SERVICE_NAME}
# - apiVersion: v1
#   kind: DeploymentConfig
#   metadata:
#     annotations:
#       description: Defines how to deploy the database
#       template.alpha.openshift.io/wait-for-ready: "true"
#     name: ${DATABASE_SERVICE_NAME}
#   spec:
#     replicas: 1
#     selector:
#       name: ${DATABASE_SERVICE_NAME}
#     strategy:
#       type: Recreate
#     template:
#       metadata:
#         labels:
#           name: ${DATABASE_SERVICE_NAME}
#         name: ${DATABASE_SERVICE_NAME}
#       spec:
#         containers:
#         - env:
#           - name: POSTGRESQL_USER
#             valueFrom:
#               secretKeyRef:
#                 key: database-user
#                 name: ${NAME}
#           - name: POSTGRESQL_PASSWORD
#             valueFrom:
#               secretKeyRef:
#                 key: database-password
#                 name: ${NAME}
#           - name: POSTGRESQL_DATABASE
#             value: ${DATABASE_NAME}
#           - name: POSTGRESQL_MAX_CONNECTIONS
#             value: ${POSTGRESQL_MAX_CONNECTIONS}
#           - name: POSTGRESQL_SHARED_BUFFERS
#             value: ${POSTGRESQL_SHARED_BUFFERS}
#           image: ' '
#           livenessProbe:
#             exec:
#               command:
#               - /usr/libexec/check-container
#               - --live
#             initialDelaySeconds: 120
#             timeoutSeconds: 10
#           name: postgresql
#           ports:
#           - containerPort: 5432
#           readinessProbe:
#             exec:
#               command:
#               - /usr/libexec/check-container
#             initialDelaySeconds: 5
#             timeoutSeconds: 1
#           resources:
#             limits:
#               memory: ${MEMORY_POSTGRESQL_LIMIT}
#           volumeMounts:
#           - mountPath: /var/lib/pgsql/data
#             name: ${DATABASE_SERVICE_NAME}-data
#         volumes:
#         - name: ${DATABASE_SERVICE_NAME}-data
#           persistentVolumeClaim:
#             claimName: ${DATABASE_SERVICE_NAME}
#     triggers:
#     - imageChangeParams:
#         automatic: true
#         containerNames:
#         - postgresql
#         from:
#           kind: ImageStreamTag
#           name: postgresql:10
#           namespace: ${NAMESPACE}
#       type: ImageChange
#     - type: ConfigChange
# parameters:
# - description: The name assigned to all of the frontend objects defined in this template.
#   displayName: Name
#   name: NAME
#   required: true
#   value: rails-pgsql-persistent
# - description: The OpenShift Namespace where the ImageStream resides.
#   displayName: Namespace
#   name: NAMESPACE
#   required: true
#   value: openshift
# - description: Maximum amount of memory the Rails container can use.
#   displayName: Memory Limit
#   name: MEMORY_LIMIT
#   required: true
#   value: 512Mi
# - description: Maximum amount of memory the PostgreSQL container can use.
#   displayName: Memory Limit (PostgreSQL)
#   name: MEMORY_POSTGRESQL_LIMIT
#   required: true
#   value: 512Mi
# - description: Volume space available for data, e.g. 512Mi, 2Gi
#   displayName: Volume Capacity
#   name: VOLUME_CAPACITY
#   required: true
#   value: 1Gi
# - description: Storage Class used to provision Persistent Volume
#   displayName: Volume Storage Class
#   name: STORAGE_CLASS
#   required: true
#   value: gp2
# - description: The URL of the repository with your application source code.
#   displayName: Git Repository URL
#   name: SOURCE_REPOSITORY_URL
#   required: true
#   value: https://github.com/sclorg/rails-ex.git
# - description: Set this to a branch name, tag or other ref of your repository if you
#     are not using the default branch.
#   displayName: Git Reference
#   name: SOURCE_REPOSITORY_REF
# - description: Set this to the relative path to your project if it is not in the root
#     of your repository.
#   displayName: Context Directory
#   name: CONTEXT_DIR
# - description: The exposed hostname that will route to the Rails service, if left
#     blank a value will be defaulted.
#   displayName: Application Hostname
#   name: APPLICATION_DOMAIN
# - description: Github trigger secret.  A difficult to guess string encoded as part
#     of the webhook URL.  Not encrypted.
#   displayName: GitHub Webhook Secret
#   from: '[a-zA-Z0-9]{40}'
#   generate: expression
#   name: GITHUB_WEBHOOK_SECRET
# - description: Your secret key for verifying the integrity of signed cookies.
#   displayName: Secret Key
#   from: '[a-z0-9]{127}'
#   generate: expression
#   name: SECRET_KEY_BASE
# - description: The application user that is used within the sample application to
#     authorize access on pages.
#   displayName: Application Username
#   name: APPLICATION_USER
#   required: true
#   value: openshift
# - description: The application password that is used within the sample application
#     to authorize access on pages.
#   displayName: Application Password
#   name: APPLICATION_PASSWORD
#   required: true
#   value: secret
# - description: Environment under which the sample application will run. Could be set
#     to production, development or test.
#   displayName: Rails Environment
#   name: RAILS_ENV
#   required: true
#   value: production
# - displayName: Database Service Name
#   name: DATABASE_SERVICE_NAME
#   required: true
#   value: postgresql
# - displayName: Database Username
#   from: user[A-Z0-9]{3}
#   generate: expression
#   name: DATABASE_USER
# - displayName: Database Password
#   from: '[a-zA-Z0-9]{8}'
#   generate: expression
#   name: DATABASE_PASSWORD
# - displayName: Database Name
#   name: DATABASE_NAME
#   required: true
#   value: root
# - displayName: Maximum Database Connections
#   name: POSTGRESQL_MAX_CONNECTIONS
#   value: "100"
# - displayName: Shared Buffer Amount
#   name: POSTGRESQL_SHARED_BUFFERS
#   value: 12MB
# - description: The custom RubyGems mirror URL
#   displayName: Custom RubyGems Mirror URL
#   name: RUBYGEM_MIRROR
#+end_example

#+begin_src bash
oc new-app -f ~/test/support/ocslab_rails-app.yaml -p STORAGE_CLASS=ocs-storagecluster-ceph-rbd -p VOLUME_CAPACITY=5Gi
#+end_src

#+begin_example
# --> Deploying template "my-database-app/rails-pgsql-persistent-storageclass" for "/home/lab-user/test/support/ocslab_rails-app.yaml" to project my-database-app
# 
#      Rails + PostgreSQL + Congigigurable StorageClass
#      ---------
#      An example Rails application with a PostgreSQL database. For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/rails-ex/blob/master/README.md.
# 
#      The following service(s) have been created in your project: rails-pgsql-persistent, postgresql.
#      
#      For more information about using this template, including OpenShift considerations, see https://github.com/sclorg/rails-ex/blob/master/README.md.
# 
#      * With parameters:
#         * Name=rails-pgsql-persistent
#         * Namespace=openshift
#         * Memory Limit=512Mi
#         * Memory Limit (PostgreSQL)=512Mi
#         * Volume Capacity=5Gi
#         * Volume Storage Class=ocs-storagecluster-ceph-rbd
#         * Git Repository URL=https://github.com/sclorg/rails-ex.git
#         * Git Reference=
#         * Context Directory=
#         * Application Hostname=
#         * GitHub Webhook Secret=WpnPTrahB2GpcFonOGDmKrO6QoPQycBRP2FO8fLw # generated
#         * Secret Key=rng82tm1a385fbn1moul2sv6v3jp3nrnvynv17odfmu0hywltjx2cgkr1x5ihrohbk83dxjwhfs8cnlqaf5wifpkm8dfoip0p75qmdc3rkhd1a7et21qc8kbebsieca # generated
#         * Application Username=openshift
#         * Application Password=secret
#         * Rails Environment=production
#         * Database Service Name=postgresql
#         * Database Username=userX5M # generated
#         * Database Password=d4U7VTMs # generated
#         * Database Name=root
#         * Maximum Database Connections=100
#         * Shared Buffer Amount=12MB
#         * Custom RubyGems Mirror URL=
# 
# --> Creating resources ...
#     secret "rails-pgsql-persistent" created
#     service "rails-pgsql-persistent" created
#     route.route.openshift.io "rails-pgsql-persistent" created
#     imagestream.image.openshift.io "rails-pgsql-persistent" created
#     buildconfig.build.openshift.io "rails-pgsql-persistent" created
#     deploymentconfig.apps.openshift.io "rails-pgsql-persistent" created
#     persistentvolumeclaim "postgresql" created
#     service "postgresql" created
#     deploymentconfig.apps.openshift.io "postgresql" created
# --> Success
#     Access your application via route 'rails-pgsql-persistent-my-database-app.apps.cluster-munich-e7ab.sandbox1596.opentlc.com' 
#     Build scheduled, use 'oc logs -f bc/rails-pgsql-persistent' to track its progress.
#     Run 'oc status' to view your app.
#+end_example

#+begin_src bash
oc status
#+end_src

#+begin_example
# In project my-database-app on server https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443
# 
# svc/postgresql - 172.30.70.67:5432
#   dc/postgresql deploys openshift/postgresql:10 
#     deployment #1 deployed 56 seconds ago - 1 pod
# 
# http://rails-pgsql-persistent-my-database-app.apps.cluster-munich-e7ab.sandbox1596.opentlc.com (svc/rails-pgsql-persistent)
#   dc/rails-pgsql-persistent deploys istag/rails-pgsql-persistent:latest <-
#     bc/rails-pgsql-persistent source builds https://github.com/sclorg/rails-ex.git on openshift/ruby:2.5 
#       build #1 running for 56 seconds - 67d882b: Merge pull request #113 from liangxia/okd (Honza Horak <hhorak@redhat.com>)
#     deployment #1 waiting on image or update
# 
# 
# 1 info identified, use 'oc status --suggest' to see details.
#+end_example

#+begin_src bash
oc get pvc -n my-database-app
#+end_src

#+begin_example
# NAME         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
# postgresql   Bound    pvc-2dce951c-5490-11ea-9d6d-06399a073c9e   5Gi        RWO            ocs-storagecluster-ceph-rbd   3m59s
#+end_example

#+begin_src bash
oc get pods -n my-database-app
#+end_src

#+begin_example
# NAME                                READY   STATUS      RESTARTS   AGE
# postgresql-1-deploy                 0/1     Completed   0          4m30s
# postgresql-1-qgbt9                  1/1     Running     0          4m20s
# rails-pgsql-persistent-1-build      0/1     Completed   0          4m30s
# rails-pgsql-persistent-1-deploy     0/1     Completed   0          2m16s
# rails-pgsql-persistent-1-hook-pre   0/1     Completed   0          2m8s
# rails-pgsql-persistent-1-wjgl4      1/1     Running     0          108s
#+end_example

#+begin_src bash
oc get route -n my-database-app
#+end_src

#+begin_example
# NAME                     HOST/PORT                                                                                 PATH   SERVICES                 PORT    TERMINATION   WILDCARD
# rails-pgsql-persistent   rails-pgsql-persistent-my-database-app.apps.cluster-munich-e7ab.sandbox1596.opentlc.com          rails-pgsql-persistent   <all>                 None
#+end_example

Copy your rails-pgsql-persistent route (different than above) to a browser window to create articles. You will need to append /articles to the end.

http://rails-pgsql-persistent-my-database-app.apps.cluster-munich-e7ab.sandbox1596.opentlc.com/articles

#+begin_src bash
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
#+end_src

#+begin_src bash
ceph df
#+end_src

#+begin_example
# ceph df
# RAW STORAGE:
#     CLASS     SIZE        AVAIL       USED        RAW USED     %RAW USED 
#     ssd       3.0 TiB     3.0 TiB     268 MiB      3.3 GiB          0.11 
#     TOTAL     3.0 TiB     3.0 TiB     268 MiB      3.3 GiB          0.11 
#  
# POOLS:
#     POOL                                           ID     STORED      OBJECTS     USED        %USED     MAX AVAIL 
#     ocs-storagecluster-cephblockpool                1      88 MiB         107     266 MiB         0       971 GiB 
#     ocs-storagecluster-cephfilesystem-metadata      2     2.2 KiB          22     384 KiB         0       971 GiB 
#     ocs-storagecluster-cephfilesystem-data0         3         0 B           0         0 B         0       971 GiB 
#+end_example

#+begin_src bash
rados df
#+end_src

#+begin_example
# rados df
# POOL_NAME                                     USED OBJECTS CLONES COPIES MISSING_ON_PRIMARY UNFOUND DEGRADED RD_OPS      RD WR_OPS      WR USED COMPR UNDER COMPR 
# ocs-storagecluster-cephblockpool           267 MiB     107      0    321                  0       0        0    233 2.4 MiB   7226 122 MiB        0 B         0 B 
# ocs-storagecluster-cephfilesystem-data0        0 B       0      0      0                  0       0        0      0     0 B      0     0 B        0 B         0 B 
# ocs-storagecluster-cephfilesystem-metadata 384 KiB      22      0     66                  0       0        0   6146 3.0 MiB     45  13 KiB        0 B         0 B 
# 
# total_objects    129
# total_used       3.3 GiB
# total_avail      3.0 TiB
# total_space      3.0 TiB
#+end_example

#+begin_src bash
rbd -p ocs-storagecluster-cephblockpool ls | grep vol
#+end_src

#+begin_example
# rbd -p ocs-storagecluster-cephblockpool ls | grep vol
# csi-vol-2de02001-5490-11ea-8ae4-0a580a800605
# csi-vol-bebc77ce-548a-11ea-8ae4-0a580a800605
#+end_example

#+begin_src bash
exit
#+end_src

** Matching PVs to RBDs

#+begin_src bash
oc get pv -o 'custom-columns=NAME:.spec.claimRef.name,PVNAME:.metadata.name,STORAGECLASS:.spec.storageClassName,VOLUMEHANDLE:.spec.csi.volumeHandle'
#+end_src

#+begin_example
# NAME                      PVNAME                                     STORAGECLASS                  VOLUMEHANDLE
# postgresql                pvc-2dce951c-5490-11ea-9d6d-06399a073c9e   ocs-storagecluster-ceph-rbd   0001-0011-openshift-storage-0000000000000001-2de02001-5490-11ea-8ae4-0a580a800605
# rook-ceph-mon-a           pvc-4415623d-548a-11ea-9d6d-06399a073c9e   gp2                           <none>
# rook-ceph-mon-b           pvc-474f8248-548a-11ea-9d6d-06399a073c9e   gp2                           <none>
# rook-ceph-mon-c           pvc-4a50b91a-548a-11ea-9d6d-06399a073c9e   gp2                           <none>
# ocs-deviceset-0-0-qdn6p   pvc-9aafc6f5-548a-11ea-9d6d-06399a073c9e   gp2                           <none>
# ocs-deviceset-1-0-x7hnz   pvc-9ab17f51-548a-11ea-9d6d-06399a073c9e   gp2                           <none>
# ocs-deviceset-2-0-rxlj7   pvc-9ab3d741-548a-11ea-9d6d-06399a073c9e   gp2                           <none>
# db-noobaa-core-0          pvc-b2bd981b-548a-11ea-9d6d-06399a073c9e   ocs-storagecluster-ceph-rbd   0001-0011-openshift-storage-0000000000000001-bebc77ce-548a-11ea-8ae4-0a580a800605
# mapit-storage             pvc-c17d1521-53ca-11ea-afeb-029a3b7b53fa   gp2                           <none>
#+end_example

#+begin_src bash
CSIVOL=$(oc get pv $(oc get pv | grep my-database-app | awk '{ print $1 }') -o jsonpath='{.spec.csi.volumeHandle}' | cut -d '-' -f 6- | awk '{print "csi-vol-"$1}')
echo $CSIVOL
#+end_src

#+begin_example
csi-vol-2de02001-5490-11ea-8ae4-0a580a800605
#+end_example

#+begin_src bash
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD rbd -p ocs-storagecluster-cephblockpool info $CSIVOL
#+end_src

#+begin_example
# rbd image 'csi-vol-2de02001-5490-11ea-8ae4-0a580a800605':
# 	size 5 GiB in 1280 objects
# 	order 22 (4 MiB objects)
# 	snapshot_count: 0
# 	id: 4177f3094512
# 	block_name_prefix: rbd_data.4177f3094512
# 	format: 2
# 	features: layering
# 	op_features: 
# 	flags: 
# 	create_timestamp: Fri Feb 21 09:54:41 2020
# 	access_timestamp: Fri Feb 21 09:54:41 2020
# 	modify_timestamp: Fri Feb 21 09:54:41 2020
#+end_example

** Create a new OCP application deployment using CephFS volume

#+begin_src bash
oc new-project my-shared-storage
#+end_src

#+begin_example
# Now using project "my-shared-storage" on server "https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443".
# 
# You can add applications to this project with the 'new-app' command. For example, try:
# 
#     oc new-app django-psql-example
# 
# to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:
# 
#     kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
# 
#+end_example

#+begin_src bash
oc new-app openshift/php:7.1~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader
#+end_src

#+begin_example
# --> Found image 8e01e80 (2 months old) in image stream "openshift/php" under tag "7.1" for "openshift/php:7.1"
# 
#     Apache 2.4 with PHP 7.1 
#     ----------------------- 
#     PHP 7.1 available as container is a base platform for building and running various PHP 7.1 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.
# 
#     Tags: builder, php, php71, rh-php71
# 
#     * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created
#       * The resulting image will be pushed to image stream tag "file-uploader:latest"
#       * Use 'oc start-build' to trigger a new build
#     * This image will be deployed in deployment config "file-uploader"
#     * Ports 8080/tcp, 8443/tcp will be load balanced by service "file-uploader"
#       * Other containers can access this service through the hostname "file-uploader"
# 
# --> Creating resources ...
#     imagestream.image.openshift.io "file-uploader" created
#     buildconfig.build.openshift.io "file-uploader" created
#     deploymentconfig.apps.openshift.io "file-uploader" created
#     service "file-uploader" created
# --> Success
#     Build scheduled, use 'oc logs -f bc/file-uploader' to track its progress.
#     Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
#      'oc expose svc/file-uploader' 
#     Run 'oc status' to view your app.
#+end_example

#+begin_src bash
oc logs -f bc/file-uploader -n my-shared-storage
#+end_src

#+begin_example
# Cloning "https://github.com/christianh814/openshift-php-upload-demo" ...
# 	Commit:	288eda3dff43b02f7f7b6b6b6f93396ffdf34cb2 (trying to modularize)
# 	Author:	Christian Hernandez <christian.hernandez@yahoo.com>
# 	Date:	Sun Oct 1 17:15:09 2017 -0700
# Caching blobs under "/var/cache/blobs".
# Getting image source signatures
# Copying blob sha256:48ed3bfd822646e50676cd7606af43e984db141bb1755904362f1eb64684c68a
# Copying blob sha256:ad46648f2433aa416763060fb4baaefe64baeda603d4e572f883c21c5482fea1
# Copying blob sha256:9c9d2ac50b32c2bb48d07fb4e80e81552acc76ade952777f5deb05fdac8f88c6
# Copying blob sha256:16df3eb2f02e0560f09ae997a937d135f747b970d5c2c9bf30b6de874827f3ba
# Copying blob sha256:d327c1598329494579ba3d62999df41f11bff9a2bfad57fb49b30324404ac42a
# Copying config sha256:8e01e80b24124ddc69686179b836d01cb818bad93be2e59e13c3ab7b0ff16196
# Writing manifest to image destination
# Storing signatures
# Generating dockerfile with builder image image-registry.openshift-image-registry.svc:5000/openshift/php@sha256:289ef5852151a1a3ea4fdf2581a86a60ada0719e54a5ddd78f5252742e6d2b8d
# STEP 1: FROM image-registry.openshift-image-registry.svc:5000/openshift/php@sha256:289ef5852151a1a3ea4fdf2581a86a60ada0719e54a5ddd78f5252742e6d2b8d
# STEP 2: LABEL "io.openshift.build.commit.date"="Sun Oct 1 17:15:09 2017 -0700" "io.openshift.build.commit.id"="288eda3dff43b02f7f7b6b6b6f93396ffdf34cb2" "io.openshift.build.commit.ref"="master" "io.openshift.build.commit.message"="trying to modularize" "io.openshift.build.source-location"="https://github.com/christianh814/openshift-php-upload-demo" "io.openshift.build.image"="image-registry.openshift-image-registry.svc:5000/openshift/php@sha256:289ef5852151a1a3ea4fdf2581a86a60ada0719e54a5ddd78f5252742e6d2b8d" "io.openshift.build.commit.author"="Christian Hernandez <christian.hernandez@yahoo.com>"
# STEP 3: ENV OPENSHIFT_BUILD_NAME="file-uploader-1" OPENSHIFT_BUILD_NAMESPACE="my-shared-storage" OPENSHIFT_BUILD_SOURCE="https://github.com/christianh814/openshift-php-upload-demo" OPENSHIFT_BUILD_COMMIT="288eda3dff43b02f7f7b6b6b6f93396ffdf34cb2"
# STEP 4: USER root
# STEP 5: COPY upload/src /tmp/src
# STEP 6: RUN chown -R 1001:0 /tmp/src
# time="2020-02-21T10:17:07Z" level=warning msg="pkg/chroot: error unmounting \"/tmp/buildah162176086/mnt/rootfs\": error checking if \"/tmp/buildah162176086/mnt/rootfs/sys/fs/cgroup/rdma\" is mounted: no such file or directory"
# time="2020-02-21T10:17:07Z" level=warning msg="pkg/bind: error unmounting \"/tmp/buildah162176086/mnt/rootfs\": error checking if \"/tmp/buildah162176086/mnt/rootfs/sys/fs/cgroup/rdma\" is mounted: no such file or directory"
# STEP 7: USER 1001
# STEP 8: RUN /usr/libexec/s2i/assemble
# ---> Installing application source...
# => sourcing 20-copy-config.sh ...
# ---> 10:17:08     Processing additional arbitrary httpd configuration provided by s2i ...
# => sourcing 00-documentroot.conf ...
# => sourcing 50-mpm-tuning.conf ...
# => sourcing 40-ssl-certs.sh ...
# time="2020-02-21T10:17:08Z" level=warning msg="pkg/chroot: error unmounting \"/tmp/buildah075961883/mnt/rootfs\": error checking if \"/tmp/buildah075961883/mnt/rootfs/sys/fs/cgroup/rdma\" is mounted: no such file or directory"
# time="2020-02-21T10:17:08Z" level=warning msg="pkg/bind: error unmounting \"/tmp/buildah075961883/mnt/rootfs\": error checking if \"/tmp/buildah075961883/mnt/rootfs/sys/fs/cgroup/rdma\" is mounted: no such file or directory"
# STEP 9: CMD /usr/libexec/s2i/run
# STEP 10: COMMIT temp.builder.openshift.io/my-shared-storage/file-uploader-1:1a68497e
# Getting image source signatures
# Copying blob sha256:a551fb857b61df88ae64cded378611456294ee7143bc068eb3a9ee1a29934658
# Copying blob sha256:08c4aa164cbaadfb32414e39d9b252a5382b9db1dd8e028fb11805074edbe608
# Copying blob sha256:7c6c57a17fb3de2b83515a0e33782d6d769fa90725da4eb75311e5ffbf601e49
# Copying blob sha256:c3b514cae9b11a6aadc78fdbb3d7b71226759927a8149f97ea400628744ad2c9
# Copying blob sha256:67ce50f5984a7a2ae8efd16c0dd2780e95982256be555d4e9a272357783ec338
# Copying blob sha256:317528d934869bca844c16a7e38e58dec3a5f56904f9c8ff187024c13e3ca6f4
# Copying config sha256:6e45a05242cee47e34d805f6e20c7cb907e8416549a77f27da598ece91b83b6c
# Writing manifest to image destination
# Storing signatures
# 6e45a05242cee47e34d805f6e20c7cb907e8416549a77f27da598ece91b83b6c
# 
# Pushing image image-registry.openshift-image-registry.svc:5000/my-shared-storage/file-uploader:latest ...
# Getting image source signatures
# Copying blob sha256:317528d934869bca844c16a7e38e58dec3a5f56904f9c8ff187024c13e3ca6f4
# Copying blob sha256:d327c1598329494579ba3d62999df41f11bff9a2bfad57fb49b30324404ac42a
# Copying blob sha256:16df3eb2f02e0560f09ae997a937d135f747b970d5c2c9bf30b6de874827f3ba
# Copying blob sha256:48ed3bfd822646e50676cd7606af43e984db141bb1755904362f1eb64684c68a
# Copying blob sha256:9c9d2ac50b32c2bb48d07fb4e80e81552acc76ade952777f5deb05fdac8f88c6
# Copying blob sha256:ad46648f2433aa416763060fb4baaefe64baeda603d4e572f883c21c5482fea1
# Copying config sha256:6e45a05242cee47e34d805f6e20c7cb907e8416549a77f27da598ece91b83b6c
# Writing manifest to image destination
# Storing signatures
# Successfully pushed image-registry.openshift-image-registry.svc:5000/my-shared-storage/file-uploader@sha256:379998b05491523b55ccbb219d9347a9c0626a775e1a344dfb6a0d7c71bd913a
# Push successful
#+end_example

#+begin_src bash
oc expose svc/file-uploader -n my-shared-storage
#+end_src

#+begin_example
# route.route.openshift.io/file-uploader exposed
#+end_example

#+begin_src bash
oc scale --replicas=3 dc/file-uploader -n my-shared-storage
#+end_src

#+begin_example
# deploymentconfig.apps.openshift.io/file-uploader scaled
#+end_example

#+begin_src bash
oc get pods -n my-shared-storage
#+end_src

#+begin_example
# NAME                     READY   STATUS              RESTARTS   AGE
# file-uploader-1-bdqs8    0/1     ContainerCreating   0          8s
# file-uploader-1-build    0/1     Completed           0          2m36s
# file-uploader-1-deploy   0/1     Completed           0          105s
# file-uploader-1-fqdrx    0/1     ContainerCreating   0          8s
# file-uploader-1-q7ngq    1/1     Running             0          97s
#+end_example
#+begin_src bash
oc get pods -n my-shared-storage
#+end_src

#+begin_example
# NAME                     READY   STATUS      RESTARTS   AGE
# file-uploader-1-bdqs8    1/1     Running     0          29s
# file-uploader-1-build    0/1     Completed   0          2m57s
# file-uploader-1-deploy   0/1     Completed   0          2m6s
# file-uploader-1-fqdrx    1/1     Running     0          29s
# file-uploader-1-q7ngq    1/1     Running     0          118s
#+end_example

#+begin_src bash
oc set volume dc/file-uploader --add --name=my-shared-storage \
-t pvc --claim-mode=ReadWriteMany --claim-size=1Gi \
--claim-name=my-shared-storage --claim-class=ocs-storagecluster-cephfs \
--mount-path=/opt/app-root/src/uploaded \
-n my-shared-storage
#+end_src

#+begin_example
# deploymentconfig.apps.openshift.io/file-uploader volume updated
#+end_example

#+begin_src bash
oc set volume -h
#+end_src

#+begin_example
# Update volumes on a pod template
# 
#  This command can add, update or remove volumes from containers for any object that has a pod template (deployment
# configs, replication controllers, or pods). You can list volumes in pod or any object that has a pod template. You can
# specify a single object or multiple, and alter volumes on all containers or just those that match a given name.
# 
#  If you alter a volume setting on a deployment config, a deployment will be triggered. Changing a replication controller
# will not affect running pods, and you cannot change a pod's volumes once it has been created.
# 
#  Volume types include:
# 
#   *  emptydir (empty directory) default : A directory allocated when the pod is created on a local host, is removed when
# the pod is deleted and is not copied across servers
#   *  hostdir (host directory): A directory with specific path on any host (requires elevated privileges)
#   *  persistentvolumeclaim or pvc (persistent volume claim): Link the volume directory in the container to a persistent
# volume claim you have allocated by name - a persistent volume claim is a request to allocate storage. Note that if your
# claim hasn't been bound, your pods will not start.
#   *  secret (mounted secret): Secret volumes mount a named secret to the provided directory.
# 
#  For descriptions on other volume types, see https://docs.openshift.com
# 
# Aliases:
# volumes, volume
# 
# Usage:
#   oc set volumes RESOURCE/NAME --add|--remove|--list [flags]
# 
# Examples:
#   # List volumes defined on all deployment configs in the current project
#   oc set volume dc --all
#   
#   # Add a new empty dir volume to deployment config (dc) 'myapp' mounted under
#   # /var/lib/myapp
#   oc set volume dc/myapp --add --mount-path=/var/lib/myapp
#   
#   # Use an existing persistent volume claim (pvc) to overwrite an existing volume 'v1'
#   oc set volume dc/myapp --add --name=v1 -t pvc --claim-name=pvc1 --overwrite
#   
#   # Remove volume 'v1' from deployment config 'myapp'
#   oc set volume dc/myapp --remove --name=v1
#   
#   # Create a new persistent volume claim that overwrites an existing volume 'v1'
#   oc set volume dc/myapp --add --name=v1 -t pvc --claim-size=1G --overwrite
#   
#   # Change the mount point for volume 'v1' to /data
#   oc set volume dc/myapp --add --name=v1 -m /data --overwrite
#   
#   # Modify the deployment config by removing volume mount "v1" from container "c1"
#   # (and by removing the volume "v1" if no other containers have volume mounts that reference it)
#   oc set volume dc/myapp --remove --name=v1 --containers=c1
#   
#   # Add new volume based on a more complex volume source (AWS EBS, GCE PD,
#   # Ceph, Gluster, NFS, ISCSI, ...)
#   oc set volume dc/myapp --add -m /data --source=<json-string>
# 
# Options:
#       --add=false: If true, add volume and/or volume mounts for containers
#       --all=false: If true, select all resources in the namespace of the specified resource types
#       --allow-missing-template-keys=true: If true, ignore any errors in templates when a field or map key is missing in
# the template. Only applies to golang and jsonpath output formats.
#       --claim-class='': StorageClass to use for the persistent volume claim
#       --claim-mode='ReadWriteOnce': Set the access mode of the claim to be created. Valid values are ReadWriteOnce
# (rwo), ReadWriteMany (rwm), or ReadOnlyMany (rom)
#       --claim-name='': Persistent volume claim name. Must be provided for persistentVolumeClaim volume type
#       --claim-size='': If specified along with a persistent volume type, create a new claim with the given size in
# bytes. Accepts SI notation: 10, 10G, 10Gi
#       --configmap-name='': Name of the persisted config map. Must be provided for configmap volume type
#       --confirm=false: If true, confirm that you really want to remove multiple volumes
#   -c, --containers='*': The names of containers in the selected pod templates to change - may use wildcards
#       --default-mode='': The default mode bits to create files with. Can be between 0000 and 0777. Defaults to 0644.
#       --dry-run=false: If true, only print the object that would be sent, without sending it.
#   -f, --filename=[]: Filename, directory, or URL to files to use to edit the resource
#   -k, --kustomize='': Process the kustomization directory. This flag can't be used together with -f or -R.
#       --local=false: If true, set image will NOT contact api-server but run locally.
#   -m, --mount-path='': Mount path inside the container. Optional param for --add or --remove
#       --name='': Name of the volume. If empty, auto generated for add operation
#   -o, --output='': Output format. One of:
# json|yaml|name|go-template|go-template-file|template|templatefile|jsonpath|jsonpath-file.
#       --overwrite=false: If true, replace existing volume source with the provided name and/or volume mount for the
# given resource
#       --path='': Host path. Must be provided for hostPath volume type
#       --read-only=false: Mount volume as ReadOnly. Optional param for --add or --remove
#   -R, --recursive=false: Process the directory used in -f, --filename recursively. Useful when you want to manage
# related manifests organized within the same directory.
#       --remove=false: If true, remove volume and/or volume mounts for containers
#       --secret-name='': Name of the persisted secret. Must be provided for secret volume type
#   -l, --selector='': Selector (label query) to filter on
#       --source='': Details of volume source as json string. This can be used if the required volume type is not
# supported by --type option. (e.g.: '{"nfs": {"path": "/tmp","server":"172.17.0.2"}}')
#       --sub-path='': Path within the local volume from which the container's volume should be mounted. Optional param
# for --add or --remove
#       --template='': Template string or path to template file to use when -o=go-template, -o=go-template-file. The
# template format is golang templates [http://golang.org/pkg/text/template/#pkg-overview].
#   -t, --type='': Type of the volume source for add operation. Supported options: emptyDir, hostPath, secret, configmap,
# persistentVolumeClaim
# 
# Use "oc options" for a list of global command-line options (applies to all commands).
#+end_example

#+begin_src bash
oc get pvc -n my-shared-storage
#+end_src

#+begin_example
# NAME                STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                AGE
# my-shared-storage   Bound    pvc-c2f1fecd-5493-11ea-99d2-0a84b9320ef6   1Gi        RWX            ocs-storagecluster-cephfs   86s
#+end_example

Try it out in your file uploader web application using your browser. Upload new files.

#+begin_src bash
oc get route file-uploader -n my-shared-storage
#+end_src

#+begin_example
# NAME            HOST/PORT                                                                          PATH   SERVICES        PORT       TERMINATION   WILDCARD
# file-uploader   file-uploader-my-shared-storage.apps.cluster-munich-e7ab.sandbox1596.opentlc.com          file-uploader   8080-tcp                 None
#+end_example

The following does NOT work...

#+begin_src bash
oc get route file-uploader -n my-shared-storage -o jsonpath --template="{.spec.host}"
#+end_src

http://file-uploader-my-shared-storage.apps.cluster-munich-e7ab.sandbox1596.opentlc.com/

** Using OCS for Prometheus Metrics

#+begin_src bash
oc get pods,pvc -n openshift-monitoring
#+end_src

#+begin_example
# NAME                                               READY   STATUS    RESTARTS   AGE
# pod/alertmanager-main-0                            3/3     Running   0          18h
# pod/alertmanager-main-1                            3/3     Running   0          18h
# pod/alertmanager-main-2                            3/3     Running   0          18h
# pod/cluster-monitoring-operator-585c8c44d9-hzmnw   1/1     Running   0          40h
# pod/grafana-69685f986d-rl757                       2/2     Running   0          18h
# pod/kube-state-metrics-7c884764fd-jdf57            3/3     Running   0          18h
# pod/node-exporter-d49v5                            2/2     Running   0          19h
# pod/node-exporter-fk267                            2/2     Running   0          40h
# pod/node-exporter-gqgjk                            2/2     Running   0          40h
# pod/node-exporter-hgtjd                            2/2     Running   0          19h
# pod/node-exporter-lkxvx                            2/2     Running   0          127m
# pod/node-exporter-nbkxw                            2/2     Running   0          19h
# pod/node-exporter-p7vrw                            2/2     Running   0          40h
# pod/node-exporter-prbc5                            2/2     Running   0          127m
# pod/node-exporter-r9ckd                            2/2     Running   0          127m
# pod/node-exporter-vm4np                            2/2     Running   0          40h
# pod/node-exporter-xddfk                            2/2     Running   0          23h
# pod/node-exporter-xhwb8                            2/2     Running   0          40h
# pod/openshift-state-metrics-7c76b98c77-kxdjp       3/3     Running   0          40h
# pod/prometheus-adapter-986777885-d7rjf             1/1     Running   0          16h
# pod/prometheus-adapter-986777885-vdcmj             1/1     Running   0          16h
# pod/prometheus-k8s-0                               6/6     Running   1          18h
# pod/prometheus-k8s-1                               6/6     Running   1          18h
# pod/prometheus-operator-7c8568cc64-x5vtt           1/1     Running   0          96m
# pod/telemeter-client-944599596-c5jzs               3/3     Running   0          18h
#+end_example

** Modifying your Prometheus environment

#+begin_src bash
oc -n openshift-monitoring get configmap cluster-monitoring-config
#+end_src

#+begin_example
# NAME                        DATA   AGE
# cluster-monitoring-config   1      18h
#+end_example

If you are missing the ConfigMap, create it using this command

#+begin_src bash
oc apply -f /opt/app-root/src/support/ocslab_cluster-monitoring-noinfra.yaml
#+end_src

If the ConfigMap already exists because of completing prior module OpenShift Infrastructure Nodes, you will apply changes to the existing ConfigMap.

#+begin_src bash
cat ~/test/support/ocslab_cluster-monitoring-withinfra.yaml
#+end_src

#+begin_example
# apiVersion: v1
# kind: ConfigMap
# metadata:
#   name: cluster-monitoring-config
#   namespace: openshift-monitoring
# data:
#   config.yaml: |+
#     alertmanagerMain:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#       volumeClaimTemplate:
#         metadata:
#           name: alertmanager
#         spec:
#           storageClassName: ocs-storagecluster-ceph-rbd
#           resources:
#             requests:
#               storage: 40Gi
#     prometheusK8s:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#       volumeClaimTemplate:
#         metadata:
#           name: prometheusdb
#         spec:
#           storageClassName: ocs-storagecluster-ceph-rbd
#           resources:
#             requests:
#               storage: 40Gi
#     prometheusOperator:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#     grafana:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#     k8sPrometheusAdapter:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#     kubeStateMetrics:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#     telemeterClient:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#+end_example

#+begin_src bash
oc apply -f ~/test/support/ocslab_cluster-monitoring-withinfra.yaml
#+end_src

#+begin_example
# Warning: oc apply should be used on resource created by either oc create --save-config or oc apply
# configmap/cluster-monitoring-config configured
#+end_example

#+begin_src bash
oc -n openshift-monitoring get configmap cluster-monitoring-config -o yaml
#+end_src

#+begin_example
# apiVersion: v1
# data:
#   config.yaml: |
#     alertmanagerMain:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#       volumeClaimTemplate:
#         metadata:
#           name: alertmanager
#         spec:
#           storageClassName: ocs-storagecluster-ceph-rbd
#           resources:
#             requests:
#               storage: 40Gi
#     prometheusK8s:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#       volumeClaimTemplate:
#         metadata:
#           name: prometheusdb
#         spec:
#           storageClassName: ocs-storagecluster-ceph-rbd
#           resources:
#             requests:
#               storage: 40Gi
#     prometheusOperator:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#     grafana:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#     k8sPrometheusAdapter:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#     kubeStateMetrics:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
#     telemeterClient:
#       nodeSelector:
#         node-role.kubernetes.io/infra: ""
# kind: ConfigMap
# metadata:
#   annotations:
#     kubectl.kubernetes.io/last-applied-configuration: |
#       {"apiVersion":"v1","data":{"config.yaml":"alertmanagerMain:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  volumeClaimTemplate:\n    metadata:\n      name: alertmanager\n    spec:\n      storageClassName: ocs-storagecluster-ceph-rbd\n      resources:\n        requests:\n          storage: 40Gi\nprometheusK8s:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n  volumeClaimTemplate:\n    metadata:\n      name: prometheusdb\n    spec:\n      storageClassName: ocs-storagecluster-ceph-rbd\n      resources:\n        requests:\n          storage: 40Gi\nprometheusOperator:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\ngrafana:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\nk8sPrometheusAdapter:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\nkubeStateMetrics:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\ntelemeterClient:\n  nodeSelector:\n    node-role.kubernetes.io/infra: \"\"\n"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"cluster-monitoring-config","namespace":"openshift-monitoring"}}
#   creationTimestamp: "2020-02-20T15:48:00Z"
#   name: cluster-monitoring-config
#   namespace: openshift-monitoring
#   resourceVersion: "756412"
#   selfLink: /api/v1/namespaces/openshift-monitoring/configmaps/cluster-monitoring-config
#   uid: 5f234086-53f8-11ea-bc7a-06399a073c9e
#+end_example

#+begin_src bash
oc get pods,pvc -n openshift-monitoring
#+end_src

#+begin_example
# NAME                                               READY   STATUS    RESTARTS   AGE
# pod/alertmanager-main-0                            3/3     Running   0          48s
# pod/alertmanager-main-1                            3/3     Running   0          48s
# pod/alertmanager-main-2                            3/3     Running   0          48s
# pod/cluster-monitoring-operator-585c8c44d9-hzmnw   1/1     Running   0          40h
# pod/grafana-69685f986d-rl757                       2/2     Running   0          18h
# pod/kube-state-metrics-7c884764fd-jdf57            3/3     Running   0          18h
# pod/node-exporter-d49v5                            2/2     Running   0          19h
# pod/node-exporter-fk267                            2/2     Running   0          40h
# pod/node-exporter-gqgjk                            2/2     Running   0          40h
# pod/node-exporter-hgtjd                            2/2     Running   0          19h
# pod/node-exporter-lkxvx                            2/2     Running   0          131m
# pod/node-exporter-nbkxw                            2/2     Running   0          19h
# pod/node-exporter-p7vrw                            2/2     Running   0          40h
# pod/node-exporter-prbc5                            2/2     Running   0          132m
# pod/node-exporter-r9ckd                            2/2     Running   0          131m
# pod/node-exporter-vm4np                            2/2     Running   0          40h
# pod/node-exporter-xddfk                            2/2     Running   0          23h
# pod/node-exporter-xhwb8                            2/2     Running   0          40h
# pod/openshift-state-metrics-7c76b98c77-kxdjp       3/3     Running   0          40h
# pod/prometheus-adapter-986777885-d7rjf             1/1     Running   0          16h
# pod/prometheus-adapter-986777885-vdcmj             1/1     Running   0          16h
# pod/prometheus-k8s-0                               6/6     Running   1          37s
# pod/prometheus-k8s-1                               6/6     Running   1          37s
# pod/prometheus-operator-7c8568cc64-x5vtt           1/1     Running   0          101m
# pod/telemeter-client-944599596-c5jzs               3/3     Running   0          18h
# 
# NAME                                                     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
# persistentvolumeclaim/alertmanager-alertmanager-main-0   Bound    pvc-39b2a3f2-5496-11ea-9d6d-06399a073c9e   40Gi       RWO            ocs-storagecluster-ceph-rbd   48s
# persistentvolumeclaim/alertmanager-alertmanager-main-1   Bound    pvc-39b9f86a-5496-11ea-9d6d-06399a073c9e   40Gi       RWO            ocs-storagecluster-ceph-rbd   48s
# persistentvolumeclaim/alertmanager-alertmanager-main-2   Bound    pvc-39c1d058-5496-11ea-9d6d-06399a073c9e   40Gi       RWO            ocs-storagecluster-ceph-rbd   48s
# persistentvolumeclaim/prometheusdb-prometheus-k8s-0      Bound    pvc-3fc8be99-5496-11ea-9d6d-06399a073c9e   40Gi       RWO            ocs-storagecluster-ceph-rbd   37s
# persistentvolumeclaim/prometheusdb-prometheus-k8s-1      Bound    pvc-3fe4181c-5496-11ea-9d6d-06399a073c9e   40Gi       RWO            ocs-storagecluster-ceph-rbd   37s
#+end_example

** Using the Multi-Cloud-Gateway
*** Checking on the MCG status

#+begin_src bash
oc project openshift-storage
#+end_src

#+begin_example
# Now using project "openshift-storage" on server "https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443".
#+end_example

#+begin_src bash
noobaa status -n openshift-storage
#+end_src

#+begin_example
# bash: noobaa: command not found
#+end_example

https://github.com/noobaa/noobaa-operator

#+begin_src bash
wget https://github.com/noobaa/noobaa-operator/releases/download/v2.0.9/noobaa-linux-v2.0.9; mv noobaa-linux-* noobaa; chmod +x noobaa
#+end_src

#+begin_example
# --2020-02-21 10:47:41--  https://github.com/noobaa/noobaa-operator/releases/download/v2.0.9/noobaa-linux-v2.0.9
# Resolving github.com (github.com)... 140.82.113.3
# Connecting to github.com (github.com)|140.82.113.3|:443... connected.
# HTTP request sent, awaiting response... 302 Found
# Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/194805859/ab289280-11e4-11ea-9078-ea2a84a4db6b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200221T104741Z&X-Amz-Expires=300&X-Amz-Signature=a7546d695fc2ade214345ee1f9781582ce3fcf532cf5205e673463fdf431a1ca&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dnoobaa-linux-v2.0.9&response-content-type=application%2Foctet-stream [following]
# --2020-02-21 10:47:41--  https://github-production-release-asset-2e65be.s3.amazonaws.com/194805859/ab289280-11e4-11ea-9078-ea2a84a4db6b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200221%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200221T104741Z&X-Amz-Expires=300&X-Amz-Signature=a7546d695fc2ade214345ee1f9781582ce3fcf532cf5205e673463fdf431a1ca&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dnoobaa-linux-v2.0.9&response-content-type=application%2Foctet-stream
# Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.46.148
# Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.46.148|:443... connected.
# HTTP request sent, awaiting response... 200 OK
# Length: 52197598 (50M) [application/octet-stream]
# Saving to: ‘noobaa-linux-v2.0.9’
# 
# 100%[====================================================================================>] 52,197,598  19.6MB/s   in 2.5s   
# 
# 2020-02-21 10:47:44 (19.6 MB/s) - ‘noobaa-linux-v2.0.9’ saved [52197598/52197598]
# 
#+end_example

#+begin_src bash
ls -lt
#+end_src

#+begin_example
# total 51140
# -rw-r--r--. 1 lab-user users   101545 Feb 21 10:42 ops-road-show.org
# drwxrwxr-x. 7 lab-user users      137 Feb 20 16:01 docs/
# drwxrwxr-x. 2 lab-user users       40 Feb 20 16:01 dns_update/
# drwxrwxr-x. 2 lab-user users       36 Feb 20 16:01 cloudformation/
# -rw-r--r--. 1 lab-user users     4101 Feb 20 16:01 infra.yaml
# drwxrwxr-x. 2 lab-user users      115 Feb 20 14:41 packer-host/
# drwxrwxr-x. 3 lab-user users       76 Feb 20 14:41 packer/
# drwxrwxr-x. 2 lab-user users     4096 Feb 20 14:39 support/
# -rw-r--r--. 1 lab-user users     1105 Feb 20 13:56 ops-road-show.org~
# -rw-r--r--. 1 lab-user users    32653 Feb 20 13:55 index.html
# drwxr-xr-x. 2 lab-user users       23 Feb 20 13:55 testdir/
# -rw-r--r--. 1 lab-user users       18 Feb 20 13:55 test.html
# -rw-r--r--. 1 lab-user users       63 Feb 20 13:55 404.html
# -rw-r--r--. 1 lab-user users     1062 Feb 20 13:55 LICENSE
# -rw-r--r--. 1 lab-user users       68 Feb 20 13:55 README.md
# -rwxr-xr-x. 1 lab-user users 52197598 Nov 28 11:41 noobaa*
#+end_example

#+begin_src bash
./noobaa
#+end_src

#+begin_example
# #                       # 
# #    /~~\___~___/~~\    # 
# #   |               |   # 
# #    \~~|\     /|~~/    # 
# #        \|   |/        # 
# #         |   |         # 
# #         \~~~/         # 
# #                       # 
# #      N O O B A A      #
# 
# Install:
#   install      Install the operator and create the noobaa system
#   uninstall    Uninstall the operator and delete the system
#   status       Status of the operator and the system
# 
# Manage:
#   backingstore Manage backing stores
#   bucketclass  Manage bucket classes
#   obc          Manage object bucket claims
#   diagnose     Collect diagnostics
#   ui           Open the NooBaa UI
# 
# Advanced:
#   operator     Deployment using operator
#   system       Manage noobaa systems
#   api          Make api call
#   bucket       Manage noobaa buckets
#   pvstore      Manage noobaa pv store
#   crd          Deployment of CRDs
#   olm          OLM related commands
# 
# Other Commands:
#   completion   Generates bash completion scripts
#   options      Print the list of global flags
#   version      Show version
# 
# Use "noobaa <command> --help" for more information about a given command.
#+end_example

#+begin_src bash
./noobaa version
#+end_src

#+begin_example
# INFO[0000] CLI version: 2.0.9                           
# INFO[0000] noobaa-image: noobaa/noobaa-core:5.2.11      
# INFO[0000] operator-image: noobaa/noobaa-operator:2.0.9 
#+end_example

#+begin_src bash
./noobaa status -n openshift-storage
#+end_src

#+begin_example
# INFO[0000] CLI version: 2.0.9                           
# INFO[0000] noobaa-image: noobaa/noobaa-core:5.2.11      
# INFO[0000] operator-image: noobaa/noobaa-operator:2.0.9 
# INFO[0000] Namespace: openshift-storage                 
# INFO[0000]                                              
# INFO[0000] CRD Status:                                  
# INFO[0000] ✅ Exists: CustomResourceDefinition "noobaas.noobaa.io" 
# INFO[0000] ✅ Exists: CustomResourceDefinition "backingstores.noobaa.io" 
# INFO[0000] ✅ Exists: CustomResourceDefinition "bucketclasses.noobaa.io" 
# INFO[0000] ✅ Exists: CustomResourceDefinition "objectbucketclaims.objectbucket.io" 
# INFO[0000] ✅ Exists: CustomResourceDefinition "objectbuckets.objectbucket.io" 
# INFO[0000]                                              
# INFO[0000] Operator Status:                             
# INFO[0000] ✅ Exists: Namespace "openshift-storage"      
# INFO[0000] ✅ Exists: ServiceAccount "noobaa"            
# INFO[0000] ✅ Exists: Role "ocs-operator.v4.2.1-tsmzq"   
# INFO[0000] ✅ Exists: RoleBinding "ocs-operator.v4.2.1-tsmzq-noobaa-kc42r" 
# INFO[0000] ✅ Exists: ClusterRole "ocs-operator.v4.2.1-7z597" 
# INFO[0000] ✅ Exists: ClusterRoleBinding "ocs-operator.v4.2.1-7z597-noobaa-6ql9t" 
# INFO[0000] ✅ Exists: Deployment "noobaa-operator"       
# INFO[0000]                                              
# INFO[0000] System Status:                               
# INFO[0000] ✅ Exists: NooBaa "noobaa"                    
# INFO[0000] ✅ Exists: StatefulSet "noobaa-core"          
# INFO[0000] ✅ Exists: Service "noobaa-mgmt"              
# INFO[0000] ✅ Exists: Service "s3"                       
# INFO[0000] ✅ Exists: Secret "noobaa-server"             
# INFO[0000] ✅ Exists: Secret "noobaa-operator"           
# INFO[0000] ✅ Exists: Secret "noobaa-admin"              
# INFO[0000] ✅ Exists: StorageClass "openshift-storage.noobaa.io" 
# INFO[0000] ✅ Exists: BucketClass "noobaa-default-bucket-class" 
# INFO[0000] ✅ (Optional) Exists: BackingStore "noobaa-default-backing-store" 
# INFO[0000] ✅ (Optional) Exists: CredentialsRequest "noobaa-cloud-creds" 
# INFO[0000] ✅ (Optional) Exists: PrometheusRule "noobaa-prometheus-rules" 
# INFO[0000] ✅ (Optional) Exists: ServiceMonitor "noobaa-service-monitor" 
# INFO[0000] ✅ (Optional) Exists: Route "noobaa-mgmt"     
# INFO[0000] ✅ (Optional) Exists: Route "s3"              
# INFO[0000] ✅ Exists: PersistentVolumeClaim "db-noobaa-core-0" 
# INFO[0000] ✅ System Phase is "Ready"                    
# INFO[0000] ✅ Exists:  "noobaa-admin"                    
# 
# #------------------#
# #- Mgmt Addresses -#
# #------------------#
# 
# ExternalDNS : [https://noobaa-mgmt-openshift-storage.apps.cluster-munich-e7ab.sandbox1596.opentlc.com https://ab2c15a6f548a11ea9d6d06399a073c9-1074882543.us-east-2.elb.amazonaws.com:443]
# ExternalIP  : []
# NodePorts   : [https://10.0.147.203:30848]
# InternalDNS : [https://noobaa-mgmt.openshift-storage.svc:443]
# InternalIP  : [https://172.30.160.54:443]
# PodPorts    : [https://10.128.6.11:8443]
# 
# #--------------------#
# #- Mgmt Credentials -#
# #--------------------#
# 
# email    : admin@noobaa.io
# password : UQiMQoHQzKyow+WpAXfUfg==
# 
# #----------------#
# #- S3 Addresses -#
# #----------------#
# 
# ExternalDNS : [https://s3-openshift-storage.apps.cluster-munich-e7ab.sandbox1596.opentlc.com https://ab2c5acc9548a11ea9d6d06399a073c9-2093889708.us-east-2.elb.amazonaws.com:443]
# ExternalIP  : []
# NodePorts   : [https://10.0.147.203:31216]
# InternalDNS : [https://s3.openshift-storage.svc:443]
# InternalIP  : [https://172.30.22.181:443]
# PodPorts    : [https://10.128.6.11:6443]
# 
# #------------------#
# #- S3 Credentials -#
# #------------------#
# 
# AWS_ACCESS_KEY_ID     : jiAP5RESdV5GkrBKo0Va
# AWS_SECRET_ACCESS_KEY : P4Z584DFatacTZM/1peJmnlhfZY+aIDDK6Q1OQ6h
# 
# #------------------#
# #- Backing Stores -#
# #------------------#
# 
# NAME                           TYPE     TARGET-BUCKET                                               PHASE   AGE       
# noobaa-default-backing-store   aws-s3   noobaa-backing-store-cf458652-82d6-4c6f-93e8-9fbcfc3151e2   Ready   1h32m3s   
# 
# #------------------#
# #- Bucket Classes -#
# #------------------#
# 
# NAME                          PLACEMENT                                                             PHASE   AGE       
# noobaa-default-bucket-class   {Tiers:[{Placement: BackingStores:[noobaa-default-backing-store]}]}   Ready   1h32m3s   
# 
# #-----------------#
# #- Bucket Claims -#
# #-----------------#
# 
# No OBC's found.
#+end_example

*** Creating an Object Bucket Claim

#+begin_src bash
./noobaa obc create test21obc -n openshift-storage
#+end_src

#+begin_example
# INFO[0000] ✅ Created: ObjectBucketClaim "test21obc"     
# INFO[0000]                                              
# INFO[0000] NOTE:                                        
# INFO[0000]   - This command has finished applying changes to the cluster. 
# INFO[0000]   - From now on, it only loops and reads the status, to monitor the operator work. 
# INFO[0000]   - You may Ctrl-C at any time to stop the loop and watch it manually. 
# INFO[0000]                                              
# INFO[0000] OBC Wait Ready:                              
# INFO[0000] ⏳ OBC "test21obc" Phase is ""                
# INFO[0003] ✅ OBC "test21obc" Phase is Bound             
# INFO[0003]                                              
# INFO[0003]                                              
# INFO[0003] ✅ Exists: ObjectBucketClaim "test21obc"      
# INFO[0003] ✅ Exists: ObjectBucket "obc-openshift-storage-test21obc" 
# INFO[0003] ✅ Exists: ConfigMap "test21obc"              
# INFO[0003] ✅ Exists: Secret "test21obc"                 
# INFO[0003] ✅ Exists: StorageClass "openshift-storage.noobaa.io" 
# INFO[0003] ✅ Exists: BucketClass "noobaa-default-bucket-class" 
# INFO[0003] ✅ Exists: NooBaa "noobaa"                    
# INFO[0003] ✅ Exists: Service "noobaa-mgmt"              
# INFO[0003] ✅ Exists: Secret "noobaa-operator"           
# INFO[0003] ✈️  RPC: bucket.read_bucket() Request: {Name:test21obc-0f9620af-2799-4671-a4f6-90218c4e2f09} 
# INFO[0003] ✅ RPC: bucket.read_bucket() Response OK: took 8.3ms 
# 
# ObjectBucketClaim info:
#   Phase                  : Bound
#   ObjectBucketClaim      : kubectl get -n openshift-storage objectbucketclaim test21obc
#   ConfigMap              : kubectl get -n openshift-storage configmap test21obc
#   Secret                 : kubectl get -n openshift-storage secret test21obc
#   ObjectBucket           : kubectl get objectbucket obc-openshift-storage-test21obc
#   StorageClass           : kubectl get storageclass openshift-storage.noobaa.io
#   BucketClass            : kubectl get -n openshift-storage bucketclass noobaa-default-bucket-class
# 
# Connection info:
#   BUCKET_HOST            : 10.0.147.203
#   BUCKET_NAME            : test21obc-0f9620af-2799-4671-a4f6-90218c4e2f09
#   BUCKET_PORT            : 31216
#   AWS_ACCESS_KEY_ID      : IUGPUD5FPwYZnwA2zKfz
#   AWS_SECRET_ACCESS_KEY  : LLAir/qA3HHr1DM2s4gfdlUvjLHKttswO6zYyoWe
# 
# Shell commands:
#   AWS S3 Alias           : alias s3='AWS_ACCESS_KEY_ID=IUGPUD5FPwYZnwA2zKfz AWS_SECRET_ACCESS_KEY=LLAir/qA3HHr1DM2s4gfdlUvjLHKttswO6zYyoWe aws s3 --no-verify-ssl --endpoint-url https://10.0.147.203:31216'
# 
# Bucket status:
#   Name                   : test21obc-0f9620af-2799-4671-a4f6-90218c4e2f09
#   Type                   : REGULAR
#   Mode                   : OPTIMAL
#   ResiliencyStatus       : OPTIMAL
#   QuotaStatus            : QUOTA_NOT_SET
#   Num Objects            : 0
#   Data Size              : 0.000 B
#   Data Size Reduced      : 0.000 B
#   Data Space Avail       : 1.000 PB
# 
#+end_example

#+begin_src bash
oc get obc -n openshift-storage
#+end_src

#+begin_example
# NAME        STORAGE-CLASS                 PHASE   AGE
# test21obc   openshift-storage.noobaa.io   Bound   47s
#+end_example

#+begin_src bash
oc get obc test21obc -o yaml -n openshift-storage
#+end_src

#+begin_example
# apiVersion: objectbucket.io/v1alpha1
# kind: ObjectBucketClaim
# metadata:
#   creationTimestamp: "2020-02-21T10:59:16Z"
#   finalizers:
#   - objectbucket.io/finalizer
#   generation: 2
#   labels:
#     app: noobaa
#     bucket-provisioner: openshift-storage.noobaa.io-obc
#     noobaa-domain: openshift-storage.noobaa.io
#   name: test21obc
#   namespace: openshift-storage
#   resourceVersion: "765864"
#   selfLink: /apis/objectbucket.io/v1alpha1/namespaces/openshift-storage/objectbucketclaims/test21obc
#   uid: 33fd718f-5499-11ea-9d6d-06399a073c9e
# spec:
#   ObjectBucketName: obc-openshift-storage-test21obc
#   bucketName: test21obc-0f9620af-2799-4671-a4f6-90218c4e2f09
#   generateBucketName: test21obc
#   storageClassName: openshift-storage.noobaa.io
# status:
#   phase: Bound
#+end_example

#+begin_src bash
oc get -n openshift-storage secret test21obc -o yaml
#+end_src

#+begin_example
# apiVersion: v1
# data:
#   AWS_ACCESS_KEY_ID: SVVHUFVENUZQd1labndBMnpLZno=
#   AWS_SECRET_ACCESS_KEY: TExBaXIvcUEzSEhyMURNMnM0Z2ZkbFV2akxIS3R0c3dPNnpZeW9XZQ==
# kind: Secret
# metadata:
#   creationTimestamp: "2020-02-21T10:59:16Z"
#   finalizers:
#   - objectbucket.io/finalizer
#   labels:
#     app: noobaa
#     bucket-provisioner: openshift-storage.noobaa.io-obc
#     noobaa-domain: openshift-storage.noobaa.io
#   name: test21obc
#   namespace: openshift-storage
#   ownerReferences:
#   - apiVersion: objectbucket.io/v1alpha1
#     blockOwnerDeletion: true
#     controller: true
#     kind: ObjectBucketClaim
#     name: test21obc
#     uid: 33fd718f-5499-11ea-9d6d-06399a073c9e
#   resourceVersion: "765859"
#   selfLink: /api/v1/namespaces/openshift-storage/secrets/test21obc
#   uid: 3413164f-5499-11ea-9d6d-06399a073c9e
# type: Opaque
#+end_example

#+begin_src bash
oc get -n openshift-storage cm test21obc -o yaml
#+end_src

#+begin_example
# apiVersion: v1
# data:
#   BUCKET_HOST: 10.0.147.203
#   BUCKET_NAME: test21obc-0f9620af-2799-4671-a4f6-90218c4e2f09
#   BUCKET_PORT: "31216"
#   BUCKET_REGION: ""
#   BUCKET_SUBREGION: ""
# kind: ConfigMap
# metadata:
#   creationTimestamp: "2020-02-21T10:59:16Z"
#   finalizers:
#   - objectbucket.io/finalizer
#   labels:
#     app: noobaa
#     bucket-provisioner: openshift-storage.noobaa.io-obc
#     noobaa-domain: openshift-storage.noobaa.io
#   name: test21obc
#   namespace: openshift-storage
#   ownerReferences:
#   - apiVersion: objectbucket.io/v1alpha1
#     blockOwnerDeletion: true
#     controller: true
#     kind: ObjectBucketClaim
#     name: test21obc
#     uid: 33fd718f-5499-11ea-9d6d-06399a073c9e
#   resourceVersion: "765860"
#   selfLink: /api/v1/namespaces/openshift-storage/configmaps/test21obc
#   uid: 341da014-5499-11ea-9d6d-06399a073c9e
#+end_example

Noobaa Managemnt Console - in Google Chrome: 
https://noobaa-mgmt-openshift-storage.apps.cluster-munich-e7ab.sandbox1596.opentlc.com/fe/systems/noobaa

*** Using an OBC inside a container

#+begin_src bash
cat ~/test/support/ocslab_obc-app-example.yaml
#+end_src

#+begin_example
# apiVersion: v1
# kind: Namespace
# metadata:
#   name: obc-test
# ---
# apiVersion: objectbucket.io/v1alpha1
# kind: ObjectBucketClaim
# metadata:
#   name: obc-test
#   namespace: obc-test
# spec:
#   generateBucketName: "obc-test-noobaa"
#   storageClassName: openshift-storage.noobaa.io
# ---
# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: obc-test
#   namespace: obc-test
#   labels:
#     app: obc-test
# spec:
#   template:
#     metadata:
#       labels:
#         app: obc-test
#     spec:
#       restartPolicy: OnFailure
#       containers:
#         - image: mesosphere/aws-cli:latest
#           command: ["sh"]
#           args: 
#             - '-c'
#             - 'set -x && s3cmd --no-check-certificate --host $BUCKET_HOST:$BUCKET_PORT --host-bucket $BUCKET_HOST:$BUCKET_PORT du'
#           name: obc-test
#           env:
#             - name: BUCKET_NAME
#               valueFrom:
#                 configMapKeyRef:
#                   name: obc-test
#                   key: BUCKET_NAME
#             - name: BUCKET_HOST
#               valueFrom:
#                 configMapKeyRef:
#                   name: obc-test
#                   key: BUCKET_HOST
#             - name: BUCKET_PORT
#               valueFrom:
#                 configMapKeyRef:
#                   name: obc-test
#                   key: BUCKET_PORT
#             - name: AWS_DEFAULT_REGION
#               valueFrom:
#                 configMapKeyRef:
#                   name: obc-test
#                   key: BUCKET_REGION
#             - name: AWS_ACCESS_KEY_ID
#               valueFrom:
#                 secretKeyRef:
#                   name: obc-test
#                   key: AWS_ACCESS_KEY_ID
#             - name: AWS_SECRET_ACCESS_KEY
#               valueFrom:
#                 secretKeyRef:
#                   name: obc-test
#+end_example

#+begin_src bash
oc apply -f ~/test/support/ocslab_obc-app-example.yaml
#+end_src

#+begin_example
# namespace/obc-test created
# objectbucketclaim.objectbucket.io/obc-test created
# job.batch/obc-test created
#+end_example

#+begin_src bash
oc get pods -n obc-test -l app=obc-test
#+end_src

#+begin_example
# NAME             READY   STATUS      RESTARTS   AGE
# obc-test-hhhtq   0/1     Completed   0          47s
#+end_example

#+begin_src bash
oc logs -n obc-test $(oc get pods -n obc-test -l app=obc-test -o jsonpath='{.items[0].metadata.name}')
#+end_src

#+begin_example
# + s3cmd --no-check-certificate --host 10.0.147.203:31216 --host-bucket 10.0.147.203:31216 du
# 0        0 objects s3://obc-test-noobaa-e9be56d7-ab2d-4219-acae-c1b21b3e7303/
# --------
# 0        Total
#+end_example

** Adding storage to the Ceph Cluster
*** Add storage worker nodes

#+begin_src bash
oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'
#+end_src

#+begin_example
# NAME                                             DESIRED   CURRENT   READY   AVAILABLE   AGE
# cluster-munich-e7ab-lqhqg-workerocs-us-east-2a   1         1         1       1           172m
# cluster-munich-e7ab-lqhqg-workerocs-us-east-2b   1         1         1       1           172m
# cluster-munich-e7ab-lqhqg-workerocs-us-east-2c   1         1         1       1           172m
#+end_example

#+begin_src bash
oc get machinesets -n openshift-machine-api -o name | grep workerocs | xargs -n1 -t oc scale -n openshift-machine-api --replicas=2
#+end_src

#+begin_example
# oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-munich-e7ab-lqhqg-workerocs-us-east-2a 
# machineset.machine.openshift.io/cluster-munich-e7ab-lqhqg-workerocs-us-east-2a scaled
# oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-munich-e7ab-lqhqg-workerocs-us-east-2b 
# machineset.machine.openshift.io/cluster-munich-e7ab-lqhqg-workerocs-us-east-2b scaled
# oc scale -n openshift-machine-api --replicas=2 machineset.machine.openshift.io/cluster-munich-e7ab-lqhqg-workerocs-us-east-2c 
# machineset.machine.openshift.io/cluster-munich-e7ab-lqhqg-workerocs-us-east-2c scaled
#+end_example

#+begin_src bash
watch "oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'"
#+end_src

#+begin_src bash
oc get machinesets -n openshift-machine-api | egrep 'NAME|workerocs'
#+end_src

#+begin_example
# NAME                                             DESIRED   CURRENT   READY   AVAILABLE   AGE
# cluster-munich-e7ab-lqhqg-workerocs-us-east-2a   2         2         1       1           176m
# cluster-munich-e7ab-lqhqg-workerocs-us-east-2b   2         2         1       1           176m
# cluster-munich-e7ab-lqhqg-workerocs-us-east-2c   2         2         1       1           176m
#+end_example

#+begin_src bash
oc get node --show-labels | grep storage-node | grep -v openshift-storage | cut -d' ' -f1
#+end_src

#+begin_example
# ip-10-0-143-103.us-east-2.compute.internal
# ip-10-0-153-85.us-east-2.compute.internal
# ip-10-0-163-36.us-east-2.compute.internal
#+end_example

#+begin_src bash
oc get nodes -o json | jq '.items[] | select(.metadata.labels.role == "storage-node") | .metadata.name' | xargs -n1 -t -I {} oc label nodes {} cluster.ocs.openshift.io/openshift-storage=""
#+end_src

#+begin_example
# oc label nodes ip-10-0-131-190.us-east-2.compute.internal cluster.ocs.openshift.io/openshift-storage= 
# error: 'cluster.ocs.openshift.io/openshift-storage' already has a value (), and --overwrite is false
# oc label nodes ip-10-0-143-103.us-east-2.compute.internal cluster.ocs.openshift.io/openshift-storage= 
# node/ip-10-0-143-103.us-east-2.compute.internal labeled
# oc label nodes ip-10-0-147-203.us-east-2.compute.internal cluster.ocs.openshift.io/openshift-storage= 
# error: 'cluster.ocs.openshift.io/openshift-storage' already has a value (), and --overwrite is false
# oc label nodes ip-10-0-153-85.us-east-2.compute.internal cluster.ocs.openshift.io/openshift-storage= 
# node/ip-10-0-153-85.us-east-2.compute.internal labeled
# oc label nodes ip-10-0-163-36.us-east-2.compute.internal cluster.ocs.openshift.io/openshift-storage= 
# node/ip-10-0-163-36.us-east-2.compute.internal labeled
# oc label nodes ip-10-0-174-204.us-east-2.compute.internal cluster.ocs.openshift.io/openshift-storage= 
# error: 'cluster.ocs.openshift.io/openshift-storage' already has a value (), and --overwrite is false
#+end_example

*** Add storage capacity

#+begin_src bash
oc get pod -o=custom-columns=NAME:.metadata.name,STATUS:.status.phase,NODE:.spec.nodeName -n openshift-storage | grep osd
#+end_src

#+begin_example
# rook-ceph-osd-0-54d86bc475-kbjdb                                  Running     ip-10-0-174-204.us-east-2.compute.internal
# rook-ceph-osd-1-5f56fcff97-cnbrg                                  Running     ip-10-0-131-190.us-east-2.compute.internal
# rook-ceph-osd-2-5d6b876dd7-hw29d                                  Running     ip-10-0-147-203.us-east-2.compute.internal
# rook-ceph-osd-3-67cb6f9499-zdsr7                                  Running     ip-10-0-143-103.us-east-2.compute.internal
# rook-ceph-osd-4-595ff56bb-tmmdx                                   Running     ip-10-0-163-36.us-east-2.compute.internal
# rook-ceph-osd-5-748b96b4d8-nzndk                                  Running     ip-10-0-153-85.us-east-2.compute.internal
# rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8               Succeeded   ip-10-0-174-204.us-east-2.compute.internal
# rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb               Succeeded   ip-10-0-163-36.us-east-2.compute.internal
# rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g               Succeeded   ip-10-0-131-190.us-east-2.compute.internal
# rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n               Succeeded   ip-10-0-143-103.us-east-2.compute.internal
# rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl               Succeeded   ip-10-0-147-203.us-east-2.compute.internal
# rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m               Succeeded   ip-10-0-153-85.us-east-2.compute.internal
#+end_example

*** Verify new storage

#+begin_src bash
TOOLS_POD=$(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)
oc rsh -n openshift-storage $TOOLS_POD
#+end_src

#+begin_src bash
ceph status
#+end_src

#+begin_example
# ceph status
#   cluster:
#     id:     cede185f-f780-4daa-a4cf-96fe25d736a4
#     health: HEALTH_OK
#  
#   services:
#     mon: 3 daemons, quorum a,b,c (age 2h)
#     mgr: a(active, since 2h)
#     mds: ocs-storagecluster-cephfilesystem:1 {0=ocs-storagecluster-cephfilesystem-a=up:active} 1 up:standby-replay
#     osd: 6 osds: 6 up (since 42s), 6 in (since 42s)
#  
#   data:
#     pools:   3 pools, 24 pgs
#     objects: 508 objects, 1.1 GiB
#     usage:   8.4 GiB used, 6.0 TiB / 6.0 TiB avail
#     pgs:     24 active+clean
#  
#   io:
#     client:   1.2 KiB/s rd, 334 KiB/s wr, 2 op/s rd, 2 op/s wr
#  
#+end_example

#+begin_src bash
ceph osd crush tree
#+end_src

#+begin_example
# ceph osd crush tree
# ID  CLASS WEIGHT  TYPE NAME                                
#  -1       5.99396 root default                             
#  -5       5.99396     region us-east-2                     
# -10       1.99799         zone us-east-2a                  
#  -9       0.99899             host ocs-deviceset-1-0-x7hnz 
#   1   ssd 0.99899                 osd.1                    
# -17       0.99899             host ocs-deviceset-1-1-wbjp9 
#   3   ssd 0.99899                 osd.3                    
# -14       1.99799         zone us-east-2b                  
# -13       0.99899             host ocs-deviceset-2-0-rxlj7 
#   2   ssd 0.99899                 osd.2                    
# -21       0.99899             host ocs-deviceset-2-1-zjt7w 
#   5   ssd 0.99899                 osd.5                    
#  -4       1.99799         zone us-east-2c                  
#  -3       0.99899             host ocs-deviceset-0-0-qdn6p 
#   0   ssd 0.99899                 osd.0                    
# -19       0.99899             host ocs-deviceset-0-1-jqljq 
#   4   ssd 0.99899                 osd.4                    
#+end_example

#+begin_src bash
exit
#+end_src

** Monitoring the OCS environment
*** Metrics

ceph_osd_op

Then let’s try a more relevant query example and enter the following text
rate(ceph_osd_op[5m]) or irate(ceph_osd_op[5m]) in the query field. When you are
done typing, simply hit [Enter]
process_cpu_seconds_total

Have a look at the difference between sum(irate(process_cpu_seconds_total[5m]))
and irate(process_cpu_seconds_total[5m]) for instance.

https://prometheus.io/docs/prometheus/latest/querying/basics/

*** Using must-gather

#+begin_src bash
oc adm must-gather
#+end_src

#+begin_example
# [must-gather      ] OUT Using must-gather plugin-in image: quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88f4ef9a524da1947682cd6f6ab0428c36d63c69a2b94b2c584ae31abba96c34
# [must-gather      ] OUT namespace/openshift-must-gather-cbzcj created
# [must-gather      ] OUT clusterrolebinding.rbac.authorization.k8s.io/must-gather-sb72s created
# [must-gather      ] OUT pod for plug-in image quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:88f4ef9a524da1947682cd6f6ab0428c36d63c69a2b94b2c584ae31abba96c34 created
# [must-gather-58kh8] POD 2020/02/21 11:40:19 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:40:19 Gathering data for ns/openshift-cluster-version...
# [must-gather-58kh8] POD 2020/02/21 11:40:19     Collecting resources for namespace "openshift-cluster-version"...
# [must-gather-58kh8] POD 2020/02/21 11:40:19     Gathering pod data for namespace "openshift-cluster-version"...
# [must-gather-58kh8] POD 2020/02/21 11:40:19         Gathering data for pod "cluster-version-operator-6545d8586b-zgppz"
# [must-gather-58kh8] POD 2020/02/21 11:40:21         Unable to gather previous container logs: previous terminated container "cluster-version-operator" in pod "cluster-version-operator-6545d8586b-zgppz" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:21         Skipping container endpoint collection for pod "cluster-version-operator-6545d8586b-zgppz" container "cluster-version-operator": No ports
# [must-gather-58kh8] POD 2020/02/21 11:40:21 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:40:21 Gathering config.openshift.io resource data...
# [must-gather-58kh8] POD 2020/02/21 11:40:23 Gathering kubeapiserver.operator.openshift.io resource data...
# [must-gather-58kh8] POD 2020/02/21 11:40:23 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:40:23     Gathering related object reference information for ClusterOperator "authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:23     Found related object "authentications.operator.openshift.io/cluster" for ClusterOperator "authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:23     Found related object "authentications.config.openshift.io/cluster" for ClusterOperator "authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:23     Found related object "infrastructures.config.openshift.io/cluster" for ClusterOperator "authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:23     Found related object "oauths.config.openshift.io/cluster" for ClusterOperator "authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:23     Found related object "namespaces/openshift-config" for ClusterOperator "authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:23     Found related object "namespaces/openshift-config-managed" for ClusterOperator "authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:23     Found related object "namespaces/openshift-authentication" for ClusterOperator "authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:23     Found related object "namespaces/openshift-authentication-operator" for ClusterOperator "authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:24 Gathering data for ns/openshift-config...
# [must-gather-58kh8] POD 2020/02/21 11:40:24     Collecting resources for namespace "openshift-config"...
# [must-gather-58kh8] POD 2020/02/21 11:40:24     Gathering pod data for namespace "openshift-config"...
# [must-gather-58kh8] POD 2020/02/21 11:40:24 Gathering data for ns/openshift-config-managed...
# [must-gather-58kh8] POD 2020/02/21 11:40:24     Collecting resources for namespace "openshift-config-managed"...
# [must-gather-58kh8] POD 2020/02/21 11:40:24     Gathering pod data for namespace "openshift-config-managed"...
# [must-gather-58kh8] POD 2020/02/21 11:40:25 Gathering data for ns/openshift-authentication...
# [must-gather-58kh8] POD 2020/02/21 11:40:25     Collecting resources for namespace "openshift-authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:25     Gathering pod data for namespace "openshift-authentication"...
# [must-gather-58kh8] POD 2020/02/21 11:40:25         Gathering data for pod "oauth-openshift-c55ccdc57-pkm8s"
# [must-gather-58kh8] POD 2020/02/21 11:40:25         Unable to gather previous container logs: previous terminated container "oauth-openshift" in pod "oauth-openshift-c55ccdc57-pkm8s" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:25         Gathering data for pod "oauth-openshift-c55ccdc57-vgjxf"
# [must-gather-58kh8] POD 2020/02/21 11:40:25         Unable to gather previous container logs: previous terminated container "oauth-openshift" in pod "oauth-openshift-c55ccdc57-vgjxf" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:25 Gathering data for ns/openshift-authentication-operator...
# [must-gather-58kh8] POD 2020/02/21 11:40:25     Collecting resources for namespace "openshift-authentication-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:40:25     Gathering pod data for namespace "openshift-authentication-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:40:25         Gathering data for pod "authentication-operator-55bc47f6d4-6ltcq"
# [must-gather-58kh8] POD 2020/02/21 11:40:26         Unable to gather previous container logs: previous terminated container "operator" in pod "authentication-operator-55bc47f6d4-6ltcq" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:26         Skipping container endpoint collection for pod "authentication-operator-55bc47f6d4-6ltcq" container "operator": No ports
# [must-gather-58kh8] POD 2020/02/21 11:40:26 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:40:26 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:40:26 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Gathering related object reference information for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "namespaces/openshift-cloud-credential-operator" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/cloud-credential-operator-iam-ro" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-image-registry" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-image-registry-azure" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-image-registry-gcs" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-image-registry-openstack" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-ingress" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-ingress-azure" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-ingress-gcp" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-machine-api-aws" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-machine-api-azure" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-machine-api-gcp" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Found related object "CredentialsRequest.cloudcredential.openshift.io/openshift-machine-api-openstack" for ClusterOperator "cloud-credential"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26 Gathering data for ns/openshift-cloud-credential-operator...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Collecting resources for namespace "openshift-cloud-credential-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26     Gathering pod data for namespace "openshift-cloud-credential-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:40:26         Gathering data for pod "cloud-credential-operator-fb7864cd9-4b89j"
# [must-gather-58kh8] POD 2020/02/21 11:40:26         Unable to gather previous container logs: previous terminated container "manager" in pod "cloud-credential-operator-fb7864cd9-4b89j" not found
# [must-gather-58kh8] POD E0221 11:40:26.601254     109 portforward.go:331] an error occurred forwarding 37587 -> 9876: error forwarding port 9876 to pod 17fd22ac44bad78c124cb77db0fc942a7ea3b0a50daed67d433d3cd688e4223d, uid : exit status 1: 2020/02/21 11:40:26 socat[713816] E connect(5, AF=2 127.0.0.1:9876, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:40:26.647487     109 portforward.go:331] an error occurred forwarding 37587 -> 9876: error forwarding port 9876 to pod 17fd22ac44bad78c124cb77db0fc942a7ea3b0a50daed67d433d3cd688e4223d, uid : exit status 1: 2020/02/21 11:40:26 socat[713826] E connect(5, AF=2 127.0.0.1:9876, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:40:26.695543     109 portforward.go:331] an error occurred forwarding 37587 -> 9876: error forwarding port 9876 to pod 17fd22ac44bad78c124cb77db0fc942a7ea3b0a50daed67d433d3cd688e4223d, uid : exit status 1: 2020/02/21 11:40:26 socat[713833] E connect(5, AF=2 127.0.0.1:9876, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:40:30 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:40:30 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:40:30 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:40:30     Gathering related object reference information for ClusterOperator "cluster-autoscaler"...
# [must-gather-58kh8] POD 2020/02/21 11:40:30     Found related object "machineautoscalers.machine.openshift.io" for ClusterOperator "cluster-autoscaler"...
# [must-gather-58kh8] POD 2020/02/21 11:40:30     Found related object "clusterautoscalers.machine.openshift.io" for ClusterOperator "cluster-autoscaler"...
# [must-gather-58kh8] POD 2020/02/21 11:40:30     Found related object "namespaces/openshift-machine-api" for ClusterOperator "cluster-autoscaler"...
# [must-gather-58kh8] POD 2020/02/21 11:40:30 Gathering data for ns/openshift-machine-api...
# [must-gather-58kh8] POD 2020/02/21 11:40:30     Collecting resources for namespace "openshift-machine-api"...
# [must-gather-58kh8] POD 2020/02/21 11:40:30     Gathering pod data for namespace "openshift-machine-api"...
# [must-gather-58kh8] POD 2020/02/21 11:40:30         Gathering data for pod "cluster-autoscaler-operator-68bc4d6bd-jcrwv"
# [must-gather-58kh8] POD 2020/02/21 11:40:30         Unable to gather previous container logs: previous terminated container "cluster-autoscaler-operator" in pod "cluster-autoscaler-operator-68bc4d6bd-jcrwv" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:31         Gathering data for pod "machine-api-controllers-cf7d74945-zftzx"
# [must-gather-58kh8] POD 2020/02/21 11:40:31         Unable to gather previous container logs: previous terminated container "controller-manager" in pod "machine-api-controllers-cf7d74945-zftzx" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:31         Skipping container endpoint collection for pod "machine-api-controllers-cf7d74945-zftzx" container "controller-manager": No ports
# [must-gather-58kh8] POD 2020/02/21 11:40:31         Unable to gather previous container logs: previous terminated container "machine-controller" in pod "machine-api-controllers-cf7d74945-zftzx" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:31         Skipping container endpoint collection for pod "machine-api-controllers-cf7d74945-zftzx" container "machine-controller": No ports
# [must-gather-58kh8] POD 2020/02/21 11:40:32         Unable to gather previous container logs: previous terminated container "nodelink-controller" in pod "machine-api-controllers-cf7d74945-zftzx" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:32         Skipping container endpoint collection for pod "machine-api-controllers-cf7d74945-zftzx" container "nodelink-controller": No ports
# [must-gather-58kh8] POD 2020/02/21 11:40:32         Gathering data for pod "machine-api-operator-68cbcfbf9-fzwsf"
# [must-gather-58kh8] POD 2020/02/21 11:40:32         Unable to gather previous container logs: previous terminated container "machine-api-operator" in pod "machine-api-operator-68cbcfbf9-fzwsf" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:33 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:40:33 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:40:33 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:40:33     Gathering related object reference information for ClusterOperator "console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:33     Found related object "consoles.operator.openshift.io/cluster" for ClusterOperator "console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:33     Found related object "consoles.config.openshift.io/cluster" for ClusterOperator "console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:33     Found related object "infrastructures.config.openshift.io/cluster" for ClusterOperator "console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:33     Found related object "proxies.config.openshift.io/cluster" for ClusterOperator "console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:33     Found related object "oauthclients.oauth.openshift.io/console" for ClusterOperator "console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:33     Found related object "namespaces/openshift-console-operator" for ClusterOperator "console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:33     Found related object "namespaces/openshift-console" for ClusterOperator "console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:33     Found related object "configmaps/console-public" for ClusterOperator "console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:34 Gathering data for ns/openshift-console-operator...
# [must-gather-58kh8] POD 2020/02/21 11:40:34     Collecting resources for namespace "openshift-console-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:40:34     Gathering pod data for namespace "openshift-console-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:40:34         Gathering data for pod "console-operator-dd677d4d7-wdxl7"
# [must-gather-58kh8] POD 2020/02/21 11:40:34         Unable to gather previous container logs: previous terminated container "console-operator" in pod "console-operator-dd677d4d7-wdxl7" not found
# [must-gather-58kh8] POD E0221 11:40:34.353149     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod a75817fe134c411b5ea6bc5378f5ef965539e936d79aee08913acba5119205a1, uid : exit status 1: 2020/02/21 11:40:34 socat[714066] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:40:34.399141     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod a75817fe134c411b5ea6bc5378f5ef965539e936d79aee08913acba5119205a1, uid : exit status 1: 2020/02/21 11:40:34 socat[714073] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:40:34.448359     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod a75817fe134c411b5ea6bc5378f5ef965539e936d79aee08913acba5119205a1, uid : exit status 1: 2020/02/21 11:40:34 socat[714081] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:40:34 Gathering data for ns/openshift-console...
# [must-gather-58kh8] POD 2020/02/21 11:40:34     Collecting resources for namespace "openshift-console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:34     Gathering pod data for namespace "openshift-console"...
# [must-gather-58kh8] POD 2020/02/21 11:40:34         Gathering data for pod "console-5bb876548-5mprz"
# [must-gather-58kh8] POD 2020/02/21 11:40:34         Unable to gather previous container logs: previous terminated container "console" in pod "console-5bb876548-5mprz" not found
# [must-gather-58kh8] POD E0221 11:40:34.954893     109 portforward.go:331] an error occurred forwarding 37587 -> 443: error forwarding port 443 to pod ff287aebc0380c8acdb7e1ab64e557136398321fff0eb278b57ff74eb86a589a, uid : exit status 1: 2020/02/21 11:40:34 socat[714131] E connect(5, AF=2 127.0.0.1:443, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:40:35.001172     109 portforward.go:331] an error occurred forwarding 37587 -> 443: error forwarding port 443 to pod ff287aebc0380c8acdb7e1ab64e557136398321fff0eb278b57ff74eb86a589a, uid : exit status 1: 2020/02/21 11:40:34 socat[714140] E connect(5, AF=2 127.0.0.1:443, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:40:35.043236     109 portforward.go:331] an error occurred forwarding 37587 -> 443: error forwarding port 443 to pod ff287aebc0380c8acdb7e1ab64e557136398321fff0eb278b57ff74eb86a589a, uid : exit status 1: 2020/02/21 11:40:35 socat[714149] E connect(5, AF=2 127.0.0.1:443, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:40:35         Gathering data for pod "console-5bb876548-ls5bk"
# [must-gather-58kh8] POD 2020/02/21 11:40:35         Unable to gather previous container logs: previous terminated container "console" in pod "console-5bb876548-ls5bk" not found
# [must-gather-58kh8] POD E0221 11:40:35.114438     109 portforward.go:331] an error occurred forwarding 37587 -> 443: error forwarding port 443 to pod 0bd02b5b09daae0a2fd7d3241f6069c3febbf99253b915f13f1eecd1e3898b15, uid : exit status 1: 2020/02/21 11:40:35 socat[380107] E connect(5, AF=2 127.0.0.1:443, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:40:35.159584     109 portforward.go:331] an error occurred forwarding 37587 -> 443: error forwarding port 443 to pod 0bd02b5b09daae0a2fd7d3241f6069c3febbf99253b915f13f1eecd1e3898b15, uid : exit status 1: 2020/02/21 11:40:35 socat[380114] E connect(5, AF=2 127.0.0.1:443, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:40:35.206577     109 portforward.go:331] an error occurred forwarding 37587 -> 443: error forwarding port 443 to pod 0bd02b5b09daae0a2fd7d3241f6069c3febbf99253b915f13f1eecd1e3898b15, uid : exit status 1: 2020/02/21 11:40:35 socat[380122] E connect(5, AF=2 127.0.0.1:443, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:40:35         Gathering data for pod "downloads-9fcbc4dc7-k6wf5"
# [must-gather-58kh8] POD 2020/02/21 11:40:35         Unable to gather previous container logs: previous terminated container "download-server" in pod "downloads-9fcbc4dc7-k6wf5" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:35         Gathering data for pod "downloads-9fcbc4dc7-zf4vs"
# [must-gather-58kh8] POD 2020/02/21 11:40:35         Unable to gather previous container logs: previous terminated container "download-server" in pod "downloads-9fcbc4dc7-zf4vs" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:36 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:40:36 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:40:36 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:40:36     Gathering related object reference information for ClusterOperator "dns"...
# [must-gather-58kh8] POD 2020/02/21 11:40:36     Found related object "namespaces/openshift-dns-operator" for ClusterOperator "dns"...
# [must-gather-58kh8] POD 2020/02/21 11:40:36     Found related object "namespaces/openshift-dns" for ClusterOperator "dns"...
# [must-gather-58kh8] POD 2020/02/21 11:40:36     Found related object "DNS.operator.openshift.io/default" for ClusterOperator "dns"...
# [must-gather-58kh8] POD 2020/02/21 11:40:36 Gathering data for ns/openshift-dns-operator...
# [must-gather-58kh8] POD 2020/02/21 11:40:36     Collecting resources for namespace "openshift-dns-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:40:36     Gathering pod data for namespace "openshift-dns-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:40:36         Gathering data for pod "dns-operator-9c5f9d7d9-q86ld"
# [must-gather-58kh8] POD 2020/02/21 11:40:36         Unable to gather previous container logs: previous terminated container "dns-operator" in pod "dns-operator-9c5f9d7d9-q86ld" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:36         Skipping container endpoint collection for pod "dns-operator-9c5f9d7d9-q86ld" container "dns-operator": No ports
# [must-gather-58kh8] POD 2020/02/21 11:40:36 Gathering data for ns/openshift-dns...
# [must-gather-58kh8] POD 2020/02/21 11:40:36     Collecting resources for namespace "openshift-dns"...
# [must-gather-58kh8] POD 2020/02/21 11:40:36     Gathering pod data for namespace "openshift-dns"...
# [must-gather-58kh8] POD 2020/02/21 11:40:36         Gathering data for pod "dns-default-878bg"
# [must-gather-58kh8] POD 2020/02/21 11:40:36         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-878bg" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:44         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-878bg" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:44         Skipping container endpoint collection for pod "dns-default-878bg" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:40:44         Gathering data for pod "dns-default-9cwn9"
# [must-gather-58kh8] POD 2020/02/21 11:40:44         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-9cwn9" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:52         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-9cwn9" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:52         Skipping container endpoint collection for pod "dns-default-9cwn9" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:40:52         Gathering data for pod "dns-default-b8tnk"
# [must-gather-58kh8] POD 2020/02/21 11:40:52         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-b8tnk" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:59         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-b8tnk" not found
# [must-gather-58kh8] POD 2020/02/21 11:40:59         Skipping container endpoint collection for pod "dns-default-b8tnk" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:40:59         Gathering data for pod "dns-default-d84d7"
# [must-gather-58kh8] POD 2020/02/21 11:40:59         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-d84d7" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:07         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-d84d7" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:07         Skipping container endpoint collection for pod "dns-default-d84d7" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:41:07         Gathering data for pod "dns-default-dscth"
# [must-gather-58kh8] POD 2020/02/21 11:41:07         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-dscth" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:15         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-dscth" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:15         Skipping container endpoint collection for pod "dns-default-dscth" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:41:15         Gathering data for pod "dns-default-dwbff"
# [must-gather-58kh8] POD 2020/02/21 11:41:15         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-dwbff" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:22         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-dwbff" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:22         Skipping container endpoint collection for pod "dns-default-dwbff" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:41:22         Gathering data for pod "dns-default-lvbnq"
# [must-gather-58kh8] POD 2020/02/21 11:41:22         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-lvbnq" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:30         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-lvbnq" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:30         Skipping container endpoint collection for pod "dns-default-lvbnq" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:41:30         Gathering data for pod "dns-default-mq5t8"
# [must-gather-58kh8] POD 2020/02/21 11:41:30         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-mq5t8" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:38         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-mq5t8" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:38         Skipping container endpoint collection for pod "dns-default-mq5t8" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:41:38         Gathering data for pod "dns-default-mzq2f"
# [must-gather-58kh8] POD 2020/02/21 11:41:38         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-mzq2f" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:45         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-mzq2f" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:45         Skipping container endpoint collection for pod "dns-default-mzq2f" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:41:45         Gathering data for pod "dns-default-p294t"
# [must-gather-58kh8] POD 2020/02/21 11:41:45         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-p294t" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:53         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-p294t" not found
# [must-gather-58kh8] POD 2020/02/21 11:41:53         Skipping container endpoint collection for pod "dns-default-p294t" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:41:53         Gathering data for pod "dns-default-s4tjk"
# [must-gather-58kh8] POD 2020/02/21 11:41:53         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-s4tjk" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:01         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-s4tjk" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:01         Skipping container endpoint collection for pod "dns-default-s4tjk" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:01         Gathering data for pod "dns-default-sv8m4"
# [must-gather-58kh8] POD 2020/02/21 11:42:01         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-sv8m4" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:08         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-sv8m4" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:08         Skipping container endpoint collection for pod "dns-default-sv8m4" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:08         Gathering data for pod "dns-default-tqk52"
# [must-gather-58kh8] POD 2020/02/21 11:42:08         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-tqk52" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:16         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-tqk52" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:16         Skipping container endpoint collection for pod "dns-default-tqk52" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:16         Gathering data for pod "dns-default-x4vgp"
# [must-gather-58kh8] POD 2020/02/21 11:42:16         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-x4vgp" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:24         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-x4vgp" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:24         Skipping container endpoint collection for pod "dns-default-x4vgp" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:24         Gathering data for pod "dns-default-zc8p6"
# [must-gather-58kh8] POD 2020/02/21 11:42:24         Unable to gather previous container logs: previous terminated container "dns" in pod "dns-default-zc8p6" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:31         Unable to gather previous container logs: previous terminated container "dns-node-resolver" in pod "dns-default-zc8p6" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:31         Skipping container endpoint collection for pod "dns-default-zc8p6" container "dns-node-resolver": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:32 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:32 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:32 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Gathering related object reference information for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "configs.imageregistry.operator.openshift.io/cluster" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "namespaces/openshift-image-registry" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "clusterroles.rbac.authorization.k8s.io/system:registry" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "clusterrolebindings.rbac.authorization.k8s.io/registry-registry-role" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "serviceaccounts/registry" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "configmaps/serviceca" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "configmaps/image-registry-certificates" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "secrets/image-registry-private-configuration" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "images.config.openshift.io/cluster" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "daemonsets.apps/node-ca" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "services/image-registry" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:32     Found related object "deployments.apps/image-registry" for ClusterOperator "image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:33 Gathering data for ns/openshift-image-registry...
# [must-gather-58kh8] POD 2020/02/21 11:42:33     Collecting resources for namespace "openshift-image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:33     Gathering pod data for namespace "openshift-image-registry"...
# [must-gather-58kh8] POD 2020/02/21 11:42:33         Gathering data for pod "cluster-image-registry-operator-8587f7bfcb-6cptn"
# [must-gather-58kh8] POD 2020/02/21 11:42:33         Unable to gather previous container logs: previous terminated container "cluster-image-registry-operator" in pod "cluster-image-registry-operator-8587f7bfcb-6cptn" not found
# [must-gather-58kh8] POD E0221 11:42:33.486528     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod 97195b3bbef7e49c509350e3e80b7908def457a47a3af1718c2f0a0619aa4789, uid : exit status 1: 2020/02/21 11:42:33 socat[717996] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:42:33.534378     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod 97195b3bbef7e49c509350e3e80b7908def457a47a3af1718c2f0a0619aa4789, uid : exit status 1: 2020/02/21 11:42:33 socat[718004] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:42:33.591415     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod 97195b3bbef7e49c509350e3e80b7908def457a47a3af1718c2f0a0619aa4789, uid : exit status 1: 2020/02/21 11:42:33 socat[718012] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:42:33         Unable to gather previous container logs: previous terminated container "cluster-image-registry-operator-watch" in pod "cluster-image-registry-operator-8587f7bfcb-6cptn" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:33         Skipping container endpoint collection for pod "cluster-image-registry-operator-8587f7bfcb-6cptn" container "cluster-image-registry-operator-watch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:33         Gathering data for pod "image-registry-6698d579c9-2cqfd"
# [must-gather-58kh8] POD 2020/02/21 11:42:33         Unable to gather previous container logs: previous terminated container "registry" in pod "image-registry-6698d579c9-2cqfd" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:33         Gathering data for pod "node-ca-22grs"
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-22grs" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Skipping container endpoint collection for pod "node-ca-22grs" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Gathering data for pod "node-ca-55tl6"
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-55tl6" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Skipping container endpoint collection for pod "node-ca-55tl6" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Gathering data for pod "node-ca-575r2"
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-575r2" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Skipping container endpoint collection for pod "node-ca-575r2" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Gathering data for pod "node-ca-9h5mc"
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-9h5mc" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Skipping container endpoint collection for pod "node-ca-9h5mc" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Gathering data for pod "node-ca-9jk2n"
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-9jk2n" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Skipping container endpoint collection for pod "node-ca-9jk2n" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Gathering data for pod "node-ca-9p9pl"
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-9p9pl" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Skipping container endpoint collection for pod "node-ca-9p9pl" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:34         Gathering data for pod "node-ca-b8sp8"
# [must-gather-58kh8] POD 2020/02/21 11:42:35         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-b8sp8" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:35         Skipping container endpoint collection for pod "node-ca-b8sp8" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:35         Gathering data for pod "node-ca-fqkj9"
# [must-gather-58kh8] POD 2020/02/21 11:42:35         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-fqkj9" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:35         Skipping container endpoint collection for pod "node-ca-fqkj9" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:35         Gathering data for pod "node-ca-gqcvf"
# [must-gather-58kh8] POD 2020/02/21 11:42:36         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-gqcvf" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:36         Skipping container endpoint collection for pod "node-ca-gqcvf" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:36         Gathering data for pod "node-ca-lf5mx"
# [must-gather-58kh8] POD 2020/02/21 11:42:36         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-lf5mx" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:36         Skipping container endpoint collection for pod "node-ca-lf5mx" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:36         Gathering data for pod "node-ca-mmmql"
# [must-gather-58kh8] POD 2020/02/21 11:42:36         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-mmmql" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:36         Skipping container endpoint collection for pod "node-ca-mmmql" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:36         Gathering data for pod "node-ca-r8fb2"
# [must-gather-58kh8] POD 2020/02/21 11:42:37         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-r8fb2" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:37         Skipping container endpoint collection for pod "node-ca-r8fb2" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:37         Gathering data for pod "node-ca-vbqgb"
# [must-gather-58kh8] POD 2020/02/21 11:42:37         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-vbqgb" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:37         Skipping container endpoint collection for pod "node-ca-vbqgb" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:37         Gathering data for pod "node-ca-vv2wl"
# [must-gather-58kh8] POD 2020/02/21 11:42:38         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-vv2wl" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:38         Skipping container endpoint collection for pod "node-ca-vv2wl" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:38         Gathering data for pod "node-ca-vxvs2"
# [must-gather-58kh8] POD 2020/02/21 11:42:38         Unable to gather previous container logs: previous terminated container "node-ca" in pod "node-ca-vxvs2" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:38         Skipping container endpoint collection for pod "node-ca-vxvs2" container "node-ca": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:39 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:39 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:39 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:42:39     Gathering related object reference information for ClusterOperator "ingress"...
# [must-gather-58kh8] POD 2020/02/21 11:42:39     Found related object "namespaces/openshift-ingress-operator" for ClusterOperator "ingress"...
# [must-gather-58kh8] POD 2020/02/21 11:42:39     Found related object "namespaces/openshift-ingress" for ClusterOperator "ingress"...
# [must-gather-58kh8] POD 2020/02/21 11:42:39     Found related object "IngressController.operator.openshift.io/default" for ClusterOperator "ingress"...
# [must-gather-58kh8] POD 2020/02/21 11:42:39     Found related object "DNSRecord.ingress.operator.openshift.io/default-wildcard" for ClusterOperator "ingress"...
# [must-gather-58kh8] POD 2020/02/21 11:42:39 Gathering data for ns/openshift-ingress-operator...
# [must-gather-58kh8] POD 2020/02/21 11:42:39     Collecting resources for namespace "openshift-ingress-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:39     Gathering pod data for namespace "openshift-ingress-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:39         Gathering data for pod "ingress-operator-86d56f89ff-whc9q"
# [must-gather-58kh8] POD 2020/02/21 11:42:39         Unable to gather previous container logs: previous terminated container "ingress-operator" in pod "ingress-operator-86d56f89ff-whc9q" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:39         Skipping container endpoint collection for pod "ingress-operator-86d56f89ff-whc9q" container "ingress-operator": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:40 Gathering data for ns/openshift-ingress...
# [must-gather-58kh8] POD 2020/02/21 11:42:40     Collecting resources for namespace "openshift-ingress"...
# [must-gather-58kh8] POD 2020/02/21 11:42:40     Gathering pod data for namespace "openshift-ingress"...
# [must-gather-58kh8] POD 2020/02/21 11:42:40         Gathering data for pod "router-default-5d5f9f8649-7mghq"
# [must-gather-58kh8] POD 2020/02/21 11:42:40         Unable to gather previous container logs: previous terminated container "router" in pod "router-default-5d5f9f8649-7mghq" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:41         Gathering data for pod "router-default-5d5f9f8649-p9p6f"
# [must-gather-58kh8] POD 2020/02/21 11:42:41         Unable to gather previous container logs: previous terminated container "router" in pod "router-default-5d5f9f8649-p9p6f" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:44 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:44 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:44 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:42:44     Gathering related object reference information for ClusterOperator "insights"...
# [must-gather-58kh8] POD 2020/02/21 11:42:44     Found related object "namespaces/openshift-insights" for ClusterOperator "insights"...
# [must-gather-58kh8] POD 2020/02/21 11:42:44     Found related object "deployments.apps/insights-operator" for ClusterOperator "insights"...
# [must-gather-58kh8] POD 2020/02/21 11:42:44 Gathering data for ns/openshift-insights...
# [must-gather-58kh8] POD 2020/02/21 11:42:44     Collecting resources for namespace "openshift-insights"...
# [must-gather-58kh8] POD 2020/02/21 11:42:44     Gathering pod data for namespace "openshift-insights"...
# [must-gather-58kh8] POD 2020/02/21 11:42:44         Gathering data for pod "insights-operator-7cd676dd46-ff2pt"
# [must-gather-58kh8] POD 2020/02/21 11:42:44         Unable to gather previous container logs: previous terminated container "operator" in pod "insights-operator-7cd676dd46-ff2pt" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:45 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:45 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:45 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:42:45     Gathering related object reference information for ClusterOperator "kube-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:42:45     Found related object "kubeapiservers.operator.openshift.io/cluster" for ClusterOperator "kube-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:42:45     Found related object "namespaces/openshift-config" for ClusterOperator "kube-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:42:45     Found related object "namespaces/openshift-config-managed" for ClusterOperator "kube-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:42:45     Found related object "namespaces/openshift-kube-apiserver-operator" for ClusterOperator "kube-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:42:45     Found related object "namespaces/openshift-kube-apiserver" for ClusterOperator "kube-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:42:45 Gathering data for ns/openshift-kube-apiserver-operator...
# [must-gather-58kh8] POD 2020/02/21 11:42:45     Collecting resources for namespace "openshift-kube-apiserver-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:45     Gathering pod data for namespace "openshift-kube-apiserver-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:45         Gathering data for pod "kube-apiserver-operator-84b79d6d8b-bgx57"
# [must-gather-58kh8] POD 2020/02/21 11:42:46 Gathering data for ns/openshift-kube-apiserver...
# [must-gather-58kh8] POD 2020/02/21 11:42:46     Collecting resources for namespace "openshift-kube-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:42:46     Gathering pod data for namespace "openshift-kube-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-2-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-2-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-2-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-2-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-2-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-2-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-3-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-3-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-4-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-4-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-6-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-6-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-6-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-6-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-6-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-6-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-7-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-7-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-8-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-8-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-8-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-8-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-8-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-8-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-9-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-9-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-9-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-9-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "installer-9-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container data collection for pod "installer-9-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Unable to gather previous container logs: previous terminated container "kube-apiserver-9" in pod "kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Unable to gather previous container logs: previous terminated container "kube-apiserver-cert-syncer-9" in pod "kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container endpoint collection for pod "kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal" container "kube-apiserver-cert-syncer-9": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Unable to gather previous container logs: previous terminated container "kube-apiserver-insecure-readyz-9" in pod "kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Unable to gather previous container logs: previous terminated container "setup" in pod "kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Skipping container endpoint collection for pod "kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal" container "setup": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:46         Gathering data for pod "kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:47         Unable to gather previous container logs: previous terminated container "kube-apiserver-9" in pod "kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:47         Unable to gather previous container logs: previous terminated container "kube-apiserver-cert-syncer-9" in pod "kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:47         Skipping container endpoint collection for pod "kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal" container "kube-apiserver-cert-syncer-9": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:47         Unable to gather previous container logs: previous terminated container "kube-apiserver-insecure-readyz-9" in pod "kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:47         Unable to gather previous container logs: previous terminated container "setup" in pod "kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:47         Skipping container endpoint collection for pod "kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal" container "setup": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:47         Gathering data for pod "kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:47         Unable to gather previous container logs: previous terminated container "kube-apiserver-9" in pod "kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:48         Unable to gather previous container logs: previous terminated container "kube-apiserver-cert-syncer-9" in pod "kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:48         Skipping container endpoint collection for pod "kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal" container "kube-apiserver-cert-syncer-9": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:48         Unable to gather previous container logs: previous terminated container "kube-apiserver-insecure-readyz-9" in pod "kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Unable to gather previous container logs: previous terminated container "setup" in pod "kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container endpoint collection for pod "kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal" container "setup": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-2-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-2-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-2-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-2-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-2-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-2-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-3-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-3-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-4-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-4-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-6-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-6-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-6-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-6-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-6-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-6-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-7-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-7-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-8-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-8-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-8-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-8-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-8-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-8-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-9-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-9-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-9-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-9-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Gathering data for pod "revision-pruner-9-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:49         Skipping container data collection for pod "revision-pruner-9-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:49 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:49 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:49 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:42:49     Gathering related object reference information for ClusterOperator "kube-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:42:49     Found related object "kubecontrollermanagers.operator.openshift.io/cluster" for ClusterOperator "kube-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:42:49     Found related object "namespaces/openshift-config" for ClusterOperator "kube-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:42:49     Found related object "namespaces/openshift-config-managed" for ClusterOperator "kube-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:42:49     Found related object "namespaces/openshift-kube-controller-manager" for ClusterOperator "kube-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:42:49     Found related object "namespaces/openshift-kube-controller-manager-operator" for ClusterOperator "kube-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:42:50 Gathering data for ns/openshift-kube-controller-manager...
# [must-gather-58kh8] POD 2020/02/21 11:42:50     Collecting resources for namespace "openshift-kube-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:42:50     Gathering pod data for namespace "openshift-kube-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-2-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-2-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-2-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-2-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-2-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-2-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-3-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-3-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-3-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-3-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-3-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-3-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-4-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-4-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-4-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-4-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-4-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-4-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-5-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-5-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-5-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-5-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "installer-5-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container data collection for pod "installer-5-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Unable to gather previous container logs: previous terminated container "kube-controller-manager-5" in pod "kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Unable to gather previous container logs: previous terminated container "kube-controller-manager-cert-syncer-5" in pod "kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container endpoint collection for pod "kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal" container "kube-controller-manager-cert-syncer-5": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Unable to gather previous container logs: previous terminated container "wait-for-host-port" in pod "kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Skipping container endpoint collection for pod "kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal" container "wait-for-host-port": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:50         Gathering data for pod "kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:51         Unable to gather previous container logs: previous terminated container "kube-controller-manager-5" in pod "kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:51         Unable to gather previous container logs: previous terminated container "kube-controller-manager-cert-syncer-5" in pod "kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:51         Skipping container endpoint collection for pod "kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal" container "kube-controller-manager-cert-syncer-5": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:51         Unable to gather previous container logs: previous terminated container "wait-for-host-port" in pod "kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:51         Skipping container endpoint collection for pod "kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal" container "wait-for-host-port": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:51         Gathering data for pod "kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Unable to gather previous container logs: previous terminated container "kube-controller-manager-5" in pod "kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Unable to gather previous container logs: previous terminated container "kube-controller-manager-cert-syncer-5" in pod "kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container endpoint collection for pod "kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal" container "kube-controller-manager-cert-syncer-5": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Unable to gather previous container logs: previous terminated container "wait-for-host-port" in pod "kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container endpoint collection for pod "kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal" container "wait-for-host-port": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-2-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-2-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-2-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-2-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-2-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-2-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-3-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-3-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-3-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-3-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-3-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-3-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-4-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-4-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-4-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-4-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-4-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-4-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-5-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-5-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-5-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-5-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Gathering data for pod "revision-pruner-5-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:52         Skipping container data collection for pod "revision-pruner-5-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:53 Gathering data for ns/openshift-kube-controller-manager-operator...
# [must-gather-58kh8] POD 2020/02/21 11:42:53     Collecting resources for namespace "openshift-kube-controller-manager-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:53     Gathering pod data for namespace "openshift-kube-controller-manager-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:53         Gathering data for pod "kube-controller-manager-operator-6b86f4675b-pjddn"
# [must-gather-58kh8] POD 2020/02/21 11:42:54 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:54 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:54 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:42:54     Gathering related object reference information for ClusterOperator "kube-scheduler"...
# [must-gather-58kh8] POD 2020/02/21 11:42:54     Found related object "kubeschedulers.operator.openshift.io/cluster" for ClusterOperator "kube-scheduler"...
# [must-gather-58kh8] POD 2020/02/21 11:42:54     Found related object "namespaces/openshift-config" for ClusterOperator "kube-scheduler"...
# [must-gather-58kh8] POD 2020/02/21 11:42:54     Found related object "namespaces/openshift-kube-scheduler" for ClusterOperator "kube-scheduler"...
# [must-gather-58kh8] POD 2020/02/21 11:42:54     Found related object "namespaces/openshift-kube-scheduler-operator" for ClusterOperator "kube-scheduler"...
# [must-gather-58kh8] POD 2020/02/21 11:42:54 Gathering data for ns/openshift-kube-scheduler...
# [must-gather-58kh8] POD 2020/02/21 11:42:54     Collecting resources for namespace "openshift-kube-scheduler"...
# [must-gather-58kh8] POD 2020/02/21 11:42:54     Gathering pod data for namespace "openshift-kube-scheduler"...
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "installer-2-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Skipping container data collection for pod "installer-2-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "installer-3-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Skipping container data collection for pod "installer-3-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "installer-5-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Skipping container data collection for pod "installer-5-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "installer-5-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Skipping container data collection for pod "installer-5-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "installer-5-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Skipping container data collection for pod "installer-5-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "installer-6-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Skipping container data collection for pod "installer-6-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "installer-6-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Skipping container data collection for pod "installer-6-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "installer-6-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Skipping container data collection for pod "installer-6-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Unable to gather previous container logs: previous terminated container "scheduler" in pod "openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Unable to gather previous container logs: previous terminated container "wait-for-host-port" in pod "openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Skipping container endpoint collection for pod "openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal" container "wait-for-host-port": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Gathering data for pod "openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:54         Unable to gather previous container logs: previous terminated container "scheduler" in pod "openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:55         Unable to gather previous container logs: previous terminated container "wait-for-host-port" in pod "openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:55         Skipping container endpoint collection for pod "openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal" container "wait-for-host-port": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:55         Gathering data for pod "openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:55         Unable to gather previous container logs: previous terminated container "scheduler" in pod "openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Unable to gather previous container logs: previous terminated container "wait-for-host-port" in pod "openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container endpoint collection for pod "openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal" container "wait-for-host-port": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Gathering data for pod "revision-pruner-2-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container data collection for pod "revision-pruner-2-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Gathering data for pod "revision-pruner-3-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container data collection for pod "revision-pruner-3-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Gathering data for pod "revision-pruner-5-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container data collection for pod "revision-pruner-5-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Gathering data for pod "revision-pruner-5-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container data collection for pod "revision-pruner-5-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Gathering data for pod "revision-pruner-5-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container data collection for pod "revision-pruner-5-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Gathering data for pod "revision-pruner-6-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container data collection for pod "revision-pruner-6-ip-10-0-131-55.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Gathering data for pod "revision-pruner-6-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container data collection for pod "revision-pruner-6-ip-10-0-150-228.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Gathering data for pod "revision-pruner-6-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container data collection for pod "revision-pruner-6-ip-10-0-173-76.us-east-2.compute.internal": Pod not running
# [must-gather-58kh8] POD 2020/02/21 11:42:56 Gathering data for ns/openshift-kube-scheduler-operator...
# [must-gather-58kh8] POD 2020/02/21 11:42:56     Collecting resources for namespace "openshift-kube-scheduler-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:56     Gathering pod data for namespace "openshift-kube-scheduler-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Gathering data for pod "openshift-kube-scheduler-operator-64f94b4cf4-498mp"
# [must-gather-58kh8] POD 2020/02/21 11:42:56         Skipping container endpoint collection for pod "openshift-kube-scheduler-operator-64f94b4cf4-498mp" container "kube-scheduler-operator-container": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:57 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:57 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:57 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Gathering related object reference information for ClusterOperator "machine-api"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Found related object "namespaces/openshift-machine-api" for ClusterOperator "machine-api"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Found related object "machines.machine.openshift.io" for ClusterOperator "machine-api"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Found related object "machinesets.machine.openshift.io" for ClusterOperator "machine-api"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:57 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:42:57 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Gathering related object reference information for ClusterOperator "machine-config"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Found related object "namespaces/openshift-machine-config-operator" for ClusterOperator "machine-config"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Found related object "machineconfigpools.machineconfiguration.openshift.io/master" for ClusterOperator "machine-config"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Found related object "machineconfigpools.machineconfiguration.openshift.io/worker" for ClusterOperator "machine-config"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Found related object "controllerconfigs.machineconfiguration.openshift.io/machine-config-controller" for ClusterOperator "machine-config"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57 Gathering data for ns/openshift-machine-config-operator...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Collecting resources for namespace "openshift-machine-config-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57     Gathering pod data for namespace "openshift-machine-config-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:42:57         Gathering data for pod "etcd-quorum-guard-777d76fbdd-4vqq5"
# [must-gather-58kh8] POD 2020/02/21 11:42:57         Unable to gather previous container logs: previous terminated container "guard" in pod "etcd-quorum-guard-777d76fbdd-4vqq5" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:57         Skipping container endpoint collection for pod "etcd-quorum-guard-777d76fbdd-4vqq5" container "guard": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:57         Gathering data for pod "etcd-quorum-guard-777d76fbdd-fnzkh"
# [must-gather-58kh8] POD 2020/02/21 11:42:57         Unable to gather previous container logs: previous terminated container "guard" in pod "etcd-quorum-guard-777d76fbdd-fnzkh" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:57         Skipping container endpoint collection for pod "etcd-quorum-guard-777d76fbdd-fnzkh" container "guard": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:57         Gathering data for pod "etcd-quorum-guard-777d76fbdd-p9vhn"
# [must-gather-58kh8] POD 2020/02/21 11:42:58         Unable to gather previous container logs: previous terminated container "guard" in pod "etcd-quorum-guard-777d76fbdd-p9vhn" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:58         Skipping container endpoint collection for pod "etcd-quorum-guard-777d76fbdd-p9vhn" container "guard": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:58         Gathering data for pod "kubelet-bootstrap-cred-manager-cg8c2"
# [must-gather-58kh8] POD 2020/02/21 11:42:59         Unable to gather previous container logs: previous terminated container "kubelet-bootstrap-cred-manager" in pod "kubelet-bootstrap-cred-manager-cg8c2" not found
# [must-gather-58kh8] POD 2020/02/21 11:42:59         Skipping container endpoint collection for pod "kubelet-bootstrap-cred-manager-cg8c2" container "kubelet-bootstrap-cred-manager": No ports
# [must-gather-58kh8] POD 2020/02/21 11:42:59         Gathering data for pod "kubelet-bootstrap-cred-manager-mdztn"
# [must-gather-58kh8] POD 2020/02/21 11:43:03         Unable to gather previous container logs: previous terminated container "kubelet-bootstrap-cred-manager" in pod "kubelet-bootstrap-cred-manager-mdztn" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:03         Skipping container endpoint collection for pod "kubelet-bootstrap-cred-manager-mdztn" container "kubelet-bootstrap-cred-manager": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:03         Gathering data for pod "kubelet-bootstrap-cred-manager-mnmhl"
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Unable to gather previous container logs: previous terminated container "kubelet-bootstrap-cred-manager" in pod "kubelet-bootstrap-cred-manager-mnmhl" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Skipping container endpoint collection for pod "kubelet-bootstrap-cred-manager-mnmhl" container "kubelet-bootstrap-cred-manager": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Gathering data for pod "machine-config-controller-84495957cf-lk4wb"
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Unable to gather previous container logs: previous terminated container "machine-config-controller" in pod "machine-config-controller-84495957cf-lk4wb" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Skipping container endpoint collection for pod "machine-config-controller-84495957cf-lk4wb" container "machine-config-controller": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Gathering data for pod "machine-config-daemon-bn76l"
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-bn76l" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Skipping container endpoint collection for pod "machine-config-daemon-bn76l" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Gathering data for pod "machine-config-daemon-bqtht"
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-bqtht" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Skipping container endpoint collection for pod "machine-config-daemon-bqtht" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Gathering data for pod "machine-config-daemon-fjqbt"
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-fjqbt" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Skipping container endpoint collection for pod "machine-config-daemon-fjqbt" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Gathering data for pod "machine-config-daemon-hdm27"
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-hdm27" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Skipping container endpoint collection for pod "machine-config-daemon-hdm27" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:05         Gathering data for pod "machine-config-daemon-lfwnq"
# [must-gather-58kh8] POD 2020/02/21 11:43:06         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-lfwnq" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:06         Skipping container endpoint collection for pod "machine-config-daemon-lfwnq" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:06         Gathering data for pod "machine-config-daemon-ml44b"
# [must-gather-58kh8] POD 2020/02/21 11:43:06         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-ml44b" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:06         Skipping container endpoint collection for pod "machine-config-daemon-ml44b" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:06         Gathering data for pod "machine-config-daemon-p9zd6"
# [must-gather-58kh8] POD 2020/02/21 11:43:06         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-p9zd6" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:06         Skipping container endpoint collection for pod "machine-config-daemon-p9zd6" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:06         Gathering data for pod "machine-config-daemon-ps76x"
# [must-gather-58kh8] POD 2020/02/21 11:43:07         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-ps76x" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:07         Skipping container endpoint collection for pod "machine-config-daemon-ps76x" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:07         Gathering data for pod "machine-config-daemon-qqhvz"
# [must-gather-58kh8] POD 2020/02/21 11:43:07         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-qqhvz" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:07         Skipping container endpoint collection for pod "machine-config-daemon-qqhvz" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:07         Gathering data for pod "machine-config-daemon-s7f54"
# [must-gather-58kh8] POD 2020/02/21 11:43:08         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-s7f54" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:08         Skipping container endpoint collection for pod "machine-config-daemon-s7f54" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:08         Gathering data for pod "machine-config-daemon-sb8sr"
# [must-gather-58kh8] POD 2020/02/21 11:43:08         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-sb8sr" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:08         Skipping container endpoint collection for pod "machine-config-daemon-sb8sr" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:08         Gathering data for pod "machine-config-daemon-sn8f9"
# [must-gather-58kh8] POD 2020/02/21 11:43:08         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-sn8f9" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:08         Skipping container endpoint collection for pod "machine-config-daemon-sn8f9" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:08         Gathering data for pod "machine-config-daemon-v6s2m"
# [must-gather-58kh8] POD 2020/02/21 11:43:09         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-v6s2m" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:09         Skipping container endpoint collection for pod "machine-config-daemon-v6s2m" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:09         Gathering data for pod "machine-config-daemon-x4nrm"
# [must-gather-58kh8] POD 2020/02/21 11:43:09         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-x4nrm" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:09         Skipping container endpoint collection for pod "machine-config-daemon-x4nrm" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:09         Gathering data for pod "machine-config-daemon-zrx7r"
# [must-gather-58kh8] POD 2020/02/21 11:43:10         Unable to gather previous container logs: previous terminated container "machine-config-daemon" in pod "machine-config-daemon-zrx7r" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:10         Skipping container endpoint collection for pod "machine-config-daemon-zrx7r" container "machine-config-daemon": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:10         Gathering data for pod "machine-config-operator-db69f9f4b-llnrn"
# [must-gather-58kh8] POD 2020/02/21 11:43:10         Unable to gather previous container logs: previous terminated container "machine-config-operator" in pod "machine-config-operator-db69f9f4b-llnrn" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:10         Skipping container endpoint collection for pod "machine-config-operator-db69f9f4b-llnrn" container "machine-config-operator": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:10         Gathering data for pod "machine-config-server-68v65"
# [must-gather-58kh8] POD 2020/02/21 11:43:10         Unable to gather previous container logs: previous terminated container "machine-config-server" in pod "machine-config-server-68v65" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:10         Skipping container endpoint collection for pod "machine-config-server-68v65" container "machine-config-server": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:10         Gathering data for pod "machine-config-server-dw77g"
# [must-gather-58kh8] POD 2020/02/21 11:43:11         Unable to gather previous container logs: previous terminated container "machine-config-server" in pod "machine-config-server-dw77g" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:11         Skipping container endpoint collection for pod "machine-config-server-dw77g" container "machine-config-server": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:11         Gathering data for pod "machine-config-server-v8l9q"
# [must-gather-58kh8] POD 2020/02/21 11:43:11         Unable to gather previous container logs: previous terminated container "machine-config-server" in pod "machine-config-server-v8l9q" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:11         Skipping container endpoint collection for pod "machine-config-server-v8l9q" container "machine-config-server": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:12 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:43:12 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:43:12 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:43:12     Gathering related object reference information for ClusterOperator "marketplace"...
# [must-gather-58kh8] POD 2020/02/21 11:43:12     Found related object "namespaces/openshift-marketplace" for ClusterOperator "marketplace"...
# [must-gather-58kh8] POD 2020/02/21 11:43:12     Found related object "OperatorSource.operators.coreos.com" for ClusterOperator "marketplace"...
# [must-gather-58kh8] POD 2020/02/21 11:43:12     Found related object "CatalogSourceConfig.operators.coreos.com" for ClusterOperator "marketplace"...
# [must-gather-58kh8] POD 2020/02/21 11:43:12     Found related object "CatalogSource.operators.coreos.com" for ClusterOperator "marketplace"...
# [must-gather-58kh8] POD 2020/02/21 11:43:12 Gathering data for ns/openshift-marketplace...
# [must-gather-58kh8] POD 2020/02/21 11:43:12     Collecting resources for namespace "openshift-marketplace"...
# [must-gather-58kh8] POD 2020/02/21 11:43:12     Gathering pod data for namespace "openshift-marketplace"...
# [must-gather-58kh8] POD 2020/02/21 11:43:12         Gathering data for pod "certified-operators-fd9b95b9f-htd9p"
# [must-gather-58kh8] POD 2020/02/21 11:43:13         Unable to gather previous container logs: previous terminated container "certified-operators" in pod "certified-operators-fd9b95b9f-htd9p" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:13         Gathering data for pod "community-operators-7db6b74f69-76rmm"
# [must-gather-58kh8] POD 2020/02/21 11:43:13         Unable to gather previous container logs: previous terminated container "community-operators" in pod "community-operators-7db6b74f69-76rmm" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:13         Gathering data for pod "marketplace-operator-78775467c7-ldb6m"
# [must-gather-58kh8] POD 2020/02/21 11:43:13         Unable to gather previous container logs: previous terminated container "marketplace-operator" in pod "marketplace-operator-78775467c7-ldb6m" not found
# [must-gather-58kh8] POD E0221 11:43:13.358025     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod f4a2fbbdf1bd646b64376ac73bc5e203cfeb7648bcf8872b711ce33d73b96636, uid : exit status 1: 2020/02/21 11:43:13 socat[719368] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:43:13.409864     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod f4a2fbbdf1bd646b64376ac73bc5e203cfeb7648bcf8872b711ce33d73b96636, uid : exit status 1: 2020/02/21 11:43:13 socat[719375] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:43:13.454010     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod f4a2fbbdf1bd646b64376ac73bc5e203cfeb7648bcf8872b711ce33d73b96636, uid : exit status 1: 2020/02/21 11:43:13 socat[719382] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:43:13         Gathering data for pod "redhat-operators-5bddbf4558-9qh4x"
# [must-gather-58kh8] POD 2020/02/21 11:43:13         Unable to gather previous container logs: previous terminated container "redhat-operators" in pod "redhat-operators-5bddbf4558-9qh4x" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:14 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:43:14 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:43:14 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:43:14     Gathering related object reference information for ClusterOperator "monitoring"...
# [must-gather-58kh8] POD 2020/02/21 11:43:14     Found related object "monitoring.operator.openshift.io/cluster" for ClusterOperator "monitoring"...
# [must-gather-58kh8] POD 2020/02/21 11:43:14     Found related object "namespaces/openshift-monitoring" for ClusterOperator "monitoring"...
# [must-gather-58kh8] POD 2020/02/21 11:43:15 Gathering data for ns/openshift-monitoring...
# [must-gather-58kh8] POD 2020/02/21 11:43:15     Collecting resources for namespace "openshift-monitoring"...
# [must-gather-58kh8] POD 2020/02/21 11:43:15     Gathering pod data for namespace "openshift-monitoring"...
# [must-gather-58kh8] POD 2020/02/21 11:43:15         Gathering data for pod "alertmanager-main-0"
# [must-gather-58kh8] POD 2020/02/21 11:43:15         Unable to gather previous container logs: previous terminated container "alertmanager" in pod "alertmanager-main-0" not found
# [must-gather-58kh8] POD E0221 11:43:15.218759     109 portforward.go:331] an error occurred forwarding 37587 -> 9094: error forwarding port 9094 to pod 05e11b349d725b7d7da2d454e3b50d64505c36fdd02dcd6acd0a34a24aeb8d03, uid : exit status 1: 2020/02/21 11:43:15 socat[1774955] E connect(5, AF=2 127.0.0.1:9094, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:43:15.252336     109 portforward.go:331] an error occurred forwarding 37587 -> 9094: error forwarding port 9094 to pod 05e11b349d725b7d7da2d454e3b50d64505c36fdd02dcd6acd0a34a24aeb8d03, uid : exit status 1: 2020/02/21 11:43:15 socat[1774962] E connect(5, AF=2 127.0.0.1:9094, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:43:15.286513     109 portforward.go:331] an error occurred forwarding 37587 -> 9094: error forwarding port 9094 to pod 05e11b349d725b7d7da2d454e3b50d64505c36fdd02dcd6acd0a34a24aeb8d03, uid : exit status 1: 2020/02/21 11:43:15 socat[1774969] E connect(5, AF=2 127.0.0.1:9094, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:43:15         Unable to gather previous container logs: previous terminated container "config-reloader" in pod "alertmanager-main-0" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:15         Skipping container endpoint collection for pod "alertmanager-main-0" container "config-reloader": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:15         Unable to gather previous container logs: previous terminated container "alertmanager-proxy" in pod "alertmanager-main-0" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:15         Gathering data for pod "alertmanager-main-1"
# [must-gather-58kh8] POD 2020/02/21 11:43:15         Unable to gather previous container logs: previous terminated container "alertmanager" in pod "alertmanager-main-1" not found
# [must-gather-58kh8] POD E0221 11:43:15.503985     109 portforward.go:331] an error occurred forwarding 37587 -> 9094: error forwarding port 9094 to pod 854c8647435ba569189c8f70a7442402423003c45f6424edfb8a5d49af0ce56f, uid : exit status 1: 2020/02/21 11:43:15 socat[1752388] E connect(5, AF=2 127.0.0.1:9094, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:43:15.544596     109 portforward.go:331] an error occurred forwarding 37587 -> 9094: error forwarding port 9094 to pod 854c8647435ba569189c8f70a7442402423003c45f6424edfb8a5d49af0ce56f, uid : exit status 1: 2020/02/21 11:43:15 socat[1752395] E connect(5, AF=2 127.0.0.1:9094, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:43:15.579630     109 portforward.go:331] an error occurred forwarding 37587 -> 9094: error forwarding port 9094 to pod 854c8647435ba569189c8f70a7442402423003c45f6424edfb8a5d49af0ce56f, uid : exit status 1: 2020/02/21 11:43:15 socat[1752402] E connect(5, AF=2 127.0.0.1:9094, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:43:15         Unable to gather previous container logs: previous terminated container "config-reloader" in pod "alertmanager-main-1" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:15         Skipping container endpoint collection for pod "alertmanager-main-1" container "config-reloader": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:16         Unable to gather previous container logs: previous terminated container "alertmanager-proxy" in pod "alertmanager-main-1" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:16         Gathering data for pod "alertmanager-main-2"
# [must-gather-58kh8] POD 2020/02/21 11:43:16         Unable to gather previous container logs: previous terminated container "alertmanager" in pod "alertmanager-main-2" not found
# [must-gather-58kh8] POD E0221 11:43:16.518555     109 portforward.go:331] an error occurred forwarding 37587 -> 9094: error forwarding port 9094 to pod b37ff624a615d8e0e08fa9d7d594a10921ee6dc38660e5b59dd92e5d49419573, uid : exit status 1: 2020/02/21 11:43:16 socat[1740861] E connect(5, AF=2 127.0.0.1:9094, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:43:16.554386     109 portforward.go:331] an error occurred forwarding 37587 -> 9094: error forwarding port 9094 to pod b37ff624a615d8e0e08fa9d7d594a10921ee6dc38660e5b59dd92e5d49419573, uid : exit status 1: 2020/02/21 11:43:16 socat[1740868] E connect(5, AF=2 127.0.0.1:9094, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:43:16.588588     109 portforward.go:331] an error occurred forwarding 37587 -> 9094: error forwarding port 9094 to pod b37ff624a615d8e0e08fa9d7d594a10921ee6dc38660e5b59dd92e5d49419573, uid : exit status 1: 2020/02/21 11:43:16 socat[1740875] E connect(5, AF=2 127.0.0.1:9094, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:43:16         Unable to gather previous container logs: previous terminated container "config-reloader" in pod "alertmanager-main-2" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:16         Skipping container endpoint collection for pod "alertmanager-main-2" container "config-reloader": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:17         Unable to gather previous container logs: previous terminated container "alertmanager-proxy" in pod "alertmanager-main-2" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:17         Gathering data for pod "cluster-monitoring-operator-585c8c44d9-hzmnw"
# [must-gather-58kh8] POD 2020/02/21 11:43:17         Unable to gather previous container logs: previous terminated container "cluster-monitoring-operator" in pod "cluster-monitoring-operator-585c8c44d9-hzmnw" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:17         Gathering data for pod "grafana-69685f986d-rl757"
# [must-gather-58kh8] POD 2020/02/21 11:43:18         Unable to gather previous container logs: previous terminated container "grafana" in pod "grafana-69685f986d-rl757" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:18         Unable to gather previous container logs: previous terminated container "grafana-proxy" in pod "grafana-69685f986d-rl757" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:18         Gathering data for pod "kube-state-metrics-7c884764fd-jdf57"
# [must-gather-58kh8] POD 2020/02/21 11:43:18         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy-main" in pod "kube-state-metrics-7c884764fd-jdf57" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:19         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy-self" in pod "kube-state-metrics-7c884764fd-jdf57" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:19         Unable to gather previous container logs: previous terminated container "kube-state-metrics" in pod "kube-state-metrics-7c884764fd-jdf57" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:19         Skipping container endpoint collection for pod "kube-state-metrics-7c884764fd-jdf57" container "kube-state-metrics": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:19         Gathering data for pod "node-exporter-86k7k"
# [must-gather-58kh8] POD 2020/02/21 11:43:20         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-86k7k" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:20         Skipping container endpoint collection for pod "node-exporter-86k7k" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:20         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-86k7k" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:20         Gathering data for pod "node-exporter-d49v5"
# [must-gather-58kh8] POD 2020/02/21 11:43:20         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-d49v5" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:20         Skipping container endpoint collection for pod "node-exporter-d49v5" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:21         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-d49v5" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:21         Gathering data for pod "node-exporter-fk267"
# [must-gather-58kh8] POD 2020/02/21 11:43:21         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-fk267" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:21         Skipping container endpoint collection for pod "node-exporter-fk267" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:22         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-fk267" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:22         Gathering data for pod "node-exporter-gqgjk"
# [must-gather-58kh8] POD 2020/02/21 11:43:22         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-gqgjk" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:22         Skipping container endpoint collection for pod "node-exporter-gqgjk" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:22         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-gqgjk" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:22         Gathering data for pod "node-exporter-hgtjd"
# [must-gather-58kh8] POD 2020/02/21 11:43:23         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-hgtjd" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:23         Skipping container endpoint collection for pod "node-exporter-hgtjd" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:23         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-hgtjd" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:23         Gathering data for pod "node-exporter-lkxvx"
# [must-gather-58kh8] POD 2020/02/21 11:43:24         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-lkxvx" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:24         Skipping container endpoint collection for pod "node-exporter-lkxvx" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:24         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-lkxvx" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:24         Gathering data for pod "node-exporter-nbkxw"
# [must-gather-58kh8] POD 2020/02/21 11:43:24         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-nbkxw" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:24         Skipping container endpoint collection for pod "node-exporter-nbkxw" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:25         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-nbkxw" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:25         Gathering data for pod "node-exporter-p7vrw"
# [must-gather-58kh8] POD 2020/02/21 11:43:25         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-p7vrw" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:25         Skipping container endpoint collection for pod "node-exporter-p7vrw" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:26         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-p7vrw" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:26         Gathering data for pod "node-exporter-pl95f"
# [must-gather-58kh8] POD 2020/02/21 11:43:26         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-pl95f" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:26         Skipping container endpoint collection for pod "node-exporter-pl95f" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:26         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-pl95f" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:26         Gathering data for pod "node-exporter-prbc5"
# [must-gather-58kh8] POD 2020/02/21 11:43:27         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-prbc5" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:27         Skipping container endpoint collection for pod "node-exporter-prbc5" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:27         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-prbc5" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:27         Gathering data for pod "node-exporter-r9ckd"
# [must-gather-58kh8] POD 2020/02/21 11:43:28         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-r9ckd" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:28         Skipping container endpoint collection for pod "node-exporter-r9ckd" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:28         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-r9ckd" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:28         Gathering data for pod "node-exporter-vm4np"
# [must-gather-58kh8] POD 2020/02/21 11:43:28         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-vm4np" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:28         Skipping container endpoint collection for pod "node-exporter-vm4np" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:29         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-vm4np" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:29         Gathering data for pod "node-exporter-wn9hs"
# [must-gather-58kh8] POD 2020/02/21 11:43:29         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-wn9hs" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:29         Skipping container endpoint collection for pod "node-exporter-wn9hs" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:30         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-wn9hs" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:30         Gathering data for pod "node-exporter-xddfk"
# [must-gather-58kh8] POD 2020/02/21 11:43:30         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-xddfk" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:30         Skipping container endpoint collection for pod "node-exporter-xddfk" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:30         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-xddfk" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:30         Gathering data for pod "node-exporter-xhwb8"
# [must-gather-58kh8] POD 2020/02/21 11:43:31         Unable to gather previous container logs: previous terminated container "node-exporter" in pod "node-exporter-xhwb8" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:31         Skipping container endpoint collection for pod "node-exporter-xhwb8" container "node-exporter": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:31         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "node-exporter-xhwb8" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:31         Gathering data for pod "openshift-state-metrics-7c76b98c77-kxdjp"
# [must-gather-58kh8] POD 2020/02/21 11:43:32         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy-main" in pod "openshift-state-metrics-7c76b98c77-kxdjp" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:32         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy-self" in pod "openshift-state-metrics-7c76b98c77-kxdjp" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:32         Unable to gather previous container logs: previous terminated container "openshift-state-metrics" in pod "openshift-state-metrics-7c76b98c77-kxdjp" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:32         Skipping container endpoint collection for pod "openshift-state-metrics-7c76b98c77-kxdjp" container "openshift-state-metrics": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:32         Gathering data for pod "prometheus-adapter-986777885-d7rjf"
# [must-gather-58kh8] POD 2020/02/21 11:43:33         Unable to gather previous container logs: previous terminated container "prometheus-adapter" in pod "prometheus-adapter-986777885-d7rjf" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:33         Gathering data for pod "prometheus-adapter-986777885-vdcmj"
# [must-gather-58kh8] POD 2020/02/21 11:43:33         Unable to gather previous container logs: previous terminated container "prometheus-adapter" in pod "prometheus-adapter-986777885-vdcmj" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:33         Gathering data for pod "prometheus-k8s-0"
# [must-gather-58kh8] POD 2020/02/21 11:43:34         Skipping container endpoint collection for pod "prometheus-k8s-0" container "prometheus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:34         Unable to gather previous container logs: previous terminated container "prometheus-config-reloader" in pod "prometheus-k8s-0" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:34         Skipping container endpoint collection for pod "prometheus-k8s-0" container "prometheus-config-reloader": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:34         Unable to gather previous container logs: previous terminated container "rules-configmap-reloader" in pod "prometheus-k8s-0" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:34         Skipping container endpoint collection for pod "prometheus-k8s-0" container "rules-configmap-reloader": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:35         Unable to gather previous container logs: previous terminated container "prometheus-proxy" in pod "prometheus-k8s-0" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:35         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "prometheus-k8s-0" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:36         Unable to gather previous container logs: previous terminated container "prom-label-proxy" in pod "prometheus-k8s-0" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:36         Skipping container endpoint collection for pod "prometheus-k8s-0" container "prom-label-proxy": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:36         Gathering data for pod "prometheus-k8s-1"
# [must-gather-58kh8] POD 2020/02/21 11:43:36         Skipping container endpoint collection for pod "prometheus-k8s-1" container "prometheus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:36         Unable to gather previous container logs: previous terminated container "prometheus-config-reloader" in pod "prometheus-k8s-1" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:36         Skipping container endpoint collection for pod "prometheus-k8s-1" container "prometheus-config-reloader": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:37         Unable to gather previous container logs: previous terminated container "rules-configmap-reloader" in pod "prometheus-k8s-1" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:37         Skipping container endpoint collection for pod "prometheus-k8s-1" container "rules-configmap-reloader": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:37         Unable to gather previous container logs: previous terminated container "prometheus-proxy" in pod "prometheus-k8s-1" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:38         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "prometheus-k8s-1" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:38         Unable to gather previous container logs: previous terminated container "prom-label-proxy" in pod "prometheus-k8s-1" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:38         Skipping container endpoint collection for pod "prometheus-k8s-1" container "prom-label-proxy": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:38         Gathering data for pod "prometheus-operator-7c8568cc64-x5vtt"
# [must-gather-58kh8] POD 2020/02/21 11:43:38         Unable to gather previous container logs: previous terminated container "prometheus-operator" in pod "prometheus-operator-7c8568cc64-x5vtt" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:38         Gathering data for pod "telemeter-client-944599596-c5jzs"
# [must-gather-58kh8] POD 2020/02/21 11:43:39         Unable to gather previous container logs: previous terminated container "telemeter-client" in pod "telemeter-client-944599596-c5jzs" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:39         Unable to gather previous container logs: previous terminated container "reload" in pod "telemeter-client-944599596-c5jzs" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:39         Skipping container endpoint collection for pod "telemeter-client-944599596-c5jzs" container "reload": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:40         Unable to gather previous container logs: previous terminated container "kube-rbac-proxy" in pod "telemeter-client-944599596-c5jzs" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:41 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:43:41 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:43:41 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Gathering related object reference information for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "configmaps/applied-cluster" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "customresourcedefinitions.apiextensions.k8s.io/network-attachment-definitions.k8s.cni.cncf.io" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "namespaces/openshift-multus" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "clusterroles.rbac.authorization.k8s.io/multus" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "serviceaccounts/multus" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "clusterrolebindings.rbac.authorization.k8s.io/multus" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "daemonsets.apps/multus" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "services/multus-admission-controller" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "clusterroles.rbac.authorization.k8s.io/multus-admission-controller-webhook" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "clusterrolebindings.rbac.authorization.k8s.io/multus-admission-controller-webhook" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "validatingwebhookconfigurations.admissionregistration.k8s.io/multus.openshift.io" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "configmaps/openshift-service-ca" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "daemonsets.apps/multus-admission-controller" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "namespaces/openshift-sdn" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "customresourcedefinitions.apiextensions.k8s.io/clusternetworks.network.openshift.io" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "customresourcedefinitions.apiextensions.k8s.io/hostsubnets.network.openshift.io" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "customresourcedefinitions.apiextensions.k8s.io/netnamespaces.network.openshift.io" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "customresourcedefinitions.apiextensions.k8s.io/egressnetworkpolicies.network.openshift.io" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "clusterroles.rbac.authorization.k8s.io/openshift-sdn" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "serviceaccounts/sdn" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "clusterrolebindings.rbac.authorization.k8s.io/openshift-sdn" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "serviceaccounts/sdn-controller" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "clusterroles.rbac.authorization.k8s.io/openshift-sdn-controller" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "clusterrolebindings.rbac.authorization.k8s.io/openshift-sdn-controller" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "roles.rbac.authorization.k8s.io/openshift-sdn-controller-leaderelection" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "rolebindings.rbac.authorization.k8s.io/openshift-sdn-controller-leaderelection" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "clusternetworks.network.openshift.io/default" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "prometheusrules.monitoring.coreos.com/networking-rules" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "daemonsets.apps/sdn-controller" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "servicemonitors.monitoring.coreos.com/monitor-sdn" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "services/sdn" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "roles.rbac.authorization.k8s.io/prometheus-k8s" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "rolebindings.rbac.authorization.k8s.io/prometheus-k8s" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "daemonsets.apps/ovs" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "configmaps/sdn-config" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Found related object "daemonsets.apps/sdn" for ClusterOperator "network"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41 Gathering data for ns/openshift-multus...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Collecting resources for namespace "openshift-multus"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41     Gathering pod data for namespace "openshift-multus"...
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Gathering data for pod "multus-66xjb"
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-66xjb" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Skipping container endpoint collection for pod "multus-66xjb" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-66xjb" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Skipping container endpoint collection for pod "multus-66xjb" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Gathering data for pod "multus-8c89h"
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-8c89h" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Skipping container endpoint collection for pod "multus-8c89h" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-8c89h" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Skipping container endpoint collection for pod "multus-8c89h" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:41         Gathering data for pod "multus-9n5xm"
# [must-gather-58kh8] POD 2020/02/21 11:43:42         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-9n5xm" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:42         Skipping container endpoint collection for pod "multus-9n5xm" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:42         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-9n5xm" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:42         Skipping container endpoint collection for pod "multus-9n5xm" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:42         Gathering data for pod "multus-admission-controller-lrvhs"
# [must-gather-58kh8] POD 2020/02/21 11:43:43         Unable to gather previous container logs: previous terminated container "multus-admission-controller" in pod "multus-admission-controller-lrvhs" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:43         Skipping container endpoint collection for pod "multus-admission-controller-lrvhs" container "multus-admission-controller": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:43         Gathering data for pod "multus-admission-controller-nhpzv"
# [must-gather-58kh8] POD 2020/02/21 11:43:43         Unable to gather previous container logs: previous terminated container "multus-admission-controller" in pod "multus-admission-controller-nhpzv" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:43         Skipping container endpoint collection for pod "multus-admission-controller-nhpzv" container "multus-admission-controller": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:43         Gathering data for pod "multus-admission-controller-pkrr8"
# [must-gather-58kh8] POD 2020/02/21 11:43:43         Unable to gather previous container logs: previous terminated container "multus-admission-controller" in pod "multus-admission-controller-pkrr8" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:43         Skipping container endpoint collection for pod "multus-admission-controller-pkrr8" container "multus-admission-controller": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:43         Gathering data for pod "multus-bftmf"
# [must-gather-58kh8] POD 2020/02/21 11:43:44         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-bftmf" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:44         Skipping container endpoint collection for pod "multus-bftmf" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:44         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-bftmf" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:44         Skipping container endpoint collection for pod "multus-bftmf" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:44         Gathering data for pod "multus-cqsb7"
# [must-gather-58kh8] POD 2020/02/21 11:43:45         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-cqsb7" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:45         Skipping container endpoint collection for pod "multus-cqsb7" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:45         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-cqsb7" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:45         Skipping container endpoint collection for pod "multus-cqsb7" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:45         Gathering data for pod "multus-dzl7n"
# [must-gather-58kh8] POD 2020/02/21 11:43:45         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-dzl7n" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:45         Skipping container endpoint collection for pod "multus-dzl7n" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:46         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-dzl7n" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:46         Skipping container endpoint collection for pod "multus-dzl7n" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:46         Gathering data for pod "multus-h5zhb"
# [must-gather-58kh8] POD 2020/02/21 11:43:46         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-h5zhb" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:46         Skipping container endpoint collection for pod "multus-h5zhb" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:47         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-h5zhb" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:47         Skipping container endpoint collection for pod "multus-h5zhb" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:47         Gathering data for pod "multus-kr7xv"
# [must-gather-58kh8] POD 2020/02/21 11:43:47         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-kr7xv" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:47         Skipping container endpoint collection for pod "multus-kr7xv" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:47         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-kr7xv" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:47         Skipping container endpoint collection for pod "multus-kr7xv" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:47         Gathering data for pod "multus-mm2lb"
# [must-gather-58kh8] POD 2020/02/21 11:43:48         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-mm2lb" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:48         Skipping container endpoint collection for pod "multus-mm2lb" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:48         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-mm2lb" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:48         Skipping container endpoint collection for pod "multus-mm2lb" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:48         Gathering data for pod "multus-nbkgp"
# [must-gather-58kh8] POD 2020/02/21 11:43:49         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-nbkgp" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:49         Skipping container endpoint collection for pod "multus-nbkgp" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:49         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-nbkgp" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:49         Skipping container endpoint collection for pod "multus-nbkgp" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:49         Gathering data for pod "multus-pwcvp"
# [must-gather-58kh8] POD 2020/02/21 11:43:49         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-pwcvp" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:49         Skipping container endpoint collection for pod "multus-pwcvp" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:50         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-pwcvp" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:50         Skipping container endpoint collection for pod "multus-pwcvp" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:50         Gathering data for pod "multus-s5sw5"
# [must-gather-58kh8] POD 2020/02/21 11:43:50         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-s5sw5" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:50         Skipping container endpoint collection for pod "multus-s5sw5" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:51         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-s5sw5" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:51         Skipping container endpoint collection for pod "multus-s5sw5" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:51         Gathering data for pod "multus-sfdmt"
# [must-gather-58kh8] POD 2020/02/21 11:43:51         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-sfdmt" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:51         Skipping container endpoint collection for pod "multus-sfdmt" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:51         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-sfdmt" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:51         Skipping container endpoint collection for pod "multus-sfdmt" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:51         Gathering data for pod "multus-t7l56"
# [must-gather-58kh8] POD 2020/02/21 11:43:52         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-t7l56" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:52         Skipping container endpoint collection for pod "multus-t7l56" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:52         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-t7l56" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:52         Skipping container endpoint collection for pod "multus-t7l56" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:52         Gathering data for pod "multus-zfcl4"
# [must-gather-58kh8] POD 2020/02/21 11:43:53         Unable to gather previous container logs: previous terminated container "kube-multus" in pod "multus-zfcl4" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:53         Skipping container endpoint collection for pod "multus-zfcl4" container "kube-multus": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:53         Unable to gather previous container logs: previous terminated container "cni-plugins" in pod "multus-zfcl4" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:53         Skipping container endpoint collection for pod "multus-zfcl4" container "cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:55 Gathering data for ns/openshift-sdn...
# [must-gather-58kh8] POD 2020/02/21 11:43:55     Collecting resources for namespace "openshift-sdn"...
# [must-gather-58kh8] POD 2020/02/21 11:43:55     Gathering pod data for namespace "openshift-sdn"...
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Gathering data for pod "ovs-2zbhf"
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-2zbhf" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Skipping container endpoint collection for pod "ovs-2zbhf" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Gathering data for pod "ovs-5g7tz"
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-5g7tz" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Skipping container endpoint collection for pod "ovs-5g7tz" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Gathering data for pod "ovs-8bfdp"
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-8bfdp" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Skipping container endpoint collection for pod "ovs-8bfdp" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Gathering data for pod "ovs-8k6gp"
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-8k6gp" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Skipping container endpoint collection for pod "ovs-8k6gp" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Gathering data for pod "ovs-95smf"
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-95smf" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Skipping container endpoint collection for pod "ovs-95smf" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:55         Gathering data for pod "ovs-9s55d"
# [must-gather-58kh8] POD 2020/02/21 11:43:56         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-9s55d" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:56         Skipping container endpoint collection for pod "ovs-9s55d" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:56         Gathering data for pod "ovs-b6h8n"
# [must-gather-58kh8] POD 2020/02/21 11:43:56         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-b6h8n" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:56         Skipping container endpoint collection for pod "ovs-b6h8n" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:56         Gathering data for pod "ovs-c2drb"
# [must-gather-58kh8] POD 2020/02/21 11:43:56         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-c2drb" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:56         Skipping container endpoint collection for pod "ovs-c2drb" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:56         Gathering data for pod "ovs-c5khk"
# [must-gather-58kh8] POD 2020/02/21 11:43:57         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-c5khk" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:57         Skipping container endpoint collection for pod "ovs-c5khk" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:57         Gathering data for pod "ovs-kzvdb"
# [must-gather-58kh8] POD 2020/02/21 11:43:57         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-kzvdb" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:57         Skipping container endpoint collection for pod "ovs-kzvdb" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:57         Gathering data for pod "ovs-m5x6s"
# [must-gather-58kh8] POD 2020/02/21 11:43:58         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-m5x6s" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:58         Skipping container endpoint collection for pod "ovs-m5x6s" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:58         Gathering data for pod "ovs-mmjdt"
# [must-gather-58kh8] POD 2020/02/21 11:43:58         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-mmjdt" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:58         Skipping container endpoint collection for pod "ovs-mmjdt" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:58         Gathering data for pod "ovs-pjmpg"
# [must-gather-58kh8] POD 2020/02/21 11:43:58         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-pjmpg" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:58         Skipping container endpoint collection for pod "ovs-pjmpg" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:58         Gathering data for pod "ovs-vxcxx"
# [must-gather-58kh8] POD 2020/02/21 11:43:59         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-vxcxx" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:59         Skipping container endpoint collection for pod "ovs-vxcxx" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:59         Gathering data for pod "ovs-z5cx4"
# [must-gather-58kh8] POD 2020/02/21 11:43:59         Unable to gather previous container logs: previous terminated container "openvswitch" in pod "ovs-z5cx4" not found
# [must-gather-58kh8] POD 2020/02/21 11:43:59         Skipping container endpoint collection for pod "ovs-z5cx4" container "openvswitch": No ports
# [must-gather-58kh8] POD 2020/02/21 11:43:59         Gathering data for pod "sdn-5gp57"
# [must-gather-58kh8] POD 2020/02/21 11:44:00         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-5gp57" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:00         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-5gp57" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:00         Skipping container endpoint collection for pod "sdn-5gp57" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:00         Gathering data for pod "sdn-78qkj"
# [must-gather-58kh8] POD 2020/02/21 11:44:00         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-78qkj" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:01         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-78qkj" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:01         Skipping container endpoint collection for pod "sdn-78qkj" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:01         Gathering data for pod "sdn-9zftm"
# [must-gather-58kh8] POD 2020/02/21 11:44:01         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-9zftm" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:02         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-9zftm" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:02         Skipping container endpoint collection for pod "sdn-9zftm" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:02         Gathering data for pod "sdn-controller-dgtx5"
# [must-gather-58kh8] POD 2020/02/21 11:44:02         Unable to gather previous container logs: previous terminated container "sdn-controller" in pod "sdn-controller-dgtx5" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:02         Skipping container endpoint collection for pod "sdn-controller-dgtx5" container "sdn-controller": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:02         Gathering data for pod "sdn-controller-xcz2l"
# [must-gather-58kh8] POD 2020/02/21 11:44:02         Unable to gather previous container logs: previous terminated container "sdn-controller" in pod "sdn-controller-xcz2l" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:02         Skipping container endpoint collection for pod "sdn-controller-xcz2l" container "sdn-controller": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:02         Gathering data for pod "sdn-controller-ztg6b"
# [must-gather-58kh8] POD 2020/02/21 11:44:03         Unable to gather previous container logs: previous terminated container "sdn-controller" in pod "sdn-controller-ztg6b" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:03         Skipping container endpoint collection for pod "sdn-controller-ztg6b" container "sdn-controller": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:03         Gathering data for pod "sdn-gdx8p"
# [must-gather-58kh8] POD 2020/02/21 11:44:03         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-gdx8p" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:04         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-gdx8p" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:04         Skipping container endpoint collection for pod "sdn-gdx8p" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:04         Gathering data for pod "sdn-hv6h9"
# [must-gather-58kh8] POD 2020/02/21 11:44:04         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-hv6h9" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:04         Skipping container endpoint collection for pod "sdn-hv6h9" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:04         Gathering data for pod "sdn-hwq7n"
# [must-gather-58kh8] POD 2020/02/21 11:44:05         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-hwq7n" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:05         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-hwq7n" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:05         Skipping container endpoint collection for pod "sdn-hwq7n" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:05         Gathering data for pod "sdn-lhmr5"
# [must-gather-58kh8] POD 2020/02/21 11:44:06         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-lhmr5" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:06         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-lhmr5" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:06         Skipping container endpoint collection for pod "sdn-lhmr5" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:06         Gathering data for pod "sdn-mp57c"
# [must-gather-58kh8] POD 2020/02/21 11:44:06         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-mp57c" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:07         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-mp57c" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:07         Skipping container endpoint collection for pod "sdn-mp57c" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:07         Gathering data for pod "sdn-n8b8n"
# [must-gather-58kh8] POD 2020/02/21 11:44:07         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-n8b8n" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:08         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-n8b8n" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:08         Skipping container endpoint collection for pod "sdn-n8b8n" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:08         Gathering data for pod "sdn-nznzf"
# [must-gather-58kh8] POD 2020/02/21 11:44:08         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-nznzf" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:08         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-nznzf" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:08         Skipping container endpoint collection for pod "sdn-nznzf" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:08         Gathering data for pod "sdn-p59fb"
# [must-gather-58kh8] POD 2020/02/21 11:44:09         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-p59fb" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:09         Skipping container endpoint collection for pod "sdn-p59fb" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:09         Gathering data for pod "sdn-qvnt5"
# [must-gather-58kh8] POD 2020/02/21 11:44:10         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-qvnt5" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:10         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-qvnt5" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:10         Skipping container endpoint collection for pod "sdn-qvnt5" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:10         Gathering data for pod "sdn-xlxs7"
# [must-gather-58kh8] POD 2020/02/21 11:44:11         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-xlxs7" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:11         Skipping container endpoint collection for pod "sdn-xlxs7" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:11         Gathering data for pod "sdn-zlwrw"
# [must-gather-58kh8] POD 2020/02/21 11:44:11         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-zlwrw" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:12         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-zlwrw" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:12         Skipping container endpoint collection for pod "sdn-zlwrw" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:12         Gathering data for pod "sdn-zzxgm"
# [must-gather-58kh8] POD 2020/02/21 11:44:12         Unable to gather previous container logs: previous terminated container "sdn" in pod "sdn-zzxgm" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:12         Unable to gather previous container logs: previous terminated container "install-cni-plugins" in pod "sdn-zzxgm" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:12         Skipping container endpoint collection for pod "sdn-zzxgm" container "install-cni-plugins": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:17 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:17 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:17 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Gathering related object reference information for ClusterOperator "node-tuning"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Found related object "namespaces/openshift-cluster-node-tuning-operator" for ClusterOperator "node-tuning"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Found related object "Tuned.tuned.openshift.io/default" for ClusterOperator "node-tuning"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Found related object "ServiceAccount/tuned" for ClusterOperator "node-tuning"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Found related object "ClusterRole.rbac.authorization.k8s.io/cluster-node-tuning:tuned" for ClusterOperator "node-tuning"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Found related object "ClusterRoleBinding.rbac.authorization.k8s.io/cluster-node-tuning:tuned" for ClusterOperator "node-tuning"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Found related object "ConfigMap/tuned-profiles" for ClusterOperator "node-tuning"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Found related object "ConfigMap/tuned-recommend" for ClusterOperator "node-tuning"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Found related object "DaemonSet.apps/tuned" for ClusterOperator "node-tuning"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17 Gathering data for ns/openshift-cluster-node-tuning-operator...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Collecting resources for namespace "openshift-cluster-node-tuning-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17     Gathering pod data for namespace "openshift-cluster-node-tuning-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Gathering data for pod "cluster-node-tuning-operator-7bddfbd6d9-rz7kw"
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Unable to gather previous container logs: previous terminated container "cluster-node-tuning-operator" in pod "cluster-node-tuning-operator-7bddfbd6d9-rz7kw" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Skipping container endpoint collection for pod "cluster-node-tuning-operator-7bddfbd6d9-rz7kw" container "cluster-node-tuning-operator": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Gathering data for pod "tuned-6lc8x"
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-6lc8x" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Skipping container endpoint collection for pod "tuned-6lc8x" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Gathering data for pod "tuned-9nmq5"
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-9nmq5" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Skipping container endpoint collection for pod "tuned-9nmq5" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Gathering data for pod "tuned-9npxx"
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-9npxx" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Skipping container endpoint collection for pod "tuned-9npxx" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Gathering data for pod "tuned-9pj28"
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-9pj28" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Skipping container endpoint collection for pod "tuned-9pj28" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:17         Gathering data for pod "tuned-b87dm"
# [must-gather-58kh8] POD 2020/02/21 11:44:18         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-b87dm" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:18         Skipping container endpoint collection for pod "tuned-b87dm" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:18         Gathering data for pod "tuned-f9vrl"
# [must-gather-58kh8] POD 2020/02/21 11:44:18         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-f9vrl" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:18         Skipping container endpoint collection for pod "tuned-f9vrl" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:18         Gathering data for pod "tuned-frzft"
# [must-gather-58kh8] POD 2020/02/21 11:44:18         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-frzft" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:18         Skipping container endpoint collection for pod "tuned-frzft" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:18         Gathering data for pod "tuned-jrrnj"
# [must-gather-58kh8] POD 2020/02/21 11:44:19         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-jrrnj" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:19         Skipping container endpoint collection for pod "tuned-jrrnj" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:19         Gathering data for pod "tuned-jxb5g"
# [must-gather-58kh8] POD 2020/02/21 11:44:19         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-jxb5g" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:19         Skipping container endpoint collection for pod "tuned-jxb5g" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:19         Gathering data for pod "tuned-nrwt7"
# [must-gather-58kh8] POD 2020/02/21 11:44:20         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-nrwt7" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:20         Skipping container endpoint collection for pod "tuned-nrwt7" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:20         Gathering data for pod "tuned-q7xrr"
# [must-gather-58kh8] POD 2020/02/21 11:44:20         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-q7xrr" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:20         Skipping container endpoint collection for pod "tuned-q7xrr" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:20         Gathering data for pod "tuned-spbmd"
# [must-gather-58kh8] POD 2020/02/21 11:44:20         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-spbmd" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:20         Skipping container endpoint collection for pod "tuned-spbmd" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:20         Gathering data for pod "tuned-w6zcj"
# [must-gather-58kh8] POD 2020/02/21 11:44:21         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-w6zcj" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:21         Skipping container endpoint collection for pod "tuned-w6zcj" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:21         Gathering data for pod "tuned-wm675"
# [must-gather-58kh8] POD 2020/02/21 11:44:21         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-wm675" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:21         Skipping container endpoint collection for pod "tuned-wm675" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:21         Gathering data for pod "tuned-xbvxv"
# [must-gather-58kh8] POD 2020/02/21 11:44:22         Unable to gather previous container logs: previous terminated container "tuned" in pod "tuned-xbvxv" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:22         Skipping container endpoint collection for pod "tuned-xbvxv" container "tuned": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:23 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:23 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:23 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Gathering related object reference information for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "openshiftapiservers.operator.openshift.io/cluster" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "namespaces/openshift-config" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "namespaces/openshift-config-managed" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "namespaces/openshift-apiserver-operator" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "namespaces/openshift-apiserver" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.apps.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.authorization.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.build.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.image.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.oauth.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.project.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.quota.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.route.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.security.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.template.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Found related object "apiservices.apiregistration.k8s.io/v1.user.openshift.io" for ClusterOperator "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23 Gathering data for ns/openshift-apiserver-operator...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Collecting resources for namespace "openshift-apiserver-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23     Gathering pod data for namespace "openshift-apiserver-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:23         Gathering data for pod "openshift-apiserver-operator-57687d5f6-t8ttl"
# [must-gather-58kh8] POD 2020/02/21 11:44:24 Gathering data for ns/openshift-apiserver...
# [must-gather-58kh8] POD 2020/02/21 11:44:24     Collecting resources for namespace "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:24     Gathering pod data for namespace "openshift-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:24         Gathering data for pod "apiserver-2b2r8"
# [must-gather-58kh8] POD 2020/02/21 11:44:25         Unable to gather previous container logs: previous terminated container "openshift-apiserver" in pod "apiserver-2b2r8" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:25         Unable to gather previous container logs: previous terminated container "fix-audit-permissions" in pod "apiserver-2b2r8" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:25         Skipping container endpoint collection for pod "apiserver-2b2r8" container "fix-audit-permissions": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:25         Gathering data for pod "apiserver-dkhr8"
# [must-gather-58kh8] POD 2020/02/21 11:44:28         Unable to gather previous container logs: previous terminated container "openshift-apiserver" in pod "apiserver-dkhr8" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:28         Unable to gather previous container logs: previous terminated container "fix-audit-permissions" in pod "apiserver-dkhr8" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:28         Skipping container endpoint collection for pod "apiserver-dkhr8" container "fix-audit-permissions": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:28         Gathering data for pod "apiserver-jpbcg"
# [must-gather-58kh8] POD 2020/02/21 11:44:29         Unable to gather previous container logs: previous terminated container "openshift-apiserver" in pod "apiserver-jpbcg" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:29         Unable to gather previous container logs: previous terminated container "fix-audit-permissions" in pod "apiserver-jpbcg" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:29         Skipping container endpoint collection for pod "apiserver-jpbcg" container "fix-audit-permissions": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:33 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:33 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:33 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:33     Gathering related object reference information for ClusterOperator "openshift-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:33     Found related object "openshiftcontrollermanagers.operator.openshift.io/cluster" for ClusterOperator "openshift-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:33     Found related object "namespaces/openshift-config" for ClusterOperator "openshift-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:33     Found related object "namespaces/openshift-config-managed" for ClusterOperator "openshift-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:33     Found related object "namespaces/openshift-controller-manager-operator" for ClusterOperator "openshift-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:33     Found related object "namespaces/openshift-controller-manager" for ClusterOperator "openshift-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:33 Gathering data for ns/openshift-controller-manager-operator...
# [must-gather-58kh8] POD 2020/02/21 11:44:33     Collecting resources for namespace "openshift-controller-manager-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:33     Gathering pod data for namespace "openshift-controller-manager-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:33         Gathering data for pod "openshift-controller-manager-operator-589c8b6689-bdmqq"
# [must-gather-58kh8] POD 2020/02/21 11:44:34 Gathering data for ns/openshift-controller-manager...
# [must-gather-58kh8] POD 2020/02/21 11:44:34     Collecting resources for namespace "openshift-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:34     Gathering pod data for namespace "openshift-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:34         Gathering data for pod "controller-manager-8h69j"
# [must-gather-58kh8] POD 2020/02/21 11:44:34         Unable to gather previous container logs: previous terminated container "controller-manager" in pod "controller-manager-8h69j" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:34         Gathering data for pod "controller-manager-g2254"
# [must-gather-58kh8] POD 2020/02/21 11:44:34         Unable to gather previous container logs: previous terminated container "controller-manager" in pod "controller-manager-g2254" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:34         Gathering data for pod "controller-manager-n8dlm"
# [must-gather-58kh8] POD 2020/02/21 11:44:34         Unable to gather previous container logs: previous terminated container "controller-manager" in pod "controller-manager-n8dlm" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:35 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:35 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:35 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:35     Gathering related object reference information for ClusterOperator "openshift-samples"...
# [must-gather-58kh8] POD 2020/02/21 11:44:35     Found related object "configs.samples.operator.openshift.io/cluster" for ClusterOperator "openshift-samples"...
# [must-gather-58kh8] POD 2020/02/21 11:44:35     Found related object "namespaces/openshift-cluster-samples-operator" for ClusterOperator "openshift-samples"...
# [must-gather-58kh8] POD 2020/02/21 11:44:35     Found related object "namespaces/openshift" for ClusterOperator "openshift-samples"...
# [must-gather-58kh8] POD 2020/02/21 11:44:35 Gathering data for ns/openshift-cluster-samples-operator...
# [must-gather-58kh8] POD 2020/02/21 11:44:35     Collecting resources for namespace "openshift-cluster-samples-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:35     Gathering pod data for namespace "openshift-cluster-samples-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:35         Gathering data for pod "cluster-samples-operator-5d4954bff-4m9cg"
# [must-gather-58kh8] POD 2020/02/21 11:44:35         Unable to gather previous container logs: previous terminated container "cluster-samples-operator" in pod "cluster-samples-operator-5d4954bff-4m9cg" not found
# [must-gather-58kh8] POD E0221 11:44:35.870261     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod a05a011599215345f53a8bf035a9cb1891936d5a379c850608dfc9b731d24046, uid : exit status 1: 2020/02/21 11:44:35 socat[387498] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:44:35.910395     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod a05a011599215345f53a8bf035a9cb1891936d5a379c850608dfc9b731d24046, uid : exit status 1: 2020/02/21 11:44:35 socat[387509] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:44:35.949111     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod a05a011599215345f53a8bf035a9cb1891936d5a379c850608dfc9b731d24046, uid : exit status 1: 2020/02/21 11:44:35 socat[387517] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD 2020/02/21 11:44:36 Gathering data for ns/openshift...
# [must-gather-58kh8] POD 2020/02/21 11:44:36     Collecting resources for namespace "openshift"...
# [must-gather-58kh8] POD 2020/02/21 11:44:36     Gathering pod data for namespace "openshift"...
# [must-gather-58kh8] POD 2020/02/21 11:44:36 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:36 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:36 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:36     Gathering related object reference information for ClusterOperator "operator-lifecycle-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:36     Found related object "ClusterServiceVersion.operators.coreos.com/packageserver" for ClusterOperator "operator-lifecycle-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:36     Found related object "namespaces/openshift-operator-lifecycle-manager" for ClusterOperator "operator-lifecycle-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:37 Gathering data for ns/openshift-operator-lifecycle-manager...
# [must-gather-58kh8] POD 2020/02/21 11:44:37     Collecting resources for namespace "openshift-operator-lifecycle-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:37     Gathering pod data for namespace "openshift-operator-lifecycle-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:37         Gathering data for pod "catalog-operator-5555bbb485-w279j"
# [must-gather-58kh8] POD 2020/02/21 11:44:37         Unable to gather previous container logs: previous terminated container "catalog-operator" in pod "catalog-operator-5555bbb485-w279j" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:37         Gathering data for pod "olm-operator-84c545cfdc-mrmfd"
# [must-gather-58kh8] POD 2020/02/21 11:44:38         Unable to gather previous container logs: previous terminated container "olm-operator" in pod "olm-operator-84c545cfdc-mrmfd" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:38         Gathering data for pod "packageserver-7448d988bc-bw6n2"
# [must-gather-58kh8] POD 2020/02/21 11:44:39         Unable to gather previous container logs: previous terminated container "packageserver" in pod "packageserver-7448d988bc-bw6n2" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:39         Gathering data for pod "packageserver-7448d988bc-ljqdf"
# [must-gather-58kh8] POD 2020/02/21 11:44:40         Unable to gather previous container logs: previous terminated container "packageserver" in pod "packageserver-7448d988bc-ljqdf" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:40 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:40 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:40 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:40     Gathering related object reference information for ClusterOperator "operator-lifecycle-manager-catalog"...
# [must-gather-58kh8] POD 2020/02/21 11:44:40     Found related object "namespaces/openshift-operator-lifecycle-manager" for ClusterOperator "operator-lifecycle-manager-catalog"...
# [must-gather-58kh8] POD 2020/02/21 11:44:40 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:40 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:40 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:40     Gathering related object reference information for ClusterOperator "operator-lifecycle-manager-packageserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:40     Found related object "namespaces/openshift-operator-lifecycle-manager" for ClusterOperator "operator-lifecycle-manager-packageserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:40     Found related object "ClusterServiceVersion.operators.coreos.com/packageserver" for ClusterOperator "operator-lifecycle-manager-packageserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41 Skipping previously-inspected resource: "openshift-operator-lifecycle-manager/operators.coreos.com/clusterserviceversions/packageserver" ...
# [must-gather-58kh8] POD 2020/02/21 11:44:41 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:41 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:41 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Gathering related object reference information for ClusterOperator "service-ca"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Found related object "servicecas.operator.openshift.io/cluster" for ClusterOperator "service-ca"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Found related object "namespaces/openshift-config" for ClusterOperator "service-ca"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Found related object "namespaces/openshift-config-managed" for ClusterOperator "service-ca"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Found related object "namespaces/openshift-service-ca-operator" for ClusterOperator "service-ca"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Found related object "namespaces/openshift-service-ca" for ClusterOperator "service-ca"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41 Gathering data for ns/openshift-service-ca-operator...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Collecting resources for namespace "openshift-service-ca-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Gathering pod data for namespace "openshift-service-ca-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41         Gathering data for pod "service-ca-operator-5bdc97dfcd-s42ds"
# [must-gather-58kh8] POD 2020/02/21 11:44:41         Unable to gather previous container logs: previous terminated container "operator" in pod "service-ca-operator-5bdc97dfcd-s42ds" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:41         Skipping container endpoint collection for pod "service-ca-operator-5bdc97dfcd-s42ds" container "operator": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:41 Gathering data for ns/openshift-service-ca...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Collecting resources for namespace "openshift-service-ca"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41     Gathering pod data for namespace "openshift-service-ca"...
# [must-gather-58kh8] POD 2020/02/21 11:44:41         Gathering data for pod "apiservice-cabundle-injector-545f9c779-d2jj2"
# [must-gather-58kh8] POD 2020/02/21 11:44:43         Unable to gather previous container logs: previous terminated container "apiservice-cabundle-injector-controller" in pod "apiservice-cabundle-injector-545f9c779-d2jj2" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:43         Skipping /version info gathering for pod "apiservice-cabundle-injector-545f9c779-d2jj2". Endpoint not found...
# [must-gather-58kh8] POD 2020/02/21 11:44:43         Gathering data for pod "configmap-cabundle-injector-b7f4cd66c-4g9sk"
# [must-gather-58kh8] POD 2020/02/21 11:44:43         Unable to gather previous container logs: previous terminated container "configmap-cabundle-injector-controller" in pod "configmap-cabundle-injector-b7f4cd66c-4g9sk" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:43         Skipping /version info gathering for pod "configmap-cabundle-injector-b7f4cd66c-4g9sk". Endpoint not found...
# [must-gather-58kh8] POD 2020/02/21 11:44:43         Gathering data for pod "service-serving-cert-signer-6c8f5dcdf7-8xsjz"
# [must-gather-58kh8] POD 2020/02/21 11:44:43         Unable to gather previous container logs: previous terminated container "service-serving-cert-signer-controller" in pod "service-serving-cert-signer-6c8f5dcdf7-8xsjz" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:44         Skipping /version info gathering for pod "service-serving-cert-signer-6c8f5dcdf7-8xsjz". Endpoint not found...
# [must-gather-58kh8] POD 2020/02/21 11:44:44 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:44 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:44 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:44     Gathering related object reference information for ClusterOperator "service-catalog-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:44     Found related object "namespaces/openshift-config" for ClusterOperator "service-catalog-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:44     Found related object "namespaces/openshift-config-managed" for ClusterOperator "service-catalog-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:44     Found related object "namespaces/openshift-service-catalog-apiserver-operator" for ClusterOperator "service-catalog-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:44     Found related object "namespaces/openshift-service-catalog-apiserver" for ClusterOperator "service-catalog-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:44     Found related object "apiservices.apiregistration.k8s.io/v1beta1.servicecatalog.k8s.io" for ClusterOperator "service-catalog-apiserver"...
# [must-gather-58kh8] POD 2020/02/21 11:44:44 Gathering data for ns/openshift-service-catalog-apiserver-operator...
# [must-gather-58kh8] POD 2020/02/21 11:44:44     Collecting resources for namespace "openshift-service-catalog-apiserver-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:44     Gathering pod data for namespace "openshift-service-catalog-apiserver-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:44         Gathering data for pod "openshift-service-catalog-apiserver-operator-5879db865f-r2gdq"
# [must-gather-58kh8] POD 2020/02/21 11:44:45         Unable to gather previous container logs: previous terminated container "operator" in pod "openshift-service-catalog-apiserver-operator-5879db865f-r2gdq" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:46 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:46 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:46 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:46     Gathering related object reference information for ClusterOperator "service-catalog-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:46     Found related object "servicecatalogcontrollermanagers.operator.openshift.io/cluster" for ClusterOperator "service-catalog-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:46     Found related object "namespaces/openshift-service-catalog-controller-manager-operator" for ClusterOperator "service-catalog-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:46     Found related object "namespaces/openshift-service-catalog-controller-manager" for ClusterOperator "service-catalog-controller-manager"...
# [must-gather-58kh8] POD 2020/02/21 11:44:46 Gathering data for ns/openshift-service-catalog-controller-manager-operator...
# [must-gather-58kh8] POD 2020/02/21 11:44:46     Collecting resources for namespace "openshift-service-catalog-controller-manager-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:46     Gathering pod data for namespace "openshift-service-catalog-controller-manager-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:46         Gathering data for pod "openshift-service-catalog-controller-manager-operator-5478wjbwg"
# [must-gather-58kh8] POD 2020/02/21 11:44:46         Unable to gather previous container logs: previous terminated container "operator" in pod "openshift-service-catalog-controller-manager-operator-5478wjbwg" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:47 Skipping previously-collected config.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:47 Skipping previously-collected operator.openshift.io resource data
# [must-gather-58kh8] POD 2020/02/21 11:44:47 Gathering cluster operator resource data...
# [must-gather-58kh8] POD 2020/02/21 11:44:47     Gathering related object reference information for ClusterOperator "storage"...
# [must-gather-58kh8] POD 2020/02/21 11:44:47     Found related object "namespaces/openshift-cluster-storage-operator" for ClusterOperator "storage"...
# [must-gather-58kh8] POD 2020/02/21 11:44:47     Found related object "infrastructures.config.openshift.io/cluster" for ClusterOperator "storage"...
# [must-gather-58kh8] POD 2020/02/21 11:44:47     Found related object "storageclasses.storage.k8s.io/gp2" for ClusterOperator "storage"...
# [must-gather-58kh8] POD 2020/02/21 11:44:47 Gathering data for ns/openshift-cluster-storage-operator...
# [must-gather-58kh8] POD 2020/02/21 11:44:47     Collecting resources for namespace "openshift-cluster-storage-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:47     Gathering pod data for namespace "openshift-cluster-storage-operator"...
# [must-gather-58kh8] POD 2020/02/21 11:44:47         Gathering data for pod "cluster-storage-operator-67d944ddfd-5ldz7"
# [must-gather-58kh8] POD 2020/02/21 11:44:47         Unable to gather previous container logs: previous terminated container "cluster-storage-operator" in pod "cluster-storage-operator-67d944ddfd-5ldz7" not found
# [must-gather-58kh8] POD E0221 11:44:47.519241     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod 249d65ae1b5ee08a1ca4914d296c8c023015f440f11595bf93f4b5f6785310e9, uid : exit status 1: 2020/02/21 11:44:47 socat[722699] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:44:47.564165     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod 249d65ae1b5ee08a1ca4914d296c8c023015f440f11595bf93f4b5f6785310e9, uid : exit status 1: 2020/02/21 11:44:47 socat[722707] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD E0221 11:44:47.604926     109 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod 249d65ae1b5ee08a1ca4914d296c8c023015f440f11595bf93f4b5f6785310e9, uid : exit status 1: 2020/02/21 11:44:47 socat[722714] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-58kh8] POD Error: [one or more errors ocurred while gathering pod-specific data for namespace: openshift-authentication
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod oauth-openshift-c55ccdc57-pkm8s:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod oauth-openshift-c55ccdc57-vgjxf:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-cloud-credential-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod cloud-credential-operator-fb7864cd9-4b89j:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], [the server doesn't have a resource type "machineautoscalers", the server doesn't have a resource type "clusterautoscalers", one or more errors ocurred while gathering pod-specific data for namespace: openshift-machine-api
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod cluster-autoscaler-operator-68bc4d6bd-jcrwv:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: the server could not find the requested resource, unable to gather container /version: the server could not find the requested resource, unable to gather container /metrics: the server could not find the requested resource], one or more errors ocurred while gathering container data for pod machine-api-operator-68cbcfbf9-fzwsf:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client]]], [one or more errors ocurred while gathering pod-specific data for namespace: openshift-console-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod console-operator-dd677d4d7-wdxl7:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering pod-specific data for namespace: openshift-console
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod console-5bb876548-5mprz:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod console-5bb876548-ls5bk:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod downloads-9fcbc4dc7-k6wf5:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: tls: oversized record received with length 24932, unable to gather container /version: Get https://localhost:37587/: tls: oversized record received with length 24932, unable to gather container /metrics: Get https://localhost:37587/metrics: tls: oversized record received with length 24932], one or more errors ocurred while gathering container data for pod downloads-9fcbc4dc7-zf4vs:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: tls: oversized record received with length 24932, unable to gather container /version: Get https://localhost:37587/: tls: oversized record received with length 24932, unable to gather container /metrics: Get https://localhost:37587/metrics: tls: oversized record received with length 24932]]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-dns
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod dns-default-878bg:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-9cwn9:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-b8tnk:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-d84d7:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-dscth:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-dwbff:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-lvbnq:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-mq5t8:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-mzq2f:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-p294t:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-s4tjk:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-sv8m4:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-tqk52:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-x4vgp:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod dns-default-zc8p6:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-image-registry
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod cluster-image-registry-operator-8587f7bfcb-6cptn:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod image-registry-6698d579c9-2cqfd:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: unexpected end of JSON input, unable to gather container /version: unexpected end of JSON input, unable to gather container /metrics: the server could not find the requested resource]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-ingress
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod router-default-5d5f9f8649-7mghq:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod router-default-5d5f9f8649-p9p6f:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-insights
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod insights-operator-7cd676dd46-ff2pt:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], [one or more errors ocurred while gathering pod-specific data for namespace: openshift-kube-apiserver-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod kube-apiserver-operator-84b79d6d8b-bgx57:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering pod-specific data for namespace: openshift-kube-apiserver
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client]], one or more errors ocurred while gathering container data for pod kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client]], one or more errors ocurred while gathering container data for pod kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client]]]], [one or more errors ocurred while gathering pod-specific data for namespace: openshift-kube-controller-manager
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-kube-controller-manager-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod kube-controller-manager-operator-6b86f4675b-pjddn:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-kube-scheduler
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-marketplace
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod certified-operators-fd9b95b9f-htd9p:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: tls: first record does not look like a TLS handshake, unable to gather container /version: Get https://localhost:37587/: tls: first record does not look like a TLS handshake, unable to gather container /metrics: Get https://localhost:37587/metrics: tls: first record does not look like a TLS handshake], one or more errors ocurred while gathering container data for pod community-operators-7db6b74f69-76rmm:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: tls: first record does not look like a TLS handshake, unable to gather container /version: Get https://localhost:37587/: tls: first record does not look like a TLS handshake, unable to gather container /metrics: Get https://localhost:37587/metrics: tls: first record does not look like a TLS handshake], one or more errors ocurred while gathering container data for pod marketplace-operator-78775467c7-ldb6m:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod redhat-operators-5bddbf4558-9qh4x:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: tls: first record does not look like a TLS handshake, unable to gather container /version: Get https://localhost:37587/: tls: first record does not look like a TLS handshake, unable to gather container /metrics: Get https://localhost:37587/metrics: tls: first record does not look like a TLS handshake]], [the server doesn't have a resource type "monitoring", one or more errors ocurred while gathering pod-specific data for namespace: openshift-monitoring
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod alertmanager-main-0:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], [unable to gather container /healthz: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW, unable to gather container /version: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW]], one or more errors ocurred while gathering container data for pod alertmanager-main-1:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], [unable to gather container /healthz: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW, unable to gather container /version: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW]], one or more errors ocurred while gathering container data for pod alertmanager-main-2:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], [unable to gather container /healthz: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW, unable to gather container /version: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW]], one or more errors ocurred while gathering container data for pod cluster-monitoring-operator-585c8c44d9-hzmnw:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod grafana-69685f986d-rl757:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], [unable to gather container /healthz: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW, unable to gather container /version: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW]], one or more errors ocurred while gathering container data for pod kube-state-metrics-7c884764fd-jdf57:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: the server has asked for the client to provide credentials, unable to gather container /version: the server has asked for the client to provide credentials, unable to gather container /metrics: the server has asked for the client to provide credentials], [unable to gather container /healthz: the server has asked for the client to provide credentials, unable to gather container /version: the server has asked for the client to provide credentials, unable to gather container /metrics: the server has asked for the client to provide credentials]], one or more errors ocurred while gathering container data for pod node-exporter-86k7k:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-d49v5:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-fk267:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-gqgjk:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-hgtjd:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-lkxvx:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-nbkxw:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-p7vrw:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-pl95f:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-prbc5:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-r9ckd:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-vm4np:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-wn9hs:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-xddfk:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod node-exporter-xhwb8:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod openshift-state-metrics-7c76b98c77-kxdjp:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: the server has asked for the client to provide credentials, unable to gather container /version: the server has asked for the client to provide credentials, unable to gather container /metrics: the server has asked for the client to provide credentials], [unable to gather container /healthz: the server has asked for the client to provide credentials, unable to gather container /version: the server has asked for the client to provide credentials, unable to gather container /metrics: the server has asked for the client to provide credentials]], one or more errors ocurred while gathering container data for pod prometheus-adapter-986777885-d7rjf:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod prometheus-adapter-986777885-vdcmj:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod prometheus-k8s-0:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW, unable to gather container /version: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW], [unable to gather container /healthz: the server has asked for the client to provide credentials, unable to gather container /version: the server has asked for the client to provide credentials, unable to gather container /metrics: the server has asked for the client to provide credentials]], one or more errors ocurred while gathering container data for pod prometheus-k8s-1:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW, unable to gather container /version: <!DOCTYPE html>
# [must-gather-58kh8] POD <html lang="en" charset="utf-8">
# [must-gather-58kh8] POD <head>
# [must-gather-58kh8] POD   <title>Log In</title>
# [must-gather-58kh8] POD   <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
# [must-gather-58kh8] POD   <style>
# [must-gather-58kh8] POD     @font-face {
# [must-gather-58kh8] POD       font-family: "Open Sans";
# [must-gather-58kh8] POD       src: url(data:application/x-font-woff;charset=utf-8;base64,d09GRgABAAAAAFeoABMAAAAAlkQAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAABGRlRNAAABqAAAABwAAAAcavCZq0dERUYAAAHEAAAAHQAAAB4AJwD1R1BPUwAAAeQAAASjAAAJni1yF0JHU1VCAAAGiAAAAIEAAACooGKInk9TLzIAAAcMAAAAYAAAAGCh3ZrDY21hcAAAB2wAAAGcAAACAv1rbL5jdnQgAAAJCAAAADIAAAA8K3MG4GZwZ20AAAk8AAAE+gAACZGLC3pBZ2FzcAAADjgAAAAIAAAACAAAABBnbHlmAAAOQAAAQG0AAHBIDuDVH2hlYWQAAE6wAAAANAAAADYHgk2EaGhlYQAATuQAAAAgAAAAJA37BfVobXR4AABPBAAAAjgAAAO8MaBM1GxvY2EAAFE8AAAB1QAAAeB9N5qybWF4cAAAUxQAAAAgAAAAIAMhAjxuYW1lAABTNAAAAeQAAARWRvKTBXBvc3QAAFUYAAAB+AAAAvgEbWOAcHJlcAAAVxAAAACQAAAAkPNEIux3ZWJmAABXoAAAAAYAAAAGxDVUvgAAAAEAAAAA0MoNVwAAAADJQhegAAAAANDkdLN42mNgZGBg4AFiMSBmYmAEwndAzALmMQAADdgBHQAAAHjarZZNTJRHGMf/uyzuFm2RtmnTj2hjKKE0tikxAbboiQCljdUF7Npiaz9MDxoTSWPSkHhAV9NDE9NYasYPGtRFUfZgEAl+tUEuHnodAoVTjxNOpgdjuv3NwKJ2K22T5skv8zLvM8/Hf+YdVhFJZerQZ4o1Nb/XoRc//7p7j6q+7N61W7V7Pv1qrzYpho/yeXnff/Mc2b2re68S/ikQUzSMCUUS3cFzp+7oTuRopC9yF+5F09EsTEXnotmS1dF0yQEYif0Sux+7H82Wzq/4LXI0/ly8Op6CL3jaD/7v6vhP8VQimUjG9yeSxLv3wIiWhQVLP2zEDVY6X3IgxClY9aOW2AlJT3SqdJ5K74aq+wJvqTK/T3V6TQ2QhEY9q6Z8Ts35jFqgFdryE9oCWyHF3+2MHYydjNsgDb3EOQiHIAOH4Qj0E28A3zPEPAvnIAuDcB4u8G4ILsIlGIYRuAKjcBXGYByukec63ICbcJu5SeJHtF5jel5VeaMaqIUNUEf++rxVA35JaIRvmD8G30Mf/ADHwcAJfE/CKTgN/fhPMD/JGCFajhylxCyDKt7XwPpIGfks+WzI14BXEhZyWXJZcllyWXJZcllyFWLbEHuadbPwjMpZWQGVIdoE0RzRnN7m70bGjdDL80E4BBk4DEdCREc0pxnWz8GqpRoL9S1Xj6/F69jDunJqqoB1nAdfyeMyzuAzBy+hSheqdBVlrIN6ampgTIYeJpat4gS+J+EUnIZ+/BdUmkClLlTq0pMq/+N3VUAle+OVWVDFUKOhRkONhhoNNRrN4DcHzaGr1UHfQmf7iutlvokczbxrgVZogy1E2gopntsZOxg7GbcRK824nbUfwkfQBTvI87gvYrn+B3h/hvxn4RxkYRDOwwXeDcFFuATDMAJXYBSuwhiMwzVqug434CbcWtzh27yz1DYFhd1biTIWVSyKeB0dVTuqdlTtqNpRtT9VFm92EG+Dt1nUMIeGDg0dGjo0dOhn0c+in0U/i34O/Rz6OfSz6OfQz6KfQz+Hfj5rjqw5subImiNrjqw5tHJo5dDKoZVDK4dWDq0cWlm0smhl0cqilUUri1YWrSxaW], [unable to gather container /healthz: the server has asked for the client to provide credentials, unable to gather container /version: the server has asked for the client to provide credentials, unable to gather container /metrics: the server has asked for the client to provide credentials]], one or more errors ocurred while gathering container data for pod prometheus-operator-7c8568cc64-x5vtt:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod telemeter-client-944599596-c5jzs:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], [unable to gather container /healthz: the server has asked for the client to provide credentials, unable to gather container /version: the server has asked for the client to provide credentials, unable to gather container /metrics: the server has asked for the client to provide credentials]]]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-sdn
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod sdn-5gp57:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-78qkj:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-9zftm:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-gdx8p:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-hv6h9:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-hwq7n:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-lhmr5:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-mp57c:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-n8b8n:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-nznzf:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-p59fb:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-qvnt5:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-xlxs7:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-zlwrw:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod sdn-zzxgm:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client]], [one or more errors ocurred while gathering pod-specific data for namespace: openshift-apiserver-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod openshift-apiserver-operator-57687d5f6-t8ttl:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering pod-specific data for namespace: openshift-apiserver
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod apiserver-2b2r8:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod apiserver-dkhr8:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod apiserver-jpbcg:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"]]], [one or more errors ocurred while gathering pod-specific data for namespace: openshift-controller-manager-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod openshift-controller-manager-operator-589c8b6689-bdmqq:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering pod-specific data for namespace: openshift-controller-manager
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod controller-manager-8h69j:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod controller-manager-g2254:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod controller-manager-n8dlm:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"]]], one or more errors ocurred while gathering pod-specific data for namespace: openshift-cluster-samples-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod cluster-samples-operator-5d4954bff-4m9cg:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering pod-specific data for namespace: openshift-operator-lifecycle-manager
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod catalog-operator-5555bbb485-w279j:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod olm-operator-84c545cfdc-mrmfd:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /version: Get https://localhost:37587/: http: server gave HTTP response to HTTPS client, unable to gather container /metrics: Get https://localhost:37587/metrics: http: server gave HTTP response to HTTPS client], one or more errors ocurred while gathering container data for pod packageserver-7448d988bc-bw6n2:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], one or more errors ocurred while gathering container data for pod packageserver-7448d988bc-ljqdf:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"]], [one or more errors ocurred while gathering pod-specific data for namespace: openshift-service-catalog-apiserver-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod openshift-service-catalog-apiserver-operator-5879db865f-r2gdq:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], namespaces "openshift-service-catalog-apiserver" not found, apiservices.apiregistration.k8s.io "v1beta1.servicecatalog.k8s.io" not found], [one or more errors ocurred while gathering pod-specific data for namespace: openshift-service-catalog-controller-manager-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod openshift-service-catalog-controller-manager-operator-5478wjbwg:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /version: forbidden: User "system:anonymous" cannot get path "/", unable to gather container /metrics: forbidden: User "system:anonymous" cannot get path "/metrics"], namespaces "openshift-service-catalog-controller-manager" not found], one or more errors ocurred while gathering pod-specific data for namespace: openshift-cluster-storage-operator
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     one or more errors ocurred while gathering container data for pod cluster-storage-operator-67d944ddfd-5ldz7:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF]]
# [must-gather-58kh8] POD 2020/02/21 11:44:48 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:48 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:48 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:48 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:48 Gathering data for ns/default...
# [must-gather-58kh8] POD 2020/02/21 11:44:48     Collecting resources for namespace "default"...
# [must-gather-58kh8] POD 2020/02/21 11:44:48     Gathering pod data for namespace "default"...
# [must-gather-58kh8] POD 2020/02/21 11:44:48 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:49 Gathering data for ns/openshift...
# [must-gather-58kh8] POD 2020/02/21 11:44:49     Collecting resources for namespace "openshift"...
# [must-gather-58kh8] POD 2020/02/21 11:44:49     Gathering pod data for namespace "openshift"...
# [must-gather-58kh8] POD 2020/02/21 11:44:49 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:49 Gathering data for ns/kube-system...
# [must-gather-58kh8] POD 2020/02/21 11:44:49     Collecting resources for namespace "kube-system"...
# [must-gather-58kh8] POD 2020/02/21 11:44:49     Gathering pod data for namespace "kube-system"...
# [must-gather-58kh8] POD 2020/02/21 11:44:50 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:50 Gathering data for ns/openshift-etcd...
# [must-gather-58kh8] POD 2020/02/21 11:44:50     Collecting resources for namespace "openshift-etcd"...
# [must-gather-58kh8] POD 2020/02/21 11:44:50     Gathering pod data for namespace "openshift-etcd"...
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Gathering data for pod "etcd-member-ip-10-0-131-55.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Unable to gather previous container logs: previous terminated container "etcd-member" in pod "etcd-member-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Unable to gather previous container logs: previous terminated container "etcd-metrics" in pod "etcd-member-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Unable to gather previous container logs: previous terminated container "discovery" in pod "etcd-member-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Skipping container endpoint collection for pod "etcd-member-ip-10-0-131-55.us-east-2.compute.internal" container "discovery": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Unable to gather previous container logs: previous terminated container "certs" in pod "etcd-member-ip-10-0-131-55.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Skipping container endpoint collection for pod "etcd-member-ip-10-0-131-55.us-east-2.compute.internal" container "certs": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Gathering data for pod "etcd-member-ip-10-0-150-228.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Unable to gather previous container logs: previous terminated container "etcd-member" in pod "etcd-member-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:50         Unable to gather previous container logs: previous terminated container "etcd-metrics" in pod "etcd-member-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:51         Unable to gather previous container logs: previous terminated container "discovery" in pod "etcd-member-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:51         Skipping container endpoint collection for pod "etcd-member-ip-10-0-150-228.us-east-2.compute.internal" container "discovery": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:51         Unable to gather previous container logs: previous terminated container "certs" in pod "etcd-member-ip-10-0-150-228.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:51         Skipping container endpoint collection for pod "etcd-member-ip-10-0-150-228.us-east-2.compute.internal" container "certs": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:51         Gathering data for pod "etcd-member-ip-10-0-173-76.us-east-2.compute.internal"
# [must-gather-58kh8] POD 2020/02/21 11:44:52         Unable to gather previous container logs: previous terminated container "etcd-member" in pod "etcd-member-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:52         Unable to gather previous container logs: previous terminated container "etcd-metrics" in pod "etcd-member-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:52         Unable to gather previous container logs: previous terminated container "discovery" in pod "etcd-member-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:52         Skipping container endpoint collection for pod "etcd-member-ip-10-0-173-76.us-east-2.compute.internal" container "discovery": No ports
# [must-gather-58kh8] POD 2020/02/21 11:44:53         Unable to gather previous container logs: previous terminated container "certs" in pod "etcd-member-ip-10-0-173-76.us-east-2.compute.internal" not found
# [must-gather-58kh8] POD 2020/02/21 11:44:53         Skipping container endpoint collection for pod "etcd-member-ip-10-0-173-76.us-east-2.compute.internal" container "certs": No ports
# [must-gather-58kh8] POD Error: one or more errors ocurred while gathering pod-specific data for namespace: openshift-etcd
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [one or more errors ocurred while gathering container data for pod etcd-member-ip-10-0-131-55.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /version: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /metrics: Get https://localhost:37587/metrics: remote error: tls: bad certificate], [unable to gather container /healthz: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /version: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /metrics: Get https://localhost:37587/metrics: remote error: tls: bad certificate]], one or more errors ocurred while gathering container data for pod etcd-member-ip-10-0-150-228.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /version: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /metrics: Get https://localhost:37587/metrics: remote error: tls: bad certificate], [unable to gather container /healthz: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /version: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /metrics: Get https://localhost:37587/metrics: remote error: tls: bad certificate]], one or more errors ocurred while gathering container data for pod etcd-member-ip-10-0-173-76.us-east-2.compute.internal:
# [must-gather-58kh8] POD 
# [must-gather-58kh8] POD     [[unable to gather container /healthz: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /version: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /metrics: Get https://localhost:37587/metrics: remote error: tls: bad certificate], [unable to gather container /healthz: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /version: Get https://localhost:37587/: remote error: tls: bad certificate, unable to gather container /metrics: Get https://localhost:37587/metrics: remote error: tls: bad certificate]]]
# [must-gather-58kh8] POD 2020/02/21 11:44:53 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:53 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:53 Finished successfully with no errors.
# [must-gather-58kh8] POD 2020/02/21 11:44:54 Finished successfully with no errors.
# [must-gather-58kh8] POD WARNING: Collecting one or more audit logs on ALL masters in your cluster. This could take a large amount of time.
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=openshift-apiserver/audit-2020-02-20T12-09-19.222.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=openshift-apiserver/audit-2020-02-21T06-37-04.661.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=openshift-apiserver/audit.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=openshift-apiserver/audit-2020-02-20T12-04-32.965.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=openshift-apiserver/audit-2020-02-21T06-28-40.494.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=openshift-apiserver/audit.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-173-76.us-east-2.compute.internal --path=openshift-apiserver/audit-2020-02-20T12-11-53.281.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-173-76.us-east-2.compute.internal --path=openshift-apiserver/audit-2020-02-21T06-37-59.254.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-173-76.us-east-2.compute.internal --path=openshift-apiserver/audit.log
# [must-gather-58kh8] POD INFO: Audit logs for openshift-apiserver collected.
# [must-gather-58kh8] POD WARNING: Collecting one or more audit logs on ALL masters in your cluster. This could take a large amount of time.
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T03-00-26.493.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T03-55-27.552.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T04-50-53.817.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T05-45-40.549.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T06-40-56.484.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T07-37-18.198.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T08-31-53.239.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T09-22-00.990.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T10-12-01.206.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T11-01-18.791.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-131-55.us-east-2.compute.internal --path=kube-apiserver/audit.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-20T23-15-48.120.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T00-42-20.269.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T02-09-33.330.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T03-36-21.456.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T05-03-02.562.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T06-29-24.507.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T07-56-32.813.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T09-14-30.555.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T10-19-25.326.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T11-22-13.770.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-150-228.us-east-2.compute.internal --path=kube-apiserver/audit.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-173-76.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-20T02-53-51.419.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-173-76.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-20T14-10-18.461.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-173-76.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-20T19-27-38.584.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-173-76.us-east-2.compute.internal --path=kube-apiserver/audit-2020-02-21T07-53-59.007.log
# [must-gather-58kh8] POD /usr/bin/oc adm node-logs ip-10-0-173-76.us-east-2.compute.internal --path=kube-apiserver/audit.log
# [must-gather-58kh8] POD INFO: Audit logs for kube-apiserver collected.
# [must-gather-58kh8] POD WARNING: Collecting one or more service logs on ALL master nodes in your cluster. This could take a large amount of time.
# [must-gather-58kh8] POD INFO: Collecting host service logs for kubelet
# [must-gather-58kh8] POD INFO: Collecting host service logs for crio
# [must-gather-58kh8] POD INFO: Waiting for worker host service log collection to complete ...
# [must-gather-58kh8] POD INFO: Worker host service log collection to complete.
# [must-gather-58kh8] OUT waiting for gather to complete
# [must-gather-58kh8] OUT downloading gather output
# [must-gather-58kh8] OUT receiving incremental file list
# [must-gather-58kh8] OUT ./
# [must-gather-58kh8] OUT version
# [must-gather-58kh8] OUT audit_logs/
# [must-gather-58kh8] OUT audit_logs/kube-apiserver.audit_logs_listing
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver.audit_logs_listing
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T03-00-26.493.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T03-55-27.552.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T04-50-53.817.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T05-45-40.549.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T06-40-56.484.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T07-37-18.198.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T08-31-53.239.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T09-22-00.990.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T10-12-01.206.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T11-01-18.791.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-20T23-15-48.120.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T00-42-20.269.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T02-09-33.330.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T03-36-21.456.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T05-03-02.562.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T06-29-24.507.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T07-56-32.813.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T09-14-30.555.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T10-19-25.326.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T11-22-13.770.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-20T02-53-51.419.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-20T14-10-18.461.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-20T19-27-38.584.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-21T07-53-59.007.log.gz
# [must-gather-58kh8] OUT audit_logs/kube-apiserver/ip-10-0-173-76.us-east-2.compute.internal-audit.log.gz
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-20T12-09-19.222.log.gz
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T06-37-04.661.log.gz
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/ip-10-0-131-55.us-east-2.compute.internal-audit.log.gz
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-20T12-04-32.965.log.gz
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T06-28-40.494.log.gz
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/ip-10-0-150-228.us-east-2.compute.internal-audit.log.gz
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-20T12-11-53.281.log.gz
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-21T06-37-59.254.log.gz
# [must-gather-58kh8] OUT audit_logs/openshift-apiserver/ip-10-0-173-76.us-east-2.compute.internal-audit.log.gz
# [must-gather-58kh8] OUT cluster-scoped-resources/
# [must-gather-58kh8] OUT cluster-scoped-resources/admissionregistration.k8s.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/admissionregistration.k8s.io/validatingwebhookconfigurations/
# [must-gather-58kh8] OUT cluster-scoped-resources/admissionregistration.k8s.io/validatingwebhookconfigurations/multus.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiextensions.k8s.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/apiextensions.k8s.io/customresourcedefinitions/
# [must-gather-58kh8] OUT cluster-scoped-resources/apiextensions.k8s.io/customresourcedefinitions/clusternetworks.network.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiextensions.k8s.io/customresourcedefinitions/egressnetworkpolicies.network.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiextensions.k8s.io/customresourcedefinitions/hostsubnets.network.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiextensions.k8s.io/customresourcedefinitions/netnamespaces.network.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiextensions.k8s.io/customresourcedefinitions/network-attachment-definitions.k8s.cni.cncf.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.apps.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.authorization.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.build.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.image.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.oauth.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.project.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.quota.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.route.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.security.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.template.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/apiregistration.k8s.io/apiservices/v1.user.openshift.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/certificates.k8s.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/certificates.k8s.io/certificatesigningrequests/
# [must-gather-58kh8] OUT cluster-scoped-resources/certificates.k8s.io/certificatesigningrequests/csr-59sdr.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/certificates.k8s.io/certificatesigningrequests/csr-cdftr.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/certificates.k8s.io/certificatesigningrequests/csr-hq488.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/certificates.k8s.io/certificatesigningrequests/csr-mw8b4.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/certificates.k8s.io/certificatesigningrequests/csr-wcj58.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/certificates.k8s.io/certificatesigningrequests/csr-x52hr.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/apiservers.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/authentications.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/builds.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusterversions.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/consoles.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/dnses.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/featuregates.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/images.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/infrastructures.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/ingresses.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/networks.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/oauths.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/operatorhubs.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/projects.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/proxies.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/schedulers.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/authentications/
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/authentications/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/authentication.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/cloud-credential.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/cluster-autoscaler.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/console.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/dns.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/image-registry.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/ingress.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/insights.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/kube-apiserver.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/kube-controller-manager.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/kube-scheduler.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/machine-api.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/machine-config.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/marketplace.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/monitoring.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/network.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/node-tuning.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/openshift-apiserver.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/openshift-controller-manager.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/openshift-samples.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/operator-lifecycle-manager-catalog.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/operator-lifecycle-manager-packageserver.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/operator-lifecycle-manager.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/service-ca.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/service-catalog-apiserver.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/service-catalog-controller-manager.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusteroperators/storage.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusterversions/
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/clusterversions/version.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/consoles/
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/consoles/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/images/
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/images/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/infrastructures/
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/infrastructures/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/oauths/
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/oauths/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/proxies/
# [must-gather-58kh8] OUT cluster-scoped-resources/config.openshift.io/proxies/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-131-190.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-131-54.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-131-55.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-133-255.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-137-100.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-137-236.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-143-103.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-147-203.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-150-228.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-153-85.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-156-47.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-163-36.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-172-64.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-173-76.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/nodes/ip-10-0-174-204.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-2dce951c-5490-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-39b2a3f2-5496-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-39b9f86a-5496-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-39c1d058-5496-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-3fc8be99-5496-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-3fe4181c-5496-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-4415623d-548a-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-474f8248-548a-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-4a50b91a-548a-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-9aafc6f5-548a-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-9ab17f51-548a-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-9ab3d741-548a-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-b2bd981b-548a-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-c17d1521-53ca-11ea-afeb-029a3b7b53fa.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-c28a8afb-549c-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-c28c6cbf-549c-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-c28e94b9-549c-11ea-9d6d-06399a073c9e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/core/persistentvolumes/pvc-c2f1fecd-5493-11ea-99d2-0a84b9320ef6.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/imageregistry.operator.openshift.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/imageregistry.operator.openshift.io/configs/
# [must-gather-58kh8] OUT cluster-scoped-resources/imageregistry.operator.openshift.io/configs/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/controllerconfigs/
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/controllerconfigs/machine-config-controller.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigpools/
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigpools/master.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigpools/worker.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/00-master.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/00-worker.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/01-master-container-runtime.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/01-master-kubelet.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/01-worker-container-runtime.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/01-worker-kubelet.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/99-master-63b278d1-5341-11ea-8707-027cbd288fd4-registries.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/99-master-ssh.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/99-worker-63b4096d-5341-11ea-8707-027cbd288fd4-registries.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/99-worker-ssh.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/rendered-master-c0e628966110e0c59a7f04c9f1aae60a.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/machineconfiguration.openshift.io/machineconfigs/rendered-worker-78cf196b47033afed652ed10fabcdae4.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/network.openshift.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/network.openshift.io/clusternetworks/
# [must-gather-58kh8] OUT cluster-scoped-resources/network.openshift.io/clusternetworks/default.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/oauth.openshift.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/oauth.openshift.io/oauthclients/
# [must-gather-58kh8] OUT cluster-scoped-resources/oauth.openshift.io/oauthclients/console.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/authentications/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/authentications/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/consoles/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/consoles/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/dnses/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/dnses/default.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/kubeapiservers/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/kubeapiservers/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/kubecontrollermanagers/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/kubecontrollermanagers/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/kubeschedulers/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/kubeschedulers/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/networks/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/networks/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/openshiftapiservers/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/openshiftapiservers/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/openshiftcontrollermanagers/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/openshiftcontrollermanagers/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/servicecas/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/servicecas/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/servicecatalogcontrollermanagers/
# [must-gather-58kh8] OUT cluster-scoped-resources/operator.openshift.io/servicecatalogcontrollermanagers/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterrolebindings/
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterrolebindings/cluster-node-tuning:tuned.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterrolebindings/multus-admission-controller-webhook.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterrolebindings/multus.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterrolebindings/openshift-sdn-controller.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterrolebindings/openshift-sdn.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterrolebindings/registry-registry-role.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterroles/
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterroles/cluster-node-tuning:tuned.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterroles/multus-admission-controller-webhook.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterroles/multus.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterroles/openshift-sdn-controller.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterroles/openshift-sdn.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/rbac.authorization.k8s.io/clusterroles/system:registry.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/samples.operator.openshift.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/samples.operator.openshift.io/configs/
# [must-gather-58kh8] OUT cluster-scoped-resources/samples.operator.openshift.io/configs/cluster.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/storageclasses/
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/storageclasses/gp2.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/storageclasses/ocs-storagecluster-ceph-rbd.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/storageclasses/ocs-storagecluster-cephfs.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/storageclasses/openshift-storage.noobaa.io.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-0ce35bf5b376e25b3d5b5be8d66c7fed8cddabe2970b71ad6ca453d370f84d6b.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-1fcc6621abf6eb22abb1bf314f00b62f8345c0bc816c27edc7b4753488bba17e.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-321f54d5c9a637f902bc06ba0c271ea776dc2394ba56925f67eba33d73d7810a.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-350081ad95ed5697981fcb3f3d3104e29299d33193652cd4e998365e8f0878f0.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-36aca77987a783ea1c4e4b40b80ded94bab7dae27c37fbe6e8eec503cb2c7701.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-6c327a1fdc4697620abd2ebe9a640ac61204750b8c33fa0a019cdf83fc3de701.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-9c0e1166d5998b3a20e96f1773607b5cd69599ac9bf9fc749cc1e6cec805aff1.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-ab4c1bfe85d62ab4252a1a915148d68ad51e111144da885664a20b8770673ffa.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-d17aff30c6fc29ef462a6515e622a34285d37223acc8ee8ee32e03327da6572f.yaml
# [must-gather-58kh8] OUT cluster-scoped-resources/storage.k8s.io/volumeattachments/csi-ee07657ab35c731c1d49cff28a8a53221eafcb1c83a9b66a80efd2c015f95cee.yaml
# [must-gather-58kh8] OUT host_service_logs/
# [must-gather-58kh8] OUT host_service_logs/masters/
# [must-gather-58kh8] OUT host_service_logs/masters/crio_service.log
# [must-gather-58kh8] OUT host_service_logs/masters/kubelet_service.log
# [must-gather-58kh8] OUT namespaces/
# [must-gather-58kh8] OUT namespaces/default/
# [must-gather-58kh8] OUT namespaces/default/default.yaml
# [must-gather-58kh8] OUT namespaces/default/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/default/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/default/apps/
# [must-gather-58kh8] OUT namespaces/default/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/default/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/default/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/default/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/default/autoscaling/
# [must-gather-58kh8] OUT namespaces/default/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/default/batch/
# [must-gather-58kh8] OUT namespaces/default/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/default/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/default/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/default/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/default/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/default/core/
# [must-gather-58kh8] OUT namespaces/default/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/default/core/events.yaml
# [must-gather-58kh8] OUT namespaces/default/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/default/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/default/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/default/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/default/core/services.yaml
# [must-gather-58kh8] OUT namespaces/default/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/default/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/default/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/default/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/
# [must-gather-58kh8] OUT namespaces/kube-system/kube-system.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/kube-system/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/apps/
# [must-gather-58kh8] OUT namespaces/kube-system/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/autoscaling/
# [must-gather-58kh8] OUT namespaces/kube-system/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/batch/
# [must-gather-58kh8] OUT namespaces/kube-system/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/kube-system/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/core/
# [must-gather-58kh8] OUT namespaces/kube-system/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/core/events.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/core/services.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/kube-system/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/kube-system/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/kube-system/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/openshift-apiserver-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/pods/openshift-apiserver-operator-57687d5f6-t8ttl/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/pods/openshift-apiserver-operator-57687d5f6-t8ttl/openshift-apiserver-operator-57687d5f6-t8ttl.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/pods/openshift-apiserver-operator-57687d5f6-t8ttl/openshift-apiserver-operator/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/pods/openshift-apiserver-operator-57687d5f6-t8ttl/openshift-apiserver-operator/openshift-apiserver-operator/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/pods/openshift-apiserver-operator-57687d5f6-t8ttl/openshift-apiserver-operator/openshift-apiserver-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/pods/openshift-apiserver-operator-57687d5f6-t8ttl/openshift-apiserver-operator/openshift-apiserver-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/pods/openshift-apiserver-operator-57687d5f6-t8ttl/openshift-apiserver-operator/openshift-apiserver-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/pods/openshift-apiserver-operator-57687d5f6-t8ttl/openshift-apiserver-operator/openshift-apiserver-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/openshift-apiserver.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/apps/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/batch/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/core/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/apiserver-2b2r8.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/fix-audit-permissions/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/fix-audit-permissions/fix-audit-permissions/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/fix-audit-permissions/fix-audit-permissions/logs/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/fix-audit-permissions/fix-audit-permissions/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/fix-audit-permissions/fix-audit-permissions/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/openshift-apiserver/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/openshift-apiserver/openshift-apiserver/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/openshift-apiserver/openshift-apiserver/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/openshift-apiserver/openshift-apiserver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/openshift-apiserver/openshift-apiserver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-2b2r8/openshift-apiserver/openshift-apiserver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/apiserver-dkhr8.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/fix-audit-permissions/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/fix-audit-permissions/fix-audit-permissions/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/fix-audit-permissions/fix-audit-permissions/logs/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/fix-audit-permissions/fix-audit-permissions/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/fix-audit-permissions/fix-audit-permissions/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/openshift-apiserver/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/openshift-apiserver/openshift-apiserver/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/openshift-apiserver/openshift-apiserver/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/openshift-apiserver/openshift-apiserver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/openshift-apiserver/openshift-apiserver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-dkhr8/openshift-apiserver/openshift-apiserver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/apiserver-jpbcg.yaml
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/fix-audit-permissions/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/fix-audit-permissions/fix-audit-permissions/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/fix-audit-permissions/fix-audit-permissions/logs/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/fix-audit-permissions/fix-audit-permissions/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/fix-audit-permissions/fix-audit-permissions/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/openshift-apiserver/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/openshift-apiserver/openshift-apiserver/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/openshift-apiserver/openshift-apiserver/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/openshift-apiserver/openshift-apiserver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/openshift-apiserver/openshift-apiserver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/pods/apiserver-jpbcg/openshift-apiserver/openshift-apiserver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-apiserver/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/openshift-authentication-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/pods/authentication-operator-55bc47f6d4-6ltcq/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/pods/authentication-operator-55bc47f6d4-6ltcq/authentication-operator-55bc47f6d4-6ltcq.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/pods/authentication-operator-55bc47f6d4-6ltcq/operator/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/pods/authentication-operator-55bc47f6d4-6ltcq/operator/operator/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/pods/authentication-operator-55bc47f6d4-6ltcq/operator/operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/pods/authentication-operator-55bc47f6d4-6ltcq/operator/operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/pods/authentication-operator-55bc47f6d4-6ltcq/operator/operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-authentication-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/openshift-authentication.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/apps/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/batch/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/core/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-pkm8s/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-pkm8s/oauth-openshift-c55ccdc57-pkm8s.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-pkm8s/oauth-openshift/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-pkm8s/oauth-openshift/oauth-openshift/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-pkm8s/oauth-openshift/oauth-openshift/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-pkm8s/oauth-openshift/oauth-openshift/logs/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-pkm8s/oauth-openshift/oauth-openshift/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-pkm8s/oauth-openshift/oauth-openshift/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-vgjxf/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-vgjxf/oauth-openshift-c55ccdc57-vgjxf.yaml
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-vgjxf/oauth-openshift/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-vgjxf/oauth-openshift/oauth-openshift/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-vgjxf/oauth-openshift/oauth-openshift/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-vgjxf/oauth-openshift/oauth-openshift/logs/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-vgjxf/oauth-openshift/oauth-openshift/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-authentication/pods/oauth-openshift-c55ccdc57-vgjxf/oauth-openshift/oauth-openshift/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-authentication/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-authentication/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/openshift-cloud-credential-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/cloud-credential-operator-iam-ro.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-image-registry-azure.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-image-registry-gcs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-image-registry-openstack.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-image-registry.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-ingress-azure.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-ingress-gcp.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-ingress.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-machine-api-aws.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-machine-api-azure.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-machine-api-gcp.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/cloudcredential.openshift.io/credentialsrequests/openshift-machine-api-openstack.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/pods/cloud-credential-operator-fb7864cd9-4b89j/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/pods/cloud-credential-operator-fb7864cd9-4b89j/cloud-credential-operator-fb7864cd9-4b89j.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/pods/cloud-credential-operator-fb7864cd9-4b89j/manager/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/pods/cloud-credential-operator-fb7864cd9-4b89j/manager/manager/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/pods/cloud-credential-operator-fb7864cd9-4b89j/manager/manager/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/pods/cloud-credential-operator-fb7864cd9-4b89j/manager/manager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/pods/cloud-credential-operator-fb7864cd9-4b89j/manager/manager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/pods/cloud-credential-operator-fb7864cd9-4b89j/manager/manager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cloud-credential-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/openshift-cluster-node-tuning-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/apps/daemonsets/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/apps/daemonsets/tuned.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/configmaps/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/configmaps/tuned-profiles.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/configmaps/tuned-recommend.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/serviceaccounts/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/core/serviceaccounts/tuned.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/cluster-node-tuning-operator-7bddfbd6d9-rz7kw/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/cluster-node-tuning-operator-7bddfbd6d9-rz7kw/cluster-node-tuning-operator-7bddfbd6d9-rz7kw.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/cluster-node-tuning-operator-7bddfbd6d9-rz7kw/cluster-node-tuning-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/cluster-node-tuning-operator-7bddfbd6d9-rz7kw/cluster-node-tuning-operator/cluster-node-tuning-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/cluster-node-tuning-operator-7bddfbd6d9-rz7kw/cluster-node-tuning-operator/cluster-node-tuning-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/cluster-node-tuning-operator-7bddfbd6d9-rz7kw/cluster-node-tuning-operator/cluster-node-tuning-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/cluster-node-tuning-operator-7bddfbd6d9-rz7kw/cluster-node-tuning-operator/cluster-node-tuning-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-6lc8x/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-6lc8x/tuned-6lc8x.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-6lc8x/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-6lc8x/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-6lc8x/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-6lc8x/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-6lc8x/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9nmq5/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9nmq5/tuned-9nmq5.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9nmq5/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9nmq5/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9nmq5/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9nmq5/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9nmq5/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9npxx/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9npxx/tuned-9npxx.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9npxx/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9npxx/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9npxx/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9npxx/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9npxx/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9pj28/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9pj28/tuned-9pj28.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9pj28/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9pj28/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9pj28/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9pj28/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-9pj28/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-b87dm/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-b87dm/tuned-b87dm.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-b87dm/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-b87dm/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-b87dm/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-b87dm/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-b87dm/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-f9vrl/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-f9vrl/tuned-f9vrl.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-f9vrl/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-f9vrl/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-f9vrl/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-f9vrl/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-f9vrl/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-frzft/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-frzft/tuned-frzft.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-frzft/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-frzft/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-frzft/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-frzft/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-frzft/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jrrnj/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jrrnj/tuned-jrrnj.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jrrnj/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jrrnj/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jrrnj/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jrrnj/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jrrnj/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jxb5g/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jxb5g/tuned-jxb5g.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jxb5g/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jxb5g/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jxb5g/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jxb5g/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-jxb5g/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-nrwt7/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-nrwt7/tuned-nrwt7.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-nrwt7/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-nrwt7/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-nrwt7/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-nrwt7/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-nrwt7/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-q7xrr/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-q7xrr/tuned-q7xrr.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-q7xrr/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-q7xrr/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-q7xrr/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-q7xrr/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-q7xrr/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-spbmd/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-spbmd/tuned-spbmd.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-spbmd/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-spbmd/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-spbmd/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-spbmd/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-spbmd/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-w6zcj/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-w6zcj/tuned-w6zcj.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-w6zcj/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-w6zcj/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-w6zcj/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-w6zcj/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-w6zcj/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-wm675/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-wm675/tuned-wm675.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-wm675/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-wm675/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-wm675/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-wm675/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-wm675/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-xbvxv/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-xbvxv/tuned-xbvxv.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-xbvxv/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-xbvxv/tuned/tuned/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-xbvxv/tuned/tuned/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-xbvxv/tuned/tuned/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/pods/tuned-xbvxv/tuned/tuned/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/tuned.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/tuned.openshift.io/tuneds/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-node-tuning-operator/tuned.openshift.io/tuneds/default.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/openshift-cluster-samples-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/pods/cluster-samples-operator-5d4954bff-4m9cg/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/pods/cluster-samples-operator-5d4954bff-4m9cg/cluster-samples-operator-5d4954bff-4m9cg.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/pods/cluster-samples-operator-5d4954bff-4m9cg/cluster-samples-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/pods/cluster-samples-operator-5d4954bff-4m9cg/cluster-samples-operator/cluster-samples-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/pods/cluster-samples-operator-5d4954bff-4m9cg/cluster-samples-operator/cluster-samples-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/pods/cluster-samples-operator-5d4954bff-4m9cg/cluster-samples-operator/cluster-samples-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/pods/cluster-samples-operator-5d4954bff-4m9cg/cluster-samples-operator/cluster-samples-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/pods/cluster-samples-operator-5d4954bff-4m9cg/cluster-samples-operator/cluster-samples-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-samples-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/openshift-cluster-storage-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/pods/cluster-storage-operator-67d944ddfd-5ldz7/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/pods/cluster-storage-operator-67d944ddfd-5ldz7/cluster-storage-operator-67d944ddfd-5ldz7.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/pods/cluster-storage-operator-67d944ddfd-5ldz7/cluster-storage-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/pods/cluster-storage-operator-67d944ddfd-5ldz7/cluster-storage-operator/cluster-storage-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/pods/cluster-storage-operator-67d944ddfd-5ldz7/cluster-storage-operator/cluster-storage-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/pods/cluster-storage-operator-67d944ddfd-5ldz7/cluster-storage-operator/cluster-storage-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/pods/cluster-storage-operator-67d944ddfd-5ldz7/cluster-storage-operator/cluster-storage-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/pods/cluster-storage-operator-67d944ddfd-5ldz7/cluster-storage-operator/cluster-storage-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-storage-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/openshift-cluster-version.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/apps/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/batch/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/core/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/pods/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/pods/cluster-version-operator-6545d8586b-zgppz/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/pods/cluster-version-operator-6545d8586b-zgppz/cluster-version-operator-6545d8586b-zgppz.yaml
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/pods/cluster-version-operator-6545d8586b-zgppz/cluster-version-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/pods/cluster-version-operator-6545d8586b-zgppz/cluster-version-operator/cluster-version-operator/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/pods/cluster-version-operator-6545d8586b-zgppz/cluster-version-operator/cluster-version-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/pods/cluster-version-operator-6545d8586b-zgppz/cluster-version-operator/cluster-version-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/pods/cluster-version-operator-6545d8586b-zgppz/cluster-version-operator/cluster-version-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-cluster-version/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/openshift-config-managed.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/apps/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/batch/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/configmaps/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/core/configmaps/console-public.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-config-managed/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/
# [must-gather-58kh8] OUT namespaces/openshift-config/openshift-config.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-config/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/apps/
# [must-gather-58kh8] OUT namespaces/openshift-config/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-config/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/batch/
# [must-gather-58kh8] OUT namespaces/openshift-config/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-config/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/core/
# [must-gather-58kh8] OUT namespaces/openshift-config/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-config/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-config/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-config/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/openshift-console-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/pods/console-operator-dd677d4d7-wdxl7/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/pods/console-operator-dd677d4d7-wdxl7/console-operator-dd677d4d7-wdxl7.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/pods/console-operator-dd677d4d7-wdxl7/console-operator/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/pods/console-operator-dd677d4d7-wdxl7/console-operator/console-operator/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/pods/console-operator-dd677d4d7-wdxl7/console-operator/console-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/pods/console-operator-dd677d4d7-wdxl7/console-operator/console-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/pods/console-operator-dd677d4d7-wdxl7/console-operator/console-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/pods/console-operator-dd677d4d7-wdxl7/console-operator/console-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-console-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/
# [must-gather-58kh8] OUT namespaces/openshift-console/openshift-console.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-console/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/apps/
# [must-gather-58kh8] OUT namespaces/openshift-console/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-console/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/batch/
# [must-gather-58kh8] OUT namespaces/openshift-console/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-console/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/core/
# [must-gather-58kh8] OUT namespaces/openshift-console/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-console/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-5mprz/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-5mprz/console-5bb876548-5mprz.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-5mprz/console/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-5mprz/console/console/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-5mprz/console/console/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-5mprz/console/console/logs/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-5mprz/console/console/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-5mprz/console/console/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-ls5bk/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-ls5bk/console-5bb876548-ls5bk.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-ls5bk/console/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-ls5bk/console/console/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-ls5bk/console/console/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-ls5bk/console/console/logs/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-ls5bk/console/console/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/console-5bb876548-ls5bk/console/console/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-k6wf5/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-k6wf5/downloads-9fcbc4dc7-k6wf5.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-k6wf5/download-server/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-k6wf5/download-server/download-server/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-k6wf5/download-server/download-server/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-k6wf5/download-server/download-server/logs/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-k6wf5/download-server/download-server/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-k6wf5/download-server/download-server/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-zf4vs/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-zf4vs/downloads-9fcbc4dc7-zf4vs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-zf4vs/download-server/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-zf4vs/download-server/download-server/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-zf4vs/download-server/download-server/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-zf4vs/download-server/download-server/logs/
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-zf4vs/download-server/download-server/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-console/pods/downloads-9fcbc4dc7-zf4vs/download-server/download-server/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-console/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-console/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/openshift-controller-manager-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/pods/openshift-controller-manager-operator-589c8b6689-bdmqq/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/pods/openshift-controller-manager-operator-589c8b6689-bdmqq/openshift-controller-manager-operator-589c8b6689-bdmqq.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/pods/openshift-controller-manager-operator-589c8b6689-bdmqq/operator/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/pods/openshift-controller-manager-operator-589c8b6689-bdmqq/operator/operator/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/pods/openshift-controller-manager-operator-589c8b6689-bdmqq/operator/operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/pods/openshift-controller-manager-operator-589c8b6689-bdmqq/operator/operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/pods/openshift-controller-manager-operator-589c8b6689-bdmqq/operator/operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/pods/openshift-controller-manager-operator-589c8b6689-bdmqq/operator/operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/openshift-controller-manager.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/apps/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/batch/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/core/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-8h69j/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-8h69j/controller-manager-8h69j.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-8h69j/controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-8h69j/controller-manager/controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-8h69j/controller-manager/controller-manager/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-8h69j/controller-manager/controller-manager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-8h69j/controller-manager/controller-manager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-8h69j/controller-manager/controller-manager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-g2254/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-g2254/controller-manager-g2254.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-g2254/controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-g2254/controller-manager/controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-g2254/controller-manager/controller-manager/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-g2254/controller-manager/controller-manager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-g2254/controller-manager/controller-manager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-g2254/controller-manager/controller-manager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-n8dlm/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-n8dlm/controller-manager-n8dlm.yaml
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-n8dlm/controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-n8dlm/controller-manager/controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-n8dlm/controller-manager/controller-manager/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-n8dlm/controller-manager/controller-manager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-n8dlm/controller-manager/controller-manager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/pods/controller-manager-n8dlm/controller-manager/controller-manager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-controller-manager/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/openshift-dns-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/pods/dns-operator-9c5f9d7d9-q86ld/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/pods/dns-operator-9c5f9d7d9-q86ld/dns-operator-9c5f9d7d9-q86ld.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/pods/dns-operator-9c5f9d7d9-q86ld/dns-operator/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/pods/dns-operator-9c5f9d7d9-q86ld/dns-operator/dns-operator/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/pods/dns-operator-9c5f9d7d9-q86ld/dns-operator/dns-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/pods/dns-operator-9c5f9d7d9-q86ld/dns-operator/dns-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/pods/dns-operator-9c5f9d7d9-q86ld/dns-operator/dns-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-dns-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/openshift-dns.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-dns/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/apps/
# [must-gather-58kh8] OUT namespaces/openshift-dns/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-dns/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/batch/
# [must-gather-58kh8] OUT namespaces/openshift-dns/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-dns/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/core/
# [must-gather-58kh8] OUT namespaces/openshift-dns/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-dns/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns-default-878bg.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-878bg/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns-default-9cwn9.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-9cwn9/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns-default-b8tnk.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-b8tnk/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns-default-d84d7.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-d84d7/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns-default-dscth.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dscth/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns-default-dwbff.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-dwbff/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns-default-lvbnq.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-lvbnq/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns-default-mq5t8.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mq5t8/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns-default-mzq2f.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-mzq2f/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns-default-p294t.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-p294t/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns-default-s4tjk.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-s4tjk/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns-default-sv8m4.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-sv8m4/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns-default-tqk52.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-tqk52/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns-default-x4vgp.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-x4vgp/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns-default-zc8p6.yaml
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns-node-resolver/dns-node-resolver/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns-node-resolver/dns-node-resolver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns-node-resolver/dns-node-resolver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns-node-resolver/dns-node-resolver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns/dns/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns/dns/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns/dns/logs/
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns/dns/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/pods/dns-default-zc8p6/dns/dns/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-dns/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-dns/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/openshift-etcd.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/apps/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/batch/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/core/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-member-ip-10-0-131-55.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/certs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/certs/certs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/certs/certs/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/certs/certs/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/certs/certs/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/discovery/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/discovery/discovery/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/discovery/discovery/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/discovery/discovery/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/discovery/discovery/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-member/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-member/etcd-member/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-member/etcd-member/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-member/etcd-member/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-member/etcd-member/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-member/etcd-member/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-metrics/etcd-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-metrics/etcd-metrics/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-metrics/etcd-metrics/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-metrics/etcd-metrics/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-131-55.us-east-2.compute.internal/etcd-metrics/etcd-metrics/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-member-ip-10-0-150-228.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/certs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/certs/certs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/certs/certs/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/certs/certs/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/certs/certs/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/discovery/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/discovery/discovery/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/discovery/discovery/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/discovery/discovery/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/discovery/discovery/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-member/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-member/etcd-member/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-member/etcd-member/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-member/etcd-member/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-member/etcd-member/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-member/etcd-member/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-metrics/etcd-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-metrics/etcd-metrics/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-metrics/etcd-metrics/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-metrics/etcd-metrics/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-150-228.us-east-2.compute.internal/etcd-metrics/etcd-metrics/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-member-ip-10-0-173-76.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/certs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/certs/certs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/certs/certs/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/certs/certs/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/certs/certs/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/discovery/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/discovery/discovery/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/discovery/discovery/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/discovery/discovery/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/discovery/discovery/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-member/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-member/etcd-member/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-member/etcd-member/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-member/etcd-member/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-member/etcd-member/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-member/etcd-member/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-metrics/etcd-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-metrics/etcd-metrics/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-metrics/etcd-metrics/logs/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-metrics/etcd-metrics/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/pods/etcd-member-ip-10-0-173-76.us-east-2.compute.internal/etcd-metrics/etcd-metrics/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-etcd/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-etcd/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/openshift-image-registry.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps/daemonsets/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps/daemonsets/node-ca.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps/deployments/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/apps/deployments/image-registry.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/batch/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/configmaps/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/configmaps/image-registry-certificates.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/configmaps/serviceca.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/secrets/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/secrets/image-registry-private-configuration.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/serviceaccounts/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/serviceaccounts/registry.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/services/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/core/services/image-registry.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator-8587f7bfcb-6cptn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator-watch/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator-watch/cluster-image-registry-operator-watch/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator-watch/cluster-image-registry-operator-watch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator-watch/cluster-image-registry-operator-watch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator-watch/cluster-image-registry-operator-watch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator/cluster-image-registry-operator/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator/cluster-image-registry-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator/cluster-image-registry-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator/cluster-image-registry-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/cluster-image-registry-operator-8587f7bfcb-6cptn/cluster-image-registry-operator/cluster-image-registry-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/image-registry-6698d579c9-2cqfd/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/image-registry-6698d579c9-2cqfd/image-registry-6698d579c9-2cqfd.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/image-registry-6698d579c9-2cqfd/registry/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/image-registry-6698d579c9-2cqfd/registry/registry/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/image-registry-6698d579c9-2cqfd/registry/registry/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/image-registry-6698d579c9-2cqfd/registry/registry/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/image-registry-6698d579c9-2cqfd/registry/registry/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/image-registry-6698d579c9-2cqfd/registry/registry/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-22grs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-22grs/node-ca-22grs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-22grs/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-22grs/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-22grs/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-22grs/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-22grs/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-55tl6/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-55tl6/node-ca-55tl6.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-55tl6/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-55tl6/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-55tl6/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-55tl6/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-55tl6/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-575r2/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-575r2/node-ca-575r2.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-575r2/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-575r2/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-575r2/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-575r2/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-575r2/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9h5mc/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9h5mc/node-ca-9h5mc.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9h5mc/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9h5mc/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9h5mc/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9h5mc/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9h5mc/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9jk2n/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9jk2n/node-ca-9jk2n.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9jk2n/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9jk2n/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9jk2n/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9jk2n/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9jk2n/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9p9pl/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9p9pl/node-ca-9p9pl.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9p9pl/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9p9pl/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9p9pl/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9p9pl/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-9p9pl/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-b8sp8/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-b8sp8/node-ca-b8sp8.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-b8sp8/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-b8sp8/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-b8sp8/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-b8sp8/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-b8sp8/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-fqkj9/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-fqkj9/node-ca-fqkj9.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-fqkj9/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-fqkj9/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-fqkj9/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-fqkj9/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-fqkj9/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-gqcvf/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-gqcvf/node-ca-gqcvf.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-gqcvf/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-gqcvf/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-gqcvf/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-gqcvf/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-gqcvf/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-lf5mx/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-lf5mx/node-ca-lf5mx.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-lf5mx/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-lf5mx/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-lf5mx/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-lf5mx/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-lf5mx/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-mmmql/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-mmmql/node-ca-mmmql.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-mmmql/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-mmmql/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-mmmql/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-mmmql/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-mmmql/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-r8fb2/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-r8fb2/node-ca-r8fb2.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-r8fb2/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-r8fb2/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-r8fb2/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-r8fb2/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-r8fb2/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vbqgb/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vbqgb/node-ca-vbqgb.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vbqgb/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vbqgb/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vbqgb/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vbqgb/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vbqgb/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vv2wl/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vv2wl/node-ca-vv2wl.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vv2wl/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vv2wl/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vv2wl/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vv2wl/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vv2wl/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vxvs2/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vxvs2/node-ca-vxvs2.yaml
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vxvs2/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vxvs2/node-ca/node-ca/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vxvs2/node-ca/node-ca/logs/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vxvs2/node-ca/node-ca/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/pods/node-ca-vxvs2/node-ca/node-ca/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-image-registry/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/openshift-ingress-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/ingress.operator.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/ingress.operator.openshift.io/dnsrecords/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/ingress.operator.openshift.io/dnsrecords/default-wildcard.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/operator.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/operator.openshift.io/ingresscontrollers/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/operator.openshift.io/ingresscontrollers/default.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/pods/ingress-operator-86d56f89ff-whc9q/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/pods/ingress-operator-86d56f89ff-whc9q/ingress-operator-86d56f89ff-whc9q.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/pods/ingress-operator-86d56f89ff-whc9q/ingress-operator/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/pods/ingress-operator-86d56f89ff-whc9q/ingress-operator/ingress-operator/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/pods/ingress-operator-86d56f89ff-whc9q/ingress-operator/ingress-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/pods/ingress-operator-86d56f89ff-whc9q/ingress-operator/ingress-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/pods/ingress-operator-86d56f89ff-whc9q/ingress-operator/ingress-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/openshift-ingress.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/apps/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/batch/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/core/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-7mghq/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-7mghq/router-default-5d5f9f8649-7mghq.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-7mghq/router/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-7mghq/router/router/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-7mghq/router/router/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-7mghq/router/router/logs/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-7mghq/router/router/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-7mghq/router/router/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-p9p6f/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-p9p6f/router-default-5d5f9f8649-p9p6f.yaml
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-p9p6f/router/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-p9p6f/router/router/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-p9p6f/router/router/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-p9p6f/router/router/logs/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-p9p6f/router/router/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-ingress/pods/router-default-5d5f9f8649-p9p6f/router/router/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-ingress/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-ingress/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/
# [must-gather-58kh8] OUT namespaces/openshift-insights/openshift-insights.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-insights/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/apps/
# [must-gather-58kh8] OUT namespaces/openshift-insights/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/apps/deployments/
# [must-gather-58kh8] OUT namespaces/openshift-insights/apps/deployments/insights-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-insights/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/batch/
# [must-gather-58kh8] OUT namespaces/openshift-insights/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-insights/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/core/
# [must-gather-58kh8] OUT namespaces/openshift-insights/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-insights/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/pods/
# [must-gather-58kh8] OUT namespaces/openshift-insights/pods/insights-operator-7cd676dd46-ff2pt/
# [must-gather-58kh8] OUT namespaces/openshift-insights/pods/insights-operator-7cd676dd46-ff2pt/insights-operator-7cd676dd46-ff2pt.yaml
# [must-gather-58kh8] OUT namespaces/openshift-insights/pods/insights-operator-7cd676dd46-ff2pt/operator/
# [must-gather-58kh8] OUT namespaces/openshift-insights/pods/insights-operator-7cd676dd46-ff2pt/operator/operator/
# [must-gather-58kh8] OUT namespaces/openshift-insights/pods/insights-operator-7cd676dd46-ff2pt/operator/operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-insights/pods/insights-operator-7cd676dd46-ff2pt/operator/operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-insights/pods/insights-operator-7cd676dd46-ff2pt/operator/operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-insights/pods/insights-operator-7cd676dd46-ff2pt/operator/operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-insights/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-insights/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/openshift-kube-apiserver-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/pods/kube-apiserver-operator-84b79d6d8b-bgx57/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/pods/kube-apiserver-operator-84b79d6d8b-bgx57/kube-apiserver-operator-84b79d6d8b-bgx57.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/pods/kube-apiserver-operator-84b79d6d8b-bgx57/kube-apiserver-operator/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/pods/kube-apiserver-operator-84b79d6d8b-bgx57/kube-apiserver-operator/kube-apiserver-operator/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/pods/kube-apiserver-operator-84b79d6d8b-bgx57/kube-apiserver-operator/kube-apiserver-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/pods/kube-apiserver-operator-84b79d6d8b-bgx57/kube-apiserver-operator/kube-apiserver-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/pods/kube-apiserver-operator-84b79d6d8b-bgx57/kube-apiserver-operator/kube-apiserver-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/pods/kube-apiserver-operator-84b79d6d8b-bgx57/kube-apiserver-operator/kube-apiserver-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/openshift-kube-apiserver.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/apps/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/batch/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/core/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/setup/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/setup/setup/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/setup/setup/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/setup/setup/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal/setup/setup/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/setup/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/setup/setup/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/setup/setup/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/setup/setup/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal/setup/setup/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-9/kube-apiserver-9/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-cert-syncer-9/kube-apiserver-cert-syncer-9/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/kube-apiserver-insecure-readyz-9/kube-apiserver-insecure-readyz-9/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/setup/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/setup/setup/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/setup/setup/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/setup/setup/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/pods/kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal/setup/setup/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-apiserver/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/openshift-kube-controller-manager-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/pods/kube-controller-manager-operator-6b86f4675b-pjddn/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/pods/kube-controller-manager-operator-6b86f4675b-pjddn/kube-controller-manager-operator-6b86f4675b-pjddn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/pods/kube-controller-manager-operator-6b86f4675b-pjddn/kube-controller-manager-operator/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/pods/kube-controller-manager-operator-6b86f4675b-pjddn/kube-controller-manager-operator/kube-controller-manager-operator/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/pods/kube-controller-manager-operator-6b86f4675b-pjddn/kube-controller-manager-operator/kube-controller-manager-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/pods/kube-controller-manager-operator-6b86f4675b-pjddn/kube-controller-manager-operator/kube-controller-manager-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/pods/kube-controller-manager-operator-6b86f4675b-pjddn/kube-controller-manager-operator/kube-controller-manager-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/pods/kube-controller-manager-operator-6b86f4675b-pjddn/kube-controller-manager-operator/kube-controller-manager-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/openshift-kube-controller-manager.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/apps/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/batch/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/core/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-5/kube-controller-manager-5/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/kube-controller-manager-cert-syncer-5/kube-controller-manager-cert-syncer-5/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-controller-manager/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/openshift-kube-scheduler-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/pods/openshift-kube-scheduler-operator-64f94b4cf4-498mp/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/pods/openshift-kube-scheduler-operator-64f94b4cf4-498mp/openshift-kube-scheduler-operator-64f94b4cf4-498mp.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/pods/openshift-kube-scheduler-operator-64f94b4cf4-498mp/kube-scheduler-operator-container/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/pods/openshift-kube-scheduler-operator-64f94b4cf4-498mp/kube-scheduler-operator-container/kube-scheduler-operator-container/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/pods/openshift-kube-scheduler-operator-64f94b4cf4-498mp/kube-scheduler-operator-container/kube-scheduler-operator-container/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/pods/openshift-kube-scheduler-operator-64f94b4cf4-498mp/kube-scheduler-operator-container/kube-scheduler-operator-container/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/pods/openshift-kube-scheduler-operator-64f94b4cf4-498mp/kube-scheduler-operator-container/kube-scheduler-operator-container/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/openshift-kube-scheduler.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/apps/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/batch/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/core/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/scheduler/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/scheduler/scheduler/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/scheduler/scheduler/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/scheduler/scheduler/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/scheduler/scheduler/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/scheduler/scheduler/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/scheduler/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/scheduler/scheduler/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/scheduler/scheduler/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/scheduler/scheduler/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/scheduler/scheduler/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/scheduler/scheduler/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal.yaml
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/scheduler/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/scheduler/scheduler/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/scheduler/scheduler/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/scheduler/scheduler/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/scheduler/scheduler/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/scheduler/scheduler/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal/wait-for-host-port/wait-for-host-port/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-kube-scheduler/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/openshift-machine-api.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/apps/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/batch/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/core/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-master-0.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-master-1.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-master-2.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-worker-us-east-2a-thxfm.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-worker-us-east-2b-wr7qc.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-worker-us-east-2c-jsnnw.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-workerocs-us-east-2a-ncm2v.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-workerocs-us-east-2a-z7psv.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-workerocs-us-east-2b-j92nx.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-workerocs-us-east-2b-zf88g.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-workerocs-us-east-2c-75plw.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/cluster-munich-e7ab-lqhqg-workerocs-us-east-2c-w5ffn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/infra-us-east-2c-4fxdc.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/infra-us-east-2c-ljb6c.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machines/infra-us-east-2c-zq2jv.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machinesets/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machinesets/cluster-munich-e7ab-lqhqg-worker-us-east-2a.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machinesets/cluster-munich-e7ab-lqhqg-worker-us-east-2b.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machinesets/cluster-munich-e7ab-lqhqg-worker-us-east-2c.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machinesets/cluster-munich-e7ab-lqhqg-workerocs-us-east-2a.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machinesets/cluster-munich-e7ab-lqhqg-workerocs-us-east-2b.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machinesets/cluster-munich-e7ab-lqhqg-workerocs-us-east-2c.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/machine.openshift.io/machinesets/infra-us-east-2c.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/cluster-autoscaler-operator-68bc4d6bd-jcrwv/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/cluster-autoscaler-operator-68bc4d6bd-jcrwv/cluster-autoscaler-operator-68bc4d6bd-jcrwv.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/cluster-autoscaler-operator-68bc4d6bd-jcrwv/cluster-autoscaler-operator/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/cluster-autoscaler-operator-68bc4d6bd-jcrwv/cluster-autoscaler-operator/cluster-autoscaler-operator/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/cluster-autoscaler-operator-68bc4d6bd-jcrwv/cluster-autoscaler-operator/cluster-autoscaler-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/cluster-autoscaler-operator-68bc4d6bd-jcrwv/cluster-autoscaler-operator/cluster-autoscaler-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/cluster-autoscaler-operator-68bc4d6bd-jcrwv/cluster-autoscaler-operator/cluster-autoscaler-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/cluster-autoscaler-operator-68bc4d6bd-jcrwv/cluster-autoscaler-operator/cluster-autoscaler-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/machine-api-controllers-cf7d74945-zftzx.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/controller-manager/controller-manager/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/controller-manager/controller-manager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/controller-manager/controller-manager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/controller-manager/controller-manager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/machine-controller/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/machine-controller/machine-controller/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/machine-controller/machine-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/machine-controller/machine-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/machine-controller/machine-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/nodelink-controller/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/nodelink-controller/nodelink-controller/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/nodelink-controller/nodelink-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/nodelink-controller/nodelink-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-controllers-cf7d74945-zftzx/nodelink-controller/nodelink-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-operator-68cbcfbf9-fzwsf/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-operator-68cbcfbf9-fzwsf/machine-api-operator-68cbcfbf9-fzwsf.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-operator-68cbcfbf9-fzwsf/machine-api-operator/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-operator-68cbcfbf9-fzwsf/machine-api-operator/machine-api-operator/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-operator-68cbcfbf9-fzwsf/machine-api-operator/machine-api-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-operator-68cbcfbf9-fzwsf/machine-api-operator/machine-api-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-operator-68cbcfbf9-fzwsf/machine-api-operator/machine-api-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/pods/machine-api-operator-68cbcfbf9-fzwsf/machine-api-operator/machine-api-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-machine-api/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/openshift-machine-config-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-4vqq5/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-4vqq5/etcd-quorum-guard-777d76fbdd-4vqq5.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-4vqq5/guard/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-4vqq5/guard/guard/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-4vqq5/guard/guard/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-4vqq5/guard/guard/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-4vqq5/guard/guard/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-fnzkh/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-fnzkh/etcd-quorum-guard-777d76fbdd-fnzkh.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-fnzkh/guard/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-fnzkh/guard/guard/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-fnzkh/guard/guard/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-fnzkh/guard/guard/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-fnzkh/guard/guard/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-p9vhn/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-p9vhn/etcd-quorum-guard-777d76fbdd-p9vhn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-p9vhn/guard/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-p9vhn/guard/guard/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-p9vhn/guard/guard/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-p9vhn/guard/guard/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/etcd-quorum-guard-777d76fbdd-p9vhn/guard/guard/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-cg8c2/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-cg8c2/kubelet-bootstrap-cred-manager-cg8c2.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-cg8c2/kubelet-bootstrap-cred-manager/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-cg8c2/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-cg8c2/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-cg8c2/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-cg8c2/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mdztn/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mdztn/kubelet-bootstrap-cred-manager-mdztn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mdztn/kubelet-bootstrap-cred-manager/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mdztn/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mdztn/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mdztn/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mdztn/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mnmhl/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mnmhl/kubelet-bootstrap-cred-manager-mnmhl.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mnmhl/kubelet-bootstrap-cred-manager/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mnmhl/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mnmhl/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mnmhl/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/kubelet-bootstrap-cred-manager-mnmhl/kubelet-bootstrap-cred-manager/kubelet-bootstrap-cred-manager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-controller-84495957cf-lk4wb/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-controller-84495957cf-lk4wb/machine-config-controller-84495957cf-lk4wb.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-controller-84495957cf-lk4wb/machine-config-controller/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-controller-84495957cf-lk4wb/machine-config-controller/machine-config-controller/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-controller-84495957cf-lk4wb/machine-config-controller/machine-config-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-controller-84495957cf-lk4wb/machine-config-controller/machine-config-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-controller-84495957cf-lk4wb/machine-config-controller/machine-config-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bn76l/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bn76l/machine-config-daemon-bn76l.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bn76l/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bn76l/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bn76l/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bn76l/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bn76l/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bqtht/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bqtht/machine-config-daemon-bqtht.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bqtht/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bqtht/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bqtht/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bqtht/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-bqtht/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-fjqbt/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-fjqbt/machine-config-daemon-fjqbt.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-fjqbt/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-fjqbt/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-fjqbt/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-fjqbt/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-fjqbt/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-hdm27/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-hdm27/machine-config-daemon-hdm27.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-hdm27/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-hdm27/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-hdm27/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-hdm27/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-hdm27/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-lfwnq/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-lfwnq/machine-config-daemon-lfwnq.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-lfwnq/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-lfwnq/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-lfwnq/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-lfwnq/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-lfwnq/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ml44b/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ml44b/machine-config-daemon-ml44b.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ml44b/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ml44b/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ml44b/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ml44b/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ml44b/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-p9zd6/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-p9zd6/machine-config-daemon-p9zd6.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-p9zd6/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-p9zd6/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-p9zd6/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-p9zd6/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-p9zd6/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ps76x/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ps76x/machine-config-daemon-ps76x.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ps76x/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ps76x/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ps76x/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ps76x/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-ps76x/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-qqhvz/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-qqhvz/machine-config-daemon-qqhvz.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-qqhvz/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-qqhvz/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-qqhvz/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-qqhvz/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-qqhvz/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-s7f54/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-s7f54/machine-config-daemon-s7f54.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-s7f54/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-s7f54/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-s7f54/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-s7f54/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-s7f54/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sb8sr/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sb8sr/machine-config-daemon-sb8sr.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sb8sr/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sb8sr/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sb8sr/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sb8sr/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sb8sr/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sn8f9/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sn8f9/machine-config-daemon-sn8f9.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sn8f9/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sn8f9/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sn8f9/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sn8f9/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-sn8f9/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-v6s2m/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-v6s2m/machine-config-daemon-v6s2m.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-v6s2m/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-v6s2m/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-v6s2m/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-v6s2m/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-v6s2m/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-x4nrm/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-x4nrm/machine-config-daemon-x4nrm.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-x4nrm/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-x4nrm/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-x4nrm/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-x4nrm/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-x4nrm/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-zrx7r/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-zrx7r/machine-config-daemon-zrx7r.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-zrx7r/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-zrx7r/machine-config-daemon/machine-config-daemon/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-zrx7r/machine-config-daemon/machine-config-daemon/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-zrx7r/machine-config-daemon/machine-config-daemon/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-daemon-zrx7r/machine-config-daemon/machine-config-daemon/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-operator-db69f9f4b-llnrn/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-operator-db69f9f4b-llnrn/machine-config-operator-db69f9f4b-llnrn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-operator-db69f9f4b-llnrn/machine-config-operator/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-operator-db69f9f4b-llnrn/machine-config-operator/machine-config-operator/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-operator-db69f9f4b-llnrn/machine-config-operator/machine-config-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-operator-db69f9f4b-llnrn/machine-config-operator/machine-config-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-operator-db69f9f4b-llnrn/machine-config-operator/machine-config-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-68v65/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-68v65/machine-config-server-68v65.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-68v65/machine-config-server/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-68v65/machine-config-server/machine-config-server/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-68v65/machine-config-server/machine-config-server/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-68v65/machine-config-server/machine-config-server/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-68v65/machine-config-server/machine-config-server/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-dw77g/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-dw77g/machine-config-server-dw77g.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-dw77g/machine-config-server/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-dw77g/machine-config-server/machine-config-server/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-dw77g/machine-config-server/machine-config-server/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-dw77g/machine-config-server/machine-config-server/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-dw77g/machine-config-server/machine-config-server/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-v8l9q/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-v8l9q/machine-config-server-v8l9q.yaml
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-v8l9q/machine-config-server/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-v8l9q/machine-config-server/machine-config-server/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-v8l9q/machine-config-server/machine-config-server/logs/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-v8l9q/machine-config-server/machine-config-server/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/pods/machine-config-server-v8l9q/machine-config-server/machine-config-server/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-machine-config-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/openshift-marketplace.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/apps/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/batch/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/core/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/operators.coreos.com/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/operators.coreos.com/catalogsources/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/operators.coreos.com/catalogsources/certified-operators.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/operators.coreos.com/catalogsources/community-operators.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/operators.coreos.com/catalogsources/redhat-operators.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/operators.coreos.com/operatorsources/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/operators.coreos.com/operatorsources/certified-operators.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/operators.coreos.com/operatorsources/community-operators.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/operators.coreos.com/operatorsources/redhat-operators.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/certified-operators-fd9b95b9f-htd9p/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/certified-operators-fd9b95b9f-htd9p/certified-operators-fd9b95b9f-htd9p.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/certified-operators-fd9b95b9f-htd9p/certified-operators/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/certified-operators-fd9b95b9f-htd9p/certified-operators/certified-operators/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/certified-operators-fd9b95b9f-htd9p/certified-operators/certified-operators/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/certified-operators-fd9b95b9f-htd9p/certified-operators/certified-operators/logs/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/certified-operators-fd9b95b9f-htd9p/certified-operators/certified-operators/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/certified-operators-fd9b95b9f-htd9p/certified-operators/certified-operators/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/community-operators-7db6b74f69-76rmm/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/community-operators-7db6b74f69-76rmm/community-operators-7db6b74f69-76rmm.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/community-operators-7db6b74f69-76rmm/community-operators/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/community-operators-7db6b74f69-76rmm/community-operators/community-operators/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/community-operators-7db6b74f69-76rmm/community-operators/community-operators/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/community-operators-7db6b74f69-76rmm/community-operators/community-operators/logs/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/community-operators-7db6b74f69-76rmm/community-operators/community-operators/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/community-operators-7db6b74f69-76rmm/community-operators/community-operators/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/marketplace-operator-78775467c7-ldb6m/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/marketplace-operator-78775467c7-ldb6m/marketplace-operator-78775467c7-ldb6m.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/marketplace-operator-78775467c7-ldb6m/marketplace-operator/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/marketplace-operator-78775467c7-ldb6m/marketplace-operator/marketplace-operator/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/marketplace-operator-78775467c7-ldb6m/marketplace-operator/marketplace-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/marketplace-operator-78775467c7-ldb6m/marketplace-operator/marketplace-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/marketplace-operator-78775467c7-ldb6m/marketplace-operator/marketplace-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/marketplace-operator-78775467c7-ldb6m/marketplace-operator/marketplace-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/redhat-operators-5bddbf4558-9qh4x/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/redhat-operators-5bddbf4558-9qh4x/redhat-operators-5bddbf4558-9qh4x.yaml
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/redhat-operators-5bddbf4558-9qh4x/redhat-operators/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/redhat-operators-5bddbf4558-9qh4x/redhat-operators/redhat-operators/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/redhat-operators-5bddbf4558-9qh4x/redhat-operators/redhat-operators/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/redhat-operators-5bddbf4558-9qh4x/redhat-operators/redhat-operators/logs/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/redhat-operators-5bddbf4558-9qh4x/redhat-operators/redhat-operators/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/pods/redhat-operators-5bddbf4558-9qh4x/redhat-operators/redhat-operators/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-marketplace/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/openshift-monitoring.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/apps/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/batch/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/core/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager-main-0.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager-proxy/alertmanager-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager-proxy/alertmanager-proxy/metrics.json
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager-proxy/alertmanager-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager-proxy/alertmanager-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager-proxy/alertmanager-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager-proxy/alertmanager-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager/alertmanager/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager/alertmanager/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager/alertmanager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager/alertmanager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/alertmanager/alertmanager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/config-reloader/config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/config-reloader/config-reloader/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/config-reloader/config-reloader/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-0/config-reloader/config-reloader/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager-main-1.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager-proxy/alertmanager-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager-proxy/alertmanager-proxy/metrics.json
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager-proxy/alertmanager-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager-proxy/alertmanager-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager-proxy/alertmanager-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager-proxy/alertmanager-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager/alertmanager/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager/alertmanager/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager/alertmanager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager/alertmanager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/alertmanager/alertmanager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/config-reloader/config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/config-reloader/config-reloader/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/config-reloader/config-reloader/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-1/config-reloader/config-reloader/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager-main-2.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager-proxy/alertmanager-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager-proxy/alertmanager-proxy/metrics.json
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager-proxy/alertmanager-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager-proxy/alertmanager-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager-proxy/alertmanager-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager-proxy/alertmanager-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager/alertmanager/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager/alertmanager/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager/alertmanager/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager/alertmanager/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/alertmanager/alertmanager/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/config-reloader/config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/config-reloader/config-reloader/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/config-reloader/config-reloader/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/alertmanager-main-2/config-reloader/config-reloader/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/cluster-monitoring-operator-585c8c44d9-hzmnw/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/cluster-monitoring-operator-585c8c44d9-hzmnw/cluster-monitoring-operator-585c8c44d9-hzmnw.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/cluster-monitoring-operator-585c8c44d9-hzmnw/cluster-monitoring-operator/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/cluster-monitoring-operator-585c8c44d9-hzmnw/cluster-monitoring-operator/cluster-monitoring-operator/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/cluster-monitoring-operator-585c8c44d9-hzmnw/cluster-monitoring-operator/cluster-monitoring-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/cluster-monitoring-operator-585c8c44d9-hzmnw/cluster-monitoring-operator/cluster-monitoring-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/cluster-monitoring-operator-585c8c44d9-hzmnw/cluster-monitoring-operator/cluster-monitoring-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/cluster-monitoring-operator-585c8c44d9-hzmnw/cluster-monitoring-operator/cluster-monitoring-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana-69685f986d-rl757.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana-proxy/grafana-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana-proxy/grafana-proxy/metrics.json
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana-proxy/grafana-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana-proxy/grafana-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana-proxy/grafana-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana-proxy/grafana-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana/grafana/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana/grafana/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana/grafana/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana/grafana/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/grafana-69685f986d-rl757/grafana/grafana/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-state-metrics-7c884764fd-jdf57.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-main/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-main/kube-rbac-proxy-main/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-main/kube-rbac-proxy-main/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-main/kube-rbac-proxy-main/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-main/kube-rbac-proxy-main/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-main/kube-rbac-proxy-main/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-self/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-self/kube-rbac-proxy-self/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-self/kube-rbac-proxy-self/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-self/kube-rbac-proxy-self/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-self/kube-rbac-proxy-self/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-rbac-proxy-self/kube-rbac-proxy-self/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-state-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-state-metrics/kube-state-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-state-metrics/kube-state-metrics/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-state-metrics/kube-state-metrics/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/kube-state-metrics-7c884764fd-jdf57/kube-state-metrics/kube-state-metrics/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/node-exporter-86k7k.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-86k7k/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/node-exporter-d49v5.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-d49v5/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/node-exporter-fk267.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-fk267/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/node-exporter-gqgjk.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-gqgjk/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/node-exporter-hgtjd.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-hgtjd/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/node-exporter-lkxvx.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-lkxvx/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/node-exporter-nbkxw.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-nbkxw/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/node-exporter-p7vrw.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-p7vrw/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/node-exporter-pl95f.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-pl95f/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/node-exporter-prbc5.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-prbc5/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/node-exporter-r9ckd.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-r9ckd/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/node-exporter-vm4np.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-vm4np/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/node-exporter-wn9hs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-wn9hs/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/node-exporter-xddfk.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xddfk/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/node-exporter-xhwb8.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/node-exporter/node-exporter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/node-exporter/node-exporter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/node-exporter/node-exporter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/node-exporter-xhwb8/node-exporter/node-exporter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/openshift-state-metrics-7c76b98c77-kxdjp.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-main/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-main/kube-rbac-proxy-main/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-main/kube-rbac-proxy-main/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-main/kube-rbac-proxy-main/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-main/kube-rbac-proxy-main/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-main/kube-rbac-proxy-main/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-self/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-self/kube-rbac-proxy-self/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-self/kube-rbac-proxy-self/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-self/kube-rbac-proxy-self/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-self/kube-rbac-proxy-self/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/kube-rbac-proxy-self/kube-rbac-proxy-self/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/openshift-state-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/openshift-state-metrics/openshift-state-metrics/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/openshift-state-metrics/openshift-state-metrics/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/openshift-state-metrics/openshift-state-metrics/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/openshift-state-metrics-7c76b98c77-kxdjp/openshift-state-metrics/openshift-state-metrics/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-d7rjf/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-d7rjf/prometheus-adapter-986777885-d7rjf.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-d7rjf/prometheus-adapter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-d7rjf/prometheus-adapter/prometheus-adapter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-d7rjf/prometheus-adapter/prometheus-adapter/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-d7rjf/prometheus-adapter/prometheus-adapter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-d7rjf/prometheus-adapter/prometheus-adapter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-d7rjf/prometheus-adapter/prometheus-adapter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-vdcmj/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-vdcmj/prometheus-adapter-986777885-vdcmj.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-vdcmj/prometheus-adapter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-vdcmj/prometheus-adapter/prometheus-adapter/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-vdcmj/prometheus-adapter/prometheus-adapter/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-vdcmj/prometheus-adapter/prometheus-adapter/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-vdcmj/prometheus-adapter/prometheus-adapter/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-adapter-986777885-vdcmj/prometheus-adapter/prometheus-adapter/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-k8s-0.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prom-label-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prom-label-proxy/prom-label-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prom-label-proxy/prom-label-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prom-label-proxy/prom-label-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prom-label-proxy/prom-label-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-config-reloader/prometheus-config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-config-reloader/prometheus-config-reloader/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-config-reloader/prometheus-config-reloader/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-config-reloader/prometheus-config-reloader/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-proxy/prometheus-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-proxy/prometheus-proxy/metrics.json
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-proxy/prometheus-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-proxy/prometheus-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-proxy/prometheus-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus-proxy/prometheus-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus/prometheus/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus/prometheus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus/prometheus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/prometheus/prometheus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/rules-configmap-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/rules-configmap-reloader/rules-configmap-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/rules-configmap-reloader/rules-configmap-reloader/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/rules-configmap-reloader/rules-configmap-reloader/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-0/rules-configmap-reloader/rules-configmap-reloader/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-k8s-1.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prom-label-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prom-label-proxy/prom-label-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prom-label-proxy/prom-label-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prom-label-proxy/prom-label-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prom-label-proxy/prom-label-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-config-reloader/prometheus-config-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-config-reloader/prometheus-config-reloader/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-config-reloader/prometheus-config-reloader/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-config-reloader/prometheus-config-reloader/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-proxy/prometheus-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-proxy/prometheus-proxy/metrics.json
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-proxy/prometheus-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-proxy/prometheus-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-proxy/prometheus-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus-proxy/prometheus-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus/prometheus/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus/prometheus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus/prometheus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/prometheus/prometheus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/rules-configmap-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/rules-configmap-reloader/rules-configmap-reloader/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/rules-configmap-reloader/rules-configmap-reloader/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/rules-configmap-reloader/rules-configmap-reloader/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-k8s-1/rules-configmap-reloader/rules-configmap-reloader/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-operator-7c8568cc64-x5vtt/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-operator-7c8568cc64-x5vtt/prometheus-operator-7c8568cc64-x5vtt.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-operator-7c8568cc64-x5vtt/prometheus-operator/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-operator-7c8568cc64-x5vtt/prometheus-operator/prometheus-operator/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-operator-7c8568cc64-x5vtt/prometheus-operator/prometheus-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-operator-7c8568cc64-x5vtt/prometheus-operator/prometheus-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-operator-7c8568cc64-x5vtt/prometheus-operator/prometheus-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/prometheus-operator-7c8568cc64-x5vtt/prometheus-operator/prometheus-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/telemeter-client-944599596-c5jzs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/kube-rbac-proxy/kube-rbac-proxy/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/kube-rbac-proxy/kube-rbac-proxy/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/kube-rbac-proxy/kube-rbac-proxy/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/kube-rbac-proxy/kube-rbac-proxy/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/kube-rbac-proxy/kube-rbac-proxy/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/reload/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/reload/reload/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/reload/reload/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/reload/reload/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/reload/reload/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/telemeter-client/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/telemeter-client/telemeter-client/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/telemeter-client/telemeter-client/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/telemeter-client/telemeter-client/logs/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/telemeter-client/telemeter-client/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/pods/telemeter-client-944599596-c5jzs/telemeter-client/telemeter-client/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-monitoring/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/openshift-multus.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps/
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps/daemonsets/
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps/daemonsets/multus-admission-controller.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/apps/daemonsets/multus.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-multus/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/batch/
# [must-gather-58kh8] OUT namespaces/openshift-multus/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-multus/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/serviceaccounts/
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/serviceaccounts/multus.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/services/
# [must-gather-58kh8] OUT namespaces/openshift-multus/core/services/multus-admission-controller.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-multus/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/multus-66xjb.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-66xjb/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/multus-8c89h.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-8c89h/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/multus-9n5xm.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-9n5xm/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-lrvhs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-lrvhs/multus-admission-controller-lrvhs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-lrvhs/multus-admission-controller/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-lrvhs/multus-admission-controller/multus-admission-controller/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-lrvhs/multus-admission-controller/multus-admission-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-lrvhs/multus-admission-controller/multus-admission-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-lrvhs/multus-admission-controller/multus-admission-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-nhpzv/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-nhpzv/multus-admission-controller-nhpzv.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-nhpzv/multus-admission-controller/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-nhpzv/multus-admission-controller/multus-admission-controller/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-nhpzv/multus-admission-controller/multus-admission-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-nhpzv/multus-admission-controller/multus-admission-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-nhpzv/multus-admission-controller/multus-admission-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-pkrr8/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-pkrr8/multus-admission-controller-pkrr8.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-pkrr8/multus-admission-controller/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-pkrr8/multus-admission-controller/multus-admission-controller/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-pkrr8/multus-admission-controller/multus-admission-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-pkrr8/multus-admission-controller/multus-admission-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-admission-controller-pkrr8/multus-admission-controller/multus-admission-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/multus-bftmf.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-bftmf/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/multus-cqsb7.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-cqsb7/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/multus-dzl7n.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-dzl7n/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/multus-h5zhb.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-h5zhb/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/multus-kr7xv.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-kr7xv/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/multus-mm2lb.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-mm2lb/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/multus-nbkgp.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-nbkgp/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/multus-pwcvp.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-pwcvp/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/multus-s5sw5.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-s5sw5/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/multus-sfdmt.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-sfdmt/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/multus-t7l56.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-t7l56/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/multus-zfcl4.yaml
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/cni-plugins/cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/cni-plugins/cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/cni-plugins/cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/cni-plugins/cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/kube-multus/kube-multus/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/kube-multus/kube-multus/logs/
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/kube-multus/kube-multus/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/pods/multus-zfcl4/kube-multus/kube-multus/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-multus/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-multus/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-network-operator/
# [must-gather-58kh8] OUT namespaces/openshift-network-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-network-operator/core/configmaps/
# [must-gather-58kh8] OUT namespaces/openshift-network-operator/core/configmaps/applied-cluster.yaml
# [must-gather-58kh8] OUT namespaces/openshift-network-operator/core/configmaps/openshift-service-ca.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/openshift-operator-lifecycle-manager.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/apps/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/batch/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/core/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/operators.coreos.com/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/operators.coreos.com/clusterserviceversions/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/operators.coreos.com/clusterserviceversions/packageserver.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/catalog-operator-5555bbb485-w279j/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/catalog-operator-5555bbb485-w279j/catalog-operator-5555bbb485-w279j.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/catalog-operator-5555bbb485-w279j/catalog-operator/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/catalog-operator-5555bbb485-w279j/catalog-operator/catalog-operator/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/catalog-operator-5555bbb485-w279j/catalog-operator/catalog-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/catalog-operator-5555bbb485-w279j/catalog-operator/catalog-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/catalog-operator-5555bbb485-w279j/catalog-operator/catalog-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/catalog-operator-5555bbb485-w279j/catalog-operator/catalog-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/olm-operator-84c545cfdc-mrmfd/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/olm-operator-84c545cfdc-mrmfd/olm-operator-84c545cfdc-mrmfd.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/olm-operator-84c545cfdc-mrmfd/olm-operator/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/olm-operator-84c545cfdc-mrmfd/olm-operator/olm-operator/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/olm-operator-84c545cfdc-mrmfd/olm-operator/olm-operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/olm-operator-84c545cfdc-mrmfd/olm-operator/olm-operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/olm-operator-84c545cfdc-mrmfd/olm-operator/olm-operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/olm-operator-84c545cfdc-mrmfd/olm-operator/olm-operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-bw6n2/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-bw6n2/packageserver-7448d988bc-bw6n2.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-bw6n2/packageserver/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-bw6n2/packageserver/packageserver/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-bw6n2/packageserver/packageserver/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-bw6n2/packageserver/packageserver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-bw6n2/packageserver/packageserver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-bw6n2/packageserver/packageserver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-ljqdf/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-ljqdf/packageserver-7448d988bc-ljqdf.yaml
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-ljqdf/packageserver/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-ljqdf/packageserver/packageserver/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-ljqdf/packageserver/packageserver/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-ljqdf/packageserver/packageserver/logs/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-ljqdf/packageserver/packageserver/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/pods/packageserver-7448d988bc-ljqdf/packageserver/packageserver/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-operator-lifecycle-manager/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/openshift-sdn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps/daemonsets/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps/daemonsets/ovs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps/daemonsets/sdn-controller.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/apps/daemonsets/sdn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/batch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/configmaps/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/configmaps/sdn-config.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/serviceaccounts/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/serviceaccounts/sdn-controller.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/serviceaccounts/sdn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/services/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/core/services/sdn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/monitoring.coreos.com/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/monitoring.coreos.com/prometheusrules/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/monitoring.coreos.com/prometheusrules/networking-rules.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/monitoring.coreos.com/servicemonitors/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/monitoring.coreos.com/servicemonitors/monitor-sdn.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-2zbhf/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-2zbhf/ovs-2zbhf.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-2zbhf/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-2zbhf/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-2zbhf/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-2zbhf/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-2zbhf/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-5g7tz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-5g7tz/ovs-5g7tz.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-5g7tz/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-5g7tz/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-5g7tz/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-5g7tz/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-5g7tz/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8bfdp/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8bfdp/ovs-8bfdp.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8bfdp/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8bfdp/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8bfdp/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8bfdp/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8bfdp/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8k6gp/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8k6gp/ovs-8k6gp.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8k6gp/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8k6gp/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8k6gp/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8k6gp/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-8k6gp/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-95smf/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-95smf/ovs-95smf.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-95smf/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-95smf/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-95smf/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-95smf/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-95smf/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-9s55d/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-9s55d/ovs-9s55d.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-9s55d/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-9s55d/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-9s55d/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-9s55d/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-9s55d/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-b6h8n/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-b6h8n/ovs-b6h8n.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-b6h8n/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-b6h8n/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-b6h8n/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-b6h8n/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-b6h8n/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c2drb/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c2drb/ovs-c2drb.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c2drb/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c2drb/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c2drb/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c2drb/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c2drb/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c5khk/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c5khk/ovs-c5khk.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c5khk/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c5khk/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c5khk/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c5khk/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-c5khk/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-kzvdb/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-kzvdb/ovs-kzvdb.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-kzvdb/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-kzvdb/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-kzvdb/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-kzvdb/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-kzvdb/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-m5x6s/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-m5x6s/ovs-m5x6s.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-m5x6s/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-m5x6s/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-m5x6s/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-m5x6s/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-m5x6s/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-mmjdt/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-mmjdt/ovs-mmjdt.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-mmjdt/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-mmjdt/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-mmjdt/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-mmjdt/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-mmjdt/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-pjmpg/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-pjmpg/ovs-pjmpg.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-pjmpg/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-pjmpg/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-pjmpg/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-pjmpg/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-pjmpg/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-vxcxx/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-vxcxx/ovs-vxcxx.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-vxcxx/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-vxcxx/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-vxcxx/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-vxcxx/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-vxcxx/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-z5cx4/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-z5cx4/ovs-z5cx4.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-z5cx4/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-z5cx4/openvswitch/openvswitch/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-z5cx4/openvswitch/openvswitch/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-z5cx4/openvswitch/openvswitch/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/ovs-z5cx4/openvswitch/openvswitch/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/sdn-5gp57.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-5gp57/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/sdn-78qkj.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-78qkj/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/sdn-9zftm.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-9zftm/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-dgtx5/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-dgtx5/sdn-controller-dgtx5.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-dgtx5/sdn-controller/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-dgtx5/sdn-controller/sdn-controller/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-dgtx5/sdn-controller/sdn-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-dgtx5/sdn-controller/sdn-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-dgtx5/sdn-controller/sdn-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-xcz2l/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-xcz2l/sdn-controller-xcz2l.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-xcz2l/sdn-controller/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-xcz2l/sdn-controller/sdn-controller/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-xcz2l/sdn-controller/sdn-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-xcz2l/sdn-controller/sdn-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-xcz2l/sdn-controller/sdn-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-ztg6b/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-ztg6b/sdn-controller-ztg6b.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-ztg6b/sdn-controller/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-ztg6b/sdn-controller/sdn-controller/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-ztg6b/sdn-controller/sdn-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-ztg6b/sdn-controller/sdn-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-controller-ztg6b/sdn-controller/sdn-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/sdn-gdx8p.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-gdx8p/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/sdn-hv6h9.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hv6h9/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/sdn-hwq7n.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-hwq7n/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/sdn-lhmr5.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-lhmr5/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/sdn-mp57c.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-mp57c/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/sdn-n8b8n.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-n8b8n/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/sdn-nznzf.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-nznzf/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/sdn-p59fb.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-p59fb/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/sdn-qvnt5.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-qvnt5/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/sdn-xlxs7.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-xlxs7/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/sdn-zlwrw.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zlwrw/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/sdn-zzxgm.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/install-cni-plugins/install-cni-plugins/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/install-cni-plugins/install-cni-plugins/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/install-cni-plugins/install-cni-plugins/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/install-cni-plugins/install-cni-plugins/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/sdn/sdn/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/sdn/sdn/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/sdn/sdn/logs/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/sdn/sdn/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/pods/sdn-zzxgm/sdn/sdn/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-sdn/rbac.authorization.k8s.io/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/rbac.authorization.k8s.io/rolebindings/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/rbac.authorization.k8s.io/rolebindings/openshift-sdn-controller-leaderelection.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/rbac.authorization.k8s.io/rolebindings/prometheus-k8s.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/rbac.authorization.k8s.io/roles/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/rbac.authorization.k8s.io/roles/openshift-sdn-controller-leaderelection.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/rbac.authorization.k8s.io/roles/prometheus-k8s.yaml
# [must-gather-58kh8] OUT namespaces/openshift-sdn/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-sdn/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/openshift-service-ca-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/pods/service-ca-operator-5bdc97dfcd-s42ds/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/pods/service-ca-operator-5bdc97dfcd-s42ds/service-ca-operator-5bdc97dfcd-s42ds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/pods/service-ca-operator-5bdc97dfcd-s42ds/operator/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/pods/service-ca-operator-5bdc97dfcd-s42ds/operator/operator/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/pods/service-ca-operator-5bdc97dfcd-s42ds/operator/operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/pods/service-ca-operator-5bdc97dfcd-s42ds/operator/operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/pods/service-ca-operator-5bdc97dfcd-s42ds/operator/operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/openshift-service-ca.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/apps/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/batch/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/core/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-545f9c779-d2jj2.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/apiservice-cabundle-injector-controller/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/apiservice-cabundle-injector-controller/metrics.json
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/apiservice-cabundle-injector-controller/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/apiservice-cabundle-injector-controller/healthz/index
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/apiservice-cabundle-injector-controller/healthz/log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/apiservice-cabundle-injector-controller/healthz/ping
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/apiservice-cabundle-injector-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/apiservice-cabundle-injector-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/apiservice-cabundle-injector-545f9c779-d2jj2/apiservice-cabundle-injector-controller/apiservice-cabundle-injector-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-b7f4cd66c-4g9sk.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/configmap-cabundle-injector-controller/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/configmap-cabundle-injector-controller/metrics.json
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/configmap-cabundle-injector-controller/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/configmap-cabundle-injector-controller/healthz/index
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/configmap-cabundle-injector-controller/healthz/log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/configmap-cabundle-injector-controller/healthz/ping
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/configmap-cabundle-injector-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/configmap-cabundle-injector-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/configmap-cabundle-injector-b7f4cd66c-4g9sk/configmap-cabundle-injector-controller/configmap-cabundle-injector-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-6c8f5dcdf7-8xsjz.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/service-serving-cert-signer-controller/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/service-serving-cert-signer-controller/metrics.json
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/service-serving-cert-signer-controller/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/service-serving-cert-signer-controller/healthz/index
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/service-serving-cert-signer-controller/healthz/log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/service-serving-cert-signer-controller/healthz/ping
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/service-serving-cert-signer-controller/logs/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/service-serving-cert-signer-controller/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/pods/service-serving-cert-signer-6c8f5dcdf7-8xsjz/service-serving-cert-signer-controller/service-serving-cert-signer-controller/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-ca/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/openshift-service-catalog-apiserver-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/pods/openshift-service-catalog-apiserver-operator-5879db865f-r2gdq/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/pods/openshift-service-catalog-apiserver-operator-5879db865f-r2gdq/openshift-service-catalog-apiserver-operator-5879db865f-r2gdq.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/pods/openshift-service-catalog-apiserver-operator-5879db865f-r2gdq/operator/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/pods/openshift-service-catalog-apiserver-operator-5879db865f-r2gdq/operator/operator/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/pods/openshift-service-catalog-apiserver-operator-5879db865f-r2gdq/operator/operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/pods/openshift-service-catalog-apiserver-operator-5879db865f-r2gdq/operator/operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/pods/openshift-service-catalog-apiserver-operator-5879db865f-r2gdq/operator/operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/pods/openshift-service-catalog-apiserver-operator-5879db865f-r2gdq/operator/operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-apiserver-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/openshift-service-catalog-controller-manager-operator.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/apps/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/batch/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/core/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/pods/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/pods/openshift-service-catalog-controller-manager-operator-5478wjbwg/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/pods/openshift-service-catalog-controller-manager-operator-5478wjbwg/openshift-service-catalog-controller-manager-operator-5478wjbwg.yaml
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/pods/openshift-service-catalog-controller-manager-operator-5478wjbwg/operator/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/pods/openshift-service-catalog-controller-manager-operator-5478wjbwg/operator/operator/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/pods/openshift-service-catalog-controller-manager-operator-5478wjbwg/operator/operator/healthz/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/pods/openshift-service-catalog-controller-manager-operator-5478wjbwg/operator/operator/logs/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/pods/openshift-service-catalog-controller-manager-operator-5478wjbwg/operator/operator/logs/current.log
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/pods/openshift-service-catalog-controller-manager-operator-5478wjbwg/operator/operator/logs/previous.log
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift-service-catalog-controller-manager-operator/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT namespaces/openshift/
# [must-gather-58kh8] OUT namespaces/openshift/openshift.yaml
# [must-gather-58kh8] OUT namespaces/openshift/apps.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift/apps/
# [must-gather-58kh8] OUT namespaces/openshift/apps/daemonsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift/apps/deployments.yaml
# [must-gather-58kh8] OUT namespaces/openshift/apps/replicasets.yaml
# [must-gather-58kh8] OUT namespaces/openshift/apps/statefulsets.yaml
# [must-gather-58kh8] OUT namespaces/openshift/autoscaling/
# [must-gather-58kh8] OUT namespaces/openshift/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-58kh8] OUT namespaces/openshift/batch/
# [must-gather-58kh8] OUT namespaces/openshift/batch/cronjobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift/batch/jobs.yaml
# [must-gather-58kh8] OUT namespaces/openshift/build.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift/build.openshift.io/buildconfigs.yaml
# [must-gather-58kh8] OUT namespaces/openshift/build.openshift.io/builds.yaml
# [must-gather-58kh8] OUT namespaces/openshift/core/
# [must-gather-58kh8] OUT namespaces/openshift/core/configmaps.yaml
# [must-gather-58kh8] OUT namespaces/openshift/core/events.yaml
# [must-gather-58kh8] OUT namespaces/openshift/core/persistentvolumeclaims.yaml
# [must-gather-58kh8] OUT namespaces/openshift/core/pods.yaml
# [must-gather-58kh8] OUT namespaces/openshift/core/replicationcontrollers.yaml
# [must-gather-58kh8] OUT namespaces/openshift/core/secrets.yaml
# [must-gather-58kh8] OUT namespaces/openshift/core/services.yaml
# [must-gather-58kh8] OUT namespaces/openshift/image.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift/image.openshift.io/imagestreams.yaml
# [must-gather-58kh8] OUT namespaces/openshift/route.openshift.io/
# [must-gather-58kh8] OUT namespaces/openshift/route.openshift.io/routes.yaml
# [must-gather-58kh8] OUT 
# [must-gather-58kh8] OUT sent 46,003 bytes  received 1,859,264,061 bytes  65,238,949.61 bytes/sec
# [must-gather-58kh8] OUT total size is 1,858,597,547  speedup is 1.00
# [must-gather      ] OUT clusterrolebinding.rbac.authorization.k8s.io/must-gather-sb72s deleted
# [must-gather      ] OUT namespace/openshift-must-gather-cbzcj deleted
#+end_example

#+begin_example
  /home/lab-user/test:
  total used in directory 51308 available 25406688
  drwxr-xr-x. 11 lab-user users     4096 Feb 21 11:45 .
  drwxr-xr-x.  3 lab-user users      132 Feb 21 11:45 must-gather.local.4385076625778664618
#+end_example

#+begin_src bash
sudo yum -y install tree
#+end_src

#+begin_example
# Loaded plugins: amazon-id, product-id, rhui-lb, search-disabled-repos, subscription-manager
# This system is not registered with an entitlement server. You can use subscription-manager to register.
# epel/x86_64/metalink                                                                                   |  18 kB  00:00:00     
# epel                                                                                                   | 5.4 kB  00:00:00     
# pinned-epel-rpms                                                                                       | 5.3 kB  00:00:00     
# rhel-7-server-ansible-2.8-rpms                                                                         | 2.9 kB  00:00:00     
# rhel-7-server-extras-rpms                                                                              | 2.9 kB  00:00:00     
# rhel-7-server-optional-rpms                                                                            | 2.9 kB  00:00:00     
# rhel-7-server-rh-common-rpms                                                                           | 2.9 kB  00:00:00     
# rhel-7-server-rpms                                                                                     | 2.9 kB  00:00:00     
# wandisco-git                                                                                           | 2.9 kB  00:00:00     
# (1/2): epel/x86_64/updateinfo                                                                          | 1.0 MB  00:00:00     
# (2/2): epel/x86_64/primary_db                                                                          | 6.7 MB  00:00:00     
# Resolving Dependencies
# --> Running transaction check
# ---> Package tree.x86_64 0:1.6.0-10.el7 will be installed
# --> Finished Dependency Resolution
# 
# Dependencies Resolved
# 
# ==============================================================================================================================
#  Package                Arch                     Version                           Repository                            Size
# ==============================================================================================================================
# Installing:
#  tree                   x86_64                   1.6.0-10.el7                      rhel-7-server-rpms                    46 k
# 
# Transaction Summary
# ==============================================================================================================================
# Install  1 Package
# 
# Total download size: 46 k
# Installed size: 87 k
# Downloading packages:
# tree-1.6.0-10.el7.x86_64.rpm                                                                           |  46 kB  00:00:00     
# Running transaction check
# Running transaction test
# Transaction test succeeded
# Running transaction
#   Installing : tree-1.6.0-10.el7.x86_64                                                                                   1/1 
#   Verifying  : tree-1.6.0-10.el7.x86_64                                                                                   1/1 
# 
# Installed:
#   tree.x86_64 0:1.6.0-10.el7                                                                                                  
# 
# Complete!
#+end_example

#+begin_src bash
tree
#+end_src

#+begin_example
# .
# ├── 404.html
# ├── cloudformation
# │   └── idm-cloudformation.yml
# ├── dns_update
# │   ├── awsdns.py
# │   └── README.md
# ├── docs
# │   ├── 404.html
# │   ├── css
# │   │   ├── highlight.css
# │   │   ├── theme.css
# │   │   └── theme_extra.css
# │   ├── fonts
# │   │   ├── fontawesome-webfont.eot
# │   │   ├── fontawesome-webfont.svg
# │   │   ├── fontawesome-webfont.ttf
# │   │   └── fontawesome-webfont.woff
# │   ├── img
# │   │   ├── favicon.ico
# │   │   ├── qwiklab-credits.png
# │   │   ├── qwiklab-end-button.png
# │   │   ├── qwiklab-lab-credentials.png
# │   │   ├── qwiklab-labguide-url.png
# │   │   ├── qwiklab-pem-key.png
# │   │   ├── qwiklab-progress-bar.png
# │   │   ├── qwiklab-start-button.png
# │   │   ├── webssh-login-fail.png
# │   │   ├── webssh-login.png
# │   │   ├── webssh-login-success.png
# │   │   └── webssh-transport-close.png
# │   ├── index.html
# │   ├── js
# │   │   ├── highlight.pack.js
# │   │   ├── jquery-2.1.1.min.js
# │   │   ├── modernizr-2.8.3.min.js
# │   │   └── theme.js
# │   ├── search
# │   │   ├── lunr.min.js
# │   │   ├── mustache.min.js
# │   │   ├── require.js
# │   │   ├── search_index.json
# │   │   ├── search.js
# │   │   ├── search-results-template.mustache
# │   │   └── text.js
# │   ├── search.html
# │   └── sitemap.xml
# ├── index.html
# ├── infra.yaml
# ├── LICENSE
# ├── must-gather.local.4385076625778664618
# │   └── quay-io-openshift-release-dev-ocp-v4-0-art-dev-sha256-88f4ef9a524da1947682cd6f6ab0428c36d63c69a2b94b2c584ae31abba96c34
# │       ├── audit_logs
# │       │   ├── kube-apiserver
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T03-00-26.493.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T03-55-27.552.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T04-50-53.817.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T05-45-40.549.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T06-40-56.484.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T07-37-18.198.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T08-31-53.239.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T09-22-00.990.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T10-12-01.206.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T11-01-18.791.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-20T23-15-48.120.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T00-42-20.269.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T02-09-33.330.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T03-36-21.456.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T05-03-02.562.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T06-29-24.507.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T07-56-32.813.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T09-14-30.555.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T10-19-25.326.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T11-22-13.770.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit.log.gz
# │       │   │   ├── ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-20T02-53-51.419.log.gz
# │       │   │   ├── ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-20T14-10-18.461.log.gz
# │       │   │   ├── ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-20T19-27-38.584.log.gz
# │       │   │   ├── ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-21T07-53-59.007.log.gz
# │       │   │   └── ip-10-0-173-76.us-east-2.compute.internal-audit.log.gz
# │       │   ├── kube-apiserver.audit_logs_listing
# │       │   ├── openshift-apiserver
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-20T12-09-19.222.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit-2020-02-21T06-37-04.661.log.gz
# │       │   │   ├── ip-10-0-131-55.us-east-2.compute.internal-audit.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-20T12-04-32.965.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit-2020-02-21T06-28-40.494.log.gz
# │       │   │   ├── ip-10-0-150-228.us-east-2.compute.internal-audit.log.gz
# │       │   │   ├── ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-20T12-11-53.281.log.gz
# │       │   │   ├── ip-10-0-173-76.us-east-2.compute.internal-audit-2020-02-21T06-37-59.254.log.gz
# │       │   │   └── ip-10-0-173-76.us-east-2.compute.internal-audit.log.gz
# │       │   └── openshift-apiserver.audit_logs_listing
# │       ├── cluster-scoped-resources
# │       │   ├── admissionregistration.k8s.io
# │       │   │   └── validatingwebhookconfigurations
# │       │   │       └── multus.openshift.io.yaml
# │       │   ├── apiextensions.k8s.io
# │       │   │   └── customresourcedefinitions
# │       │   │       ├── clusternetworks.network.openshift.io.yaml
# │       │   │       ├── egressnetworkpolicies.network.openshift.io.yaml
# │       │   │       ├── hostsubnets.network.openshift.io.yaml
# │       │   │       ├── netnamespaces.network.openshift.io.yaml
# │       │   │       └── network-attachment-definitions.k8s.cni.cncf.io.yaml
# │       │   ├── apiregistration.k8s.io
# │       │   │   └── apiservices
# │       │   │       ├── v1.apps.openshift.io.yaml
# │       │   │       ├── v1.authorization.openshift.io.yaml
# │       │   │       ├── v1.build.openshift.io.yaml
# │       │   │       ├── v1.image.openshift.io.yaml
# │       │   │       ├── v1.oauth.openshift.io.yaml
# │       │   │       ├── v1.project.openshift.io.yaml
# │       │   │       ├── v1.quota.openshift.io.yaml
# │       │   │       ├── v1.route.openshift.io.yaml
# │       │   │       ├── v1.security.openshift.io.yaml
# │       │   │       ├── v1.template.openshift.io.yaml
# │       │   │       └── v1.user.openshift.io.yaml
# │       │   ├── certificates.k8s.io
# │       │   │   └── certificatesigningrequests
# │       │   │       ├── csr-59sdr.yaml
# │       │   │       ├── csr-cdftr.yaml
# │       │   │       ├── csr-hq488.yaml
# │       │   │       ├── csr-mw8b4.yaml
# │       │   │       ├── csr-wcj58.yaml
# │       │   │       └── csr-x52hr.yaml
# │       │   ├── config.openshift.io
# │       │   │   ├── apiservers.yaml
# │       │   │   ├── authentications
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── authentications.yaml
# │       │   │   ├── builds.yaml
# │       │   │   ├── clusteroperators
# │       │   │   │   ├── authentication.yaml
# │       │   │   │   ├── cloud-credential.yaml
# │       │   │   │   ├── cluster-autoscaler.yaml
# │       │   │   │   ├── console.yaml
# │       │   │   │   ├── dns.yaml
# │       │   │   │   ├── image-registry.yaml
# │       │   │   │   ├── ingress.yaml
# │       │   │   │   ├── insights.yaml
# │       │   │   │   ├── kube-apiserver.yaml
# │       │   │   │   ├── kube-controller-manager.yaml
# │       │   │   │   ├── kube-scheduler.yaml
# │       │   │   │   ├── machine-api.yaml
# │       │   │   │   ├── machine-config.yaml
# │       │   │   │   ├── marketplace.yaml
# │       │   │   │   ├── monitoring.yaml
# │       │   │   │   ├── network.yaml
# │       │   │   │   ├── node-tuning.yaml
# │       │   │   │   ├── openshift-apiserver.yaml
# │       │   │   │   ├── openshift-controller-manager.yaml
# │       │   │   │   ├── openshift-samples.yaml
# │       │   │   │   ├── operator-lifecycle-manager-catalog.yaml
# │       │   │   │   ├── operator-lifecycle-manager-packageserver.yaml
# │       │   │   │   ├── operator-lifecycle-manager.yaml
# │       │   │   │   ├── service-catalog-apiserver.yaml
# │       │   │   │   ├── service-catalog-controller-manager.yaml
# │       │   │   │   ├── service-ca.yaml
# │       │   │   │   └── storage.yaml
# │       │   │   ├── clusteroperators.yaml
# │       │   │   ├── clusterversions
# │       │   │   │   └── version.yaml
# │       │   │   ├── clusterversions.yaml
# │       │   │   ├── consoles
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── consoles.yaml
# │       │   │   ├── dnses.yaml
# │       │   │   ├── featuregates.yaml
# │       │   │   ├── images
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── images.yaml
# │       │   │   ├── infrastructures
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── infrastructures.yaml
# │       │   │   ├── ingresses.yaml
# │       │   │   ├── networks.yaml
# │       │   │   ├── oauths
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── oauths.yaml
# │       │   │   ├── operatorhubs.yaml
# │       │   │   ├── projects.yaml
# │       │   │   ├── proxies
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── proxies.yaml
# │       │   │   └── schedulers.yaml
# │       │   ├── core
# │       │   │   ├── nodes
# │       │   │   │   ├── ip-10-0-131-190.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-131-54.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-131-55.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-133-255.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-137-100.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-137-236.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-143-103.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-147-203.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-150-228.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-153-85.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-156-47.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-163-36.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-172-64.us-east-2.compute.internal.yaml
# │       │   │   │   ├── ip-10-0-173-76.us-east-2.compute.internal.yaml
# │       │   │   │   └── ip-10-0-174-204.us-east-2.compute.internal.yaml
# │       │   │   └── persistentvolumes
# │       │   │       ├── pvc-2dce951c-5490-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-39b2a3f2-5496-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-39b9f86a-5496-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-39c1d058-5496-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-3fc8be99-5496-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-3fe4181c-5496-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-4415623d-548a-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-474f8248-548a-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-4a50b91a-548a-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-9aafc6f5-548a-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-9ab17f51-548a-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-9ab3d741-548a-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-b2bd981b-548a-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-c17d1521-53ca-11ea-afeb-029a3b7b53fa.yaml
# │       │   │       ├── pvc-c28a8afb-549c-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-c28c6cbf-549c-11ea-9d6d-06399a073c9e.yaml
# │       │   │       ├── pvc-c28e94b9-549c-11ea-9d6d-06399a073c9e.yaml
# │       │   │       └── pvc-c2f1fecd-5493-11ea-99d2-0a84b9320ef6.yaml
# │       │   ├── imageregistry.operator.openshift.io
# │       │   │   └── configs
# │       │   │       └── cluster.yaml
# │       │   ├── machineconfiguration.openshift.io
# │       │   │   ├── controllerconfigs
# │       │   │   │   └── machine-config-controller.yaml
# │       │   │   ├── machineconfigpools
# │       │   │   │   ├── master.yaml
# │       │   │   │   └── worker.yaml
# │       │   │   └── machineconfigs
# │       │   │       ├── 00-master.yaml
# │       │   │       ├── 00-worker.yaml
# │       │   │       ├── 01-master-container-runtime.yaml
# │       │   │       ├── 01-master-kubelet.yaml
# │       │   │       ├── 01-worker-container-runtime.yaml
# │       │   │       ├── 01-worker-kubelet.yaml
# │       │   │       ├── 99-master-63b278d1-5341-11ea-8707-027cbd288fd4-registries.yaml
# │       │   │       ├── 99-master-ssh.yaml
# │       │   │       ├── 99-worker-63b4096d-5341-11ea-8707-027cbd288fd4-registries.yaml
# │       │   │       ├── 99-worker-ssh.yaml
# │       │   │       ├── rendered-master-c0e628966110e0c59a7f04c9f1aae60a.yaml
# │       │   │       └── rendered-worker-78cf196b47033afed652ed10fabcdae4.yaml
# │       │   ├── network.openshift.io
# │       │   │   └── clusternetworks
# │       │   │       └── default.yaml
# │       │   ├── oauth.openshift.io
# │       │   │   └── oauthclients
# │       │   │       └── console.yaml
# │       │   ├── operator.openshift.io
# │       │   │   ├── authentications
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── consoles
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── dnses
# │       │   │   │   └── default.yaml
# │       │   │   ├── kubeapiservers
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── kubecontrollermanagers
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── kubeschedulers
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── networks
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── openshiftapiservers
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── openshiftcontrollermanagers
# │       │   │   │   └── cluster.yaml
# │       │   │   ├── servicecas
# │       │   │   │   └── cluster.yaml
# │       │   │   └── servicecatalogcontrollermanagers
# │       │   │       └── cluster.yaml
# │       │   ├── rbac.authorization.k8s.io
# │       │   │   ├── clusterrolebindings
# │       │   │   │   ├── cluster-node-tuning:tuned.yaml
# │       │   │   │   ├── multus-admission-controller-webhook.yaml
# │       │   │   │   ├── multus.yaml
# │       │   │   │   ├── openshift-sdn-controller.yaml
# │       │   │   │   ├── openshift-sdn.yaml
# │       │   │   │   └── registry-registry-role.yaml
# │       │   │   └── clusterroles
# │       │   │       ├── cluster-node-tuning:tuned.yaml
# │       │   │       ├── multus-admission-controller-webhook.yaml
# │       │   │       ├── multus.yaml
# │       │   │       ├── openshift-sdn-controller.yaml
# │       │   │       ├── openshift-sdn.yaml
# │       │   │       └── system:registry.yaml
# │       │   ├── samples.operator.openshift.io
# │       │   │   └── configs
# │       │   │       └── cluster.yaml
# │       │   └── storage.k8s.io
# │       │       ├── storageclasses
# │       │       │   ├── gp2.yaml
# │       │       │   ├── ocs-storagecluster-cephfs.yaml
# │       │       │   ├── ocs-storagecluster-ceph-rbd.yaml
# │       │       │   └── openshift-storage.noobaa.io.yaml
# │       │       └── volumeattachments
# │       │           ├── csi-0ce35bf5b376e25b3d5b5be8d66c7fed8cddabe2970b71ad6ca453d370f84d6b.yaml
# │       │           ├── csi-1fcc6621abf6eb22abb1bf314f00b62f8345c0bc816c27edc7b4753488bba17e.yaml
# │       │           ├── csi-321f54d5c9a637f902bc06ba0c271ea776dc2394ba56925f67eba33d73d7810a.yaml
# │       │           ├── csi-350081ad95ed5697981fcb3f3d3104e29299d33193652cd4e998365e8f0878f0.yaml
# │       │           ├── csi-36aca77987a783ea1c4e4b40b80ded94bab7dae27c37fbe6e8eec503cb2c7701.yaml
# │       │           ├── csi-6c327a1fdc4697620abd2ebe9a640ac61204750b8c33fa0a019cdf83fc3de701.yaml
# │       │           ├── csi-9c0e1166d5998b3a20e96f1773607b5cd69599ac9bf9fc749cc1e6cec805aff1.yaml
# │       │           ├── csi-ab4c1bfe85d62ab4252a1a915148d68ad51e111144da885664a20b8770673ffa.yaml
# │       │           ├── csi-d17aff30c6fc29ef462a6515e622a34285d37223acc8ee8ee32e03327da6572f.yaml
# │       │           └── csi-ee07657ab35c731c1d49cff28a8a53221eafcb1c83a9b66a80efd2c015f95cee.yaml
# │       ├── host_service_logs
# │       │   └── masters
# │       │       ├── crio_service.log
# │       │       └── kubelet_service.log
# │       ├── namespaces
# │       │   ├── default
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── default.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── kube-system
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── kube-system.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-apiserver
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-apiserver.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── apiserver-2b2r8
# │       │   │   │   │   ├── apiserver-2b2r8.yaml
# │       │   │   │   │   ├── fix-audit-permissions
# │       │   │   │   │   │   └── fix-audit-permissions
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── openshift-apiserver
# │       │   │   │   │       └── openshift-apiserver
# │       │   │   │   │           ├── healthz
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── apiserver-dkhr8
# │       │   │   │   │   ├── apiserver-dkhr8.yaml
# │       │   │   │   │   ├── fix-audit-permissions
# │       │   │   │   │   │   └── fix-audit-permissions
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── openshift-apiserver
# │       │   │   │   │       └── openshift-apiserver
# │       │   │   │   │           ├── healthz
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   └── apiserver-jpbcg
# │       │   │   │       ├── apiserver-jpbcg.yaml
# │       │   │   │       ├── fix-audit-permissions
# │       │   │   │       │   └── fix-audit-permissions
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── openshift-apiserver
# │       │   │   │           └── openshift-apiserver
# │       │   │   │               ├── healthz
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-apiserver-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-apiserver-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── openshift-apiserver-operator-57687d5f6-t8ttl
# │       │   │   │       ├── openshift-apiserver-operator
# │       │   │   │       │   └── openshift-apiserver-operator
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── openshift-apiserver-operator-57687d5f6-t8ttl.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-authentication
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-authentication.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── oauth-openshift-c55ccdc57-pkm8s
# │       │   │   │   │   ├── oauth-openshift
# │       │   │   │   │   │   └── oauth-openshift
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── oauth-openshift-c55ccdc57-pkm8s.yaml
# │       │   │   │   └── oauth-openshift-c55ccdc57-vgjxf
# │       │   │   │       ├── oauth-openshift
# │       │   │   │       │   └── oauth-openshift
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── oauth-openshift-c55ccdc57-vgjxf.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-authentication-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-authentication-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── authentication-operator-55bc47f6d4-6ltcq
# │       │   │   │       ├── authentication-operator-55bc47f6d4-6ltcq.yaml
# │       │   │   │       └── operator
# │       │   │   │           └── operator
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-cloud-credential-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── cloudcredential.openshift.io
# │       │   │   │   └── credentialsrequests
# │       │   │   │       ├── cloud-credential-operator-iam-ro.yaml
# │       │   │   │       ├── openshift-image-registry-azure.yaml
# │       │   │   │       ├── openshift-image-registry-gcs.yaml
# │       │   │   │       ├── openshift-image-registry-openstack.yaml
# │       │   │   │       ├── openshift-image-registry.yaml
# │       │   │   │       ├── openshift-ingress-azure.yaml
# │       │   │   │       ├── openshift-ingress-gcp.yaml
# │       │   │   │       ├── openshift-ingress.yaml
# │       │   │   │       ├── openshift-machine-api-aws.yaml
# │       │   │   │       ├── openshift-machine-api-azure.yaml
# │       │   │   │       ├── openshift-machine-api-gcp.yaml
# │       │   │   │       └── openshift-machine-api-openstack.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-cloud-credential-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── cloud-credential-operator-fb7864cd9-4b89j
# │       │   │   │       ├── cloud-credential-operator-fb7864cd9-4b89j.yaml
# │       │   │   │       └── manager
# │       │   │   │           └── manager
# │       │   │   │               ├── healthz
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-cluster-node-tuning-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets
# │       │   │   │   │   └── tuned.yaml
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps
# │       │   │   │   │   ├── tuned-profiles.yaml
# │       │   │   │   │   └── tuned-recommend.yaml
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   ├── serviceaccounts
# │       │   │   │   │   └── tuned.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-cluster-node-tuning-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── cluster-node-tuning-operator-7bddfbd6d9-rz7kw
# │       │   │   │   │   ├── cluster-node-tuning-operator
# │       │   │   │   │   │   └── cluster-node-tuning-operator
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── cluster-node-tuning-operator-7bddfbd6d9-rz7kw.yaml
# │       │   │   │   ├── tuned-6lc8x
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-6lc8x.yaml
# │       │   │   │   ├── tuned-9nmq5
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-9nmq5.yaml
# │       │   │   │   ├── tuned-9npxx
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-9npxx.yaml
# │       │   │   │   ├── tuned-9pj28
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-9pj28.yaml
# │       │   │   │   ├── tuned-b87dm
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-b87dm.yaml
# │       │   │   │   ├── tuned-f9vrl
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-f9vrl.yaml
# │       │   │   │   ├── tuned-frzft
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-frzft.yaml
# │       │   │   │   ├── tuned-jrrnj
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-jrrnj.yaml
# │       │   │   │   ├── tuned-jxb5g
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-jxb5g.yaml
# │       │   │   │   ├── tuned-nrwt7
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-nrwt7.yaml
# │       │   │   │   ├── tuned-q7xrr
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-q7xrr.yaml
# │       │   │   │   ├── tuned-spbmd
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-spbmd.yaml
# │       │   │   │   ├── tuned-w6zcj
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-w6zcj.yaml
# │       │   │   │   ├── tuned-wm675
# │       │   │   │   │   ├── tuned
# │       │   │   │   │   │   └── tuned
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── tuned-wm675.yaml
# │       │   │   │   └── tuned-xbvxv
# │       │   │   │       ├── tuned
# │       │   │   │       │   └── tuned
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── tuned-xbvxv.yaml
# │       │   │   ├── route.openshift.io
# │       │   │   │   └── routes.yaml
# │       │   │   └── tuned.openshift.io
# │       │   │       └── tuneds
# │       │   │           └── default.yaml
# │       │   ├── openshift-cluster-samples-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-cluster-samples-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── cluster-samples-operator-5d4954bff-4m9cg
# │       │   │   │       ├── cluster-samples-operator
# │       │   │   │       │   └── cluster-samples-operator
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── cluster-samples-operator-5d4954bff-4m9cg.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-cluster-storage-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-cluster-storage-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── cluster-storage-operator-67d944ddfd-5ldz7
# │       │   │   │       ├── cluster-storage-operator
# │       │   │   │       │   └── cluster-storage-operator
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── cluster-storage-operator-67d944ddfd-5ldz7.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-cluster-version
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-cluster-version.yaml
# │       │   │   ├── pods
# │       │   │   │   └── cluster-version-operator-6545d8586b-zgppz
# │       │   │   │       ├── cluster-version-operator
# │       │   │   │       │   └── cluster-version-operator
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── cluster-version-operator-6545d8586b-zgppz.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-config
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-config.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-config-managed
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps
# │       │   │   │   │   └── console-public.yaml
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-config-managed.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-console
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-console.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── console-5bb876548-5mprz
# │       │   │   │   │   ├── console
# │       │   │   │   │   │   └── console
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── console-5bb876548-5mprz.yaml
# │       │   │   │   ├── console-5bb876548-ls5bk
# │       │   │   │   │   ├── console
# │       │   │   │   │   │   └── console
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── console-5bb876548-ls5bk.yaml
# │       │   │   │   ├── downloads-9fcbc4dc7-k6wf5
# │       │   │   │   │   ├── downloads-9fcbc4dc7-k6wf5.yaml
# │       │   │   │   │   └── download-server
# │       │   │   │   │       └── download-server
# │       │   │   │   │           ├── healthz
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   └── downloads-9fcbc4dc7-zf4vs
# │       │   │   │       ├── downloads-9fcbc4dc7-zf4vs.yaml
# │       │   │   │       └── download-server
# │       │   │   │           └── download-server
# │       │   │   │               ├── healthz
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-console-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-console-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── console-operator-dd677d4d7-wdxl7
# │       │   │   │       ├── console-operator
# │       │   │   │       │   └── console-operator
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── console-operator-dd677d4d7-wdxl7.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-controller-manager
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-controller-manager.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── controller-manager-8h69j
# │       │   │   │   │   ├── controller-manager
# │       │   │   │   │   │   └── controller-manager
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── controller-manager-8h69j.yaml
# │       │   │   │   ├── controller-manager-g2254
# │       │   │   │   │   ├── controller-manager
# │       │   │   │   │   │   └── controller-manager
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── controller-manager-g2254.yaml
# │       │   │   │   └── controller-manager-n8dlm
# │       │   │   │       ├── controller-manager
# │       │   │   │       │   └── controller-manager
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── controller-manager-n8dlm.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-controller-manager-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-controller-manager-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── openshift-controller-manager-operator-589c8b6689-bdmqq
# │       │   │   │       ├── openshift-controller-manager-operator-589c8b6689-bdmqq.yaml
# │       │   │   │       └── operator
# │       │   │   │           └── operator
# │       │   │   │               ├── healthz
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-dns
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-dns.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── dns-default-878bg
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-878bg.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-9cwn9
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-9cwn9.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-b8tnk
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-b8tnk.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-d84d7
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-d84d7.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-dscth
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-dscth.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-dwbff
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-dwbff.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-lvbnq
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-lvbnq.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-mq5t8
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-mq5t8.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-mzq2f
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-mzq2f.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-p294t
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-p294t.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-s4tjk
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-s4tjk.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-sv8m4
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-sv8m4.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-tqk52
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-tqk52.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── dns-default-x4vgp
# │       │   │   │   │   ├── dns
# │       │   │   │   │   │   └── dns
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── dns-default-x4vgp.yaml
# │       │   │   │   │   └── dns-node-resolver
# │       │   │   │   │       └── dns-node-resolver
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   └── dns-default-zc8p6
# │       │   │   │       ├── dns
# │       │   │   │       │   └── dns
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── dns-default-zc8p6.yaml
# │       │   │   │       └── dns-node-resolver
# │       │   │   │           └── dns-node-resolver
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-dns-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-dns-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── dns-operator-9c5f9d7d9-q86ld
# │       │   │   │       ├── dns-operator
# │       │   │   │       │   └── dns-operator
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── dns-operator-9c5f9d7d9-q86ld.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-etcd
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-etcd.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── etcd-member-ip-10-0-131-55.us-east-2.compute.internal
# │       │   │   │   │   ├── certs
# │       │   │   │   │   │   └── certs
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── discovery
# │       │   │   │   │   │   └── discovery
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── etcd-member
# │       │   │   │   │   │   └── etcd-member
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── etcd-member-ip-10-0-131-55.us-east-2.compute.internal.yaml
# │       │   │   │   │   └── etcd-metrics
# │       │   │   │   │       └── etcd-metrics
# │       │   │   │   │           ├── healthz
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── etcd-member-ip-10-0-150-228.us-east-2.compute.internal
# │       │   │   │   │   ├── certs
# │       │   │   │   │   │   └── certs
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── discovery
# │       │   │   │   │   │   └── discovery
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── etcd-member
# │       │   │   │   │   │   └── etcd-member
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── etcd-member-ip-10-0-150-228.us-east-2.compute.internal.yaml
# │       │   │   │   │   └── etcd-metrics
# │       │   │   │   │       └── etcd-metrics
# │       │   │   │   │           ├── healthz
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   └── etcd-member-ip-10-0-173-76.us-east-2.compute.internal
# │       │   │   │       ├── certs
# │       │   │   │       │   └── certs
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── discovery
# │       │   │   │       │   └── discovery
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── etcd-member
# │       │   │   │       │   └── etcd-member
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── etcd-member-ip-10-0-173-76.us-east-2.compute.internal.yaml
# │       │   │   │       └── etcd-metrics
# │       │   │   │           └── etcd-metrics
# │       │   │   │               ├── healthz
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-image-registry
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets
# │       │   │   │   │   └── node-ca.yaml
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments
# │       │   │   │   │   └── image-registry.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps
# │       │   │   │   │   ├── image-registry-certificates.yaml
# │       │   │   │   │   └── serviceca.yaml
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets
# │       │   │   │   │   └── image-registry-private-configuration.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   ├── serviceaccounts
# │       │   │   │   │   └── registry.yaml
# │       │   │   │   ├── services
# │       │   │   │   │   └── image-registry.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-image-registry.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── cluster-image-registry-operator-8587f7bfcb-6cptn
# │       │   │   │   │   ├── cluster-image-registry-operator
# │       │   │   │   │   │   └── cluster-image-registry-operator
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── cluster-image-registry-operator-8587f7bfcb-6cptn.yaml
# │       │   │   │   │   └── cluster-image-registry-operator-watch
# │       │   │   │   │       └── cluster-image-registry-operator-watch
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── image-registry-6698d579c9-2cqfd
# │       │   │   │   │   ├── image-registry-6698d579c9-2cqfd.yaml
# │       │   │   │   │   └── registry
# │       │   │   │   │       └── registry
# │       │   │   │   │           ├── healthz
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── node-ca-22grs
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-22grs.yaml
# │       │   │   │   ├── node-ca-55tl6
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-55tl6.yaml
# │       │   │   │   ├── node-ca-575r2
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-575r2.yaml
# │       │   │   │   ├── node-ca-9h5mc
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-9h5mc.yaml
# │       │   │   │   ├── node-ca-9jk2n
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-9jk2n.yaml
# │       │   │   │   ├── node-ca-9p9pl
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-9p9pl.yaml
# │       │   │   │   ├── node-ca-b8sp8
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-b8sp8.yaml
# │       │   │   │   ├── node-ca-fqkj9
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-fqkj9.yaml
# │       │   │   │   ├── node-ca-gqcvf
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-gqcvf.yaml
# │       │   │   │   ├── node-ca-lf5mx
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-lf5mx.yaml
# │       │   │   │   ├── node-ca-mmmql
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-mmmql.yaml
# │       │   │   │   ├── node-ca-r8fb2
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-r8fb2.yaml
# │       │   │   │   ├── node-ca-vbqgb
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-vbqgb.yaml
# │       │   │   │   ├── node-ca-vv2wl
# │       │   │   │   │   ├── node-ca
# │       │   │   │   │   │   └── node-ca
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-ca-vv2wl.yaml
# │       │   │   │   └── node-ca-vxvs2
# │       │   │   │       ├── node-ca
# │       │   │   │       │   └── node-ca
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── node-ca-vxvs2.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-ingress
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-ingress.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── router-default-5d5f9f8649-7mghq
# │       │   │   │   │   ├── router
# │       │   │   │   │   │   └── router
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── router-default-5d5f9f8649-7mghq.yaml
# │       │   │   │   └── router-default-5d5f9f8649-p9p6f
# │       │   │   │       ├── router
# │       │   │   │       │   └── router
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── router-default-5d5f9f8649-p9p6f.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-ingress-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── ingress.operator.openshift.io
# │       │   │   │   └── dnsrecords
# │       │   │   │       └── default-wildcard.yaml
# │       │   │   ├── openshift-ingress-operator.yaml
# │       │   │   ├── operator.openshift.io
# │       │   │   │   └── ingresscontrollers
# │       │   │   │       └── default.yaml
# │       │   │   ├── pods
# │       │   │   │   └── ingress-operator-86d56f89ff-whc9q
# │       │   │   │       ├── ingress-operator
# │       │   │   │       │   └── ingress-operator
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── ingress-operator-86d56f89ff-whc9q.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-insights
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments
# │       │   │   │   │   └── insights-operator.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-insights.yaml
# │       │   │   ├── pods
# │       │   │   │   └── insights-operator-7cd676dd46-ff2pt
# │       │   │   │       ├── insights-operator-7cd676dd46-ff2pt.yaml
# │       │   │   │       └── operator
# │       │   │   │           └── operator
# │       │   │   │               ├── healthz
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-kube-apiserver
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-kube-apiserver.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal
# │       │   │   │   │   ├── kube-apiserver-9
# │       │   │   │   │   │   └── kube-apiserver-9
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-apiserver-cert-syncer-9
# │       │   │   │   │   │   └── kube-apiserver-cert-syncer-9
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-apiserver-insecure-readyz-9
# │       │   │   │   │   │   └── kube-apiserver-insecure-readyz-9
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-apiserver-ip-10-0-131-55.us-east-2.compute.internal.yaml
# │       │   │   │   │   └── setup
# │       │   │   │   │       └── setup
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal
# │       │   │   │   │   ├── kube-apiserver-9
# │       │   │   │   │   │   └── kube-apiserver-9
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-apiserver-cert-syncer-9
# │       │   │   │   │   │   └── kube-apiserver-cert-syncer-9
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-apiserver-insecure-readyz-9
# │       │   │   │   │   │   └── kube-apiserver-insecure-readyz-9
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-apiserver-ip-10-0-150-228.us-east-2.compute.internal.yaml
# │       │   │   │   │   └── setup
# │       │   │   │   │       └── setup
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   └── kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal
# │       │   │   │       ├── kube-apiserver-9
# │       │   │   │       │   └── kube-apiserver-9
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── kube-apiserver-cert-syncer-9
# │       │   │   │       │   └── kube-apiserver-cert-syncer-9
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── kube-apiserver-insecure-readyz-9
# │       │   │   │       │   └── kube-apiserver-insecure-readyz-9
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── kube-apiserver-ip-10-0-173-76.us-east-2.compute.internal.yaml
# │       │   │   │       └── setup
# │       │   │   │           └── setup
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-kube-apiserver-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-kube-apiserver-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── kube-apiserver-operator-84b79d6d8b-bgx57
# │       │   │   │       ├── kube-apiserver-operator
# │       │   │   │       │   └── kube-apiserver-operator
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── kube-apiserver-operator-84b79d6d8b-bgx57.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-kube-controller-manager
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-kube-controller-manager.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal
# │       │   │   │   │   ├── kube-controller-manager-5
# │       │   │   │   │   │   └── kube-controller-manager-5
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-controller-manager-cert-syncer-5
# │       │   │   │   │   │   └── kube-controller-manager-cert-syncer-5
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-controller-manager-ip-10-0-131-55.us-east-2.compute.internal.yaml
# │       │   │   │   │   └── wait-for-host-port
# │       │   │   │   │       └── wait-for-host-port
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal
# │       │   │   │   │   ├── kube-controller-manager-5
# │       │   │   │   │   │   └── kube-controller-manager-5
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-controller-manager-cert-syncer-5
# │       │   │   │   │   │   └── kube-controller-manager-cert-syncer-5
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-controller-manager-ip-10-0-150-228.us-east-2.compute.internal.yaml
# │       │   │   │   │   └── wait-for-host-port
# │       │   │   │   │       └── wait-for-host-port
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   └── kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal
# │       │   │   │       ├── kube-controller-manager-5
# │       │   │   │       │   └── kube-controller-manager-5
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── kube-controller-manager-cert-syncer-5
# │       │   │   │       │   └── kube-controller-manager-cert-syncer-5
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── kube-controller-manager-ip-10-0-173-76.us-east-2.compute.internal.yaml
# │       │   │   │       └── wait-for-host-port
# │       │   │   │           └── wait-for-host-port
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-kube-controller-manager-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-kube-controller-manager-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── kube-controller-manager-operator-6b86f4675b-pjddn
# │       │   │   │       ├── kube-controller-manager-operator
# │       │   │   │       │   └── kube-controller-manager-operator
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── kube-controller-manager-operator-6b86f4675b-pjddn.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-kube-scheduler
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-kube-scheduler.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal
# │       │   │   │   │   ├── openshift-kube-scheduler-ip-10-0-131-55.us-east-2.compute.internal.yaml
# │       │   │   │   │   ├── scheduler
# │       │   │   │   │   │   └── scheduler
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── wait-for-host-port
# │       │   │   │   │       └── wait-for-host-port
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal
# │       │   │   │   │   ├── openshift-kube-scheduler-ip-10-0-150-228.us-east-2.compute.internal.yaml
# │       │   │   │   │   ├── scheduler
# │       │   │   │   │   │   └── scheduler
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── wait-for-host-port
# │       │   │   │   │       └── wait-for-host-port
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   └── openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal
# │       │   │   │       ├── openshift-kube-scheduler-ip-10-0-173-76.us-east-2.compute.internal.yaml
# │       │   │   │       ├── scheduler
# │       │   │   │       │   └── scheduler
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── wait-for-host-port
# │       │   │   │           └── wait-for-host-port
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-kube-scheduler-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-kube-scheduler-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── openshift-kube-scheduler-operator-64f94b4cf4-498mp
# │       │   │   │       ├── kube-scheduler-operator-container
# │       │   │   │       │   └── kube-scheduler-operator-container
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── openshift-kube-scheduler-operator-64f94b4cf4-498mp.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-machine-api
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── machine.openshift.io
# │       │   │   │   ├── machines
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-master-0.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-master-1.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-master-2.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-workerocs-us-east-2a-ncm2v.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-workerocs-us-east-2a-z7psv.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-workerocs-us-east-2b-j92nx.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-workerocs-us-east-2b-zf88g.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-workerocs-us-east-2c-75plw.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-workerocs-us-east-2c-w5ffn.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-worker-us-east-2a-thxfm.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-worker-us-east-2b-wr7qc.yaml
# │       │   │   │   │   ├── cluster-munich-e7ab-lqhqg-worker-us-east-2c-jsnnw.yaml
# │       │   │   │   │   ├── infra-us-east-2c-4fxdc.yaml
# │       │   │   │   │   ├── infra-us-east-2c-ljb6c.yaml
# │       │   │   │   │   └── infra-us-east-2c-zq2jv.yaml
# │       │   │   │   └── machinesets
# │       │   │   │       ├── cluster-munich-e7ab-lqhqg-workerocs-us-east-2a.yaml
# │       │   │   │       ├── cluster-munich-e7ab-lqhqg-workerocs-us-east-2b.yaml
# │       │   │   │       ├── cluster-munich-e7ab-lqhqg-workerocs-us-east-2c.yaml
# │       │   │   │       ├── cluster-munich-e7ab-lqhqg-worker-us-east-2a.yaml
# │       │   │   │       ├── cluster-munich-e7ab-lqhqg-worker-us-east-2b.yaml
# │       │   │   │       ├── cluster-munich-e7ab-lqhqg-worker-us-east-2c.yaml
# │       │   │   │       └── infra-us-east-2c.yaml
# │       │   │   ├── openshift-machine-api.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── cluster-autoscaler-operator-68bc4d6bd-jcrwv
# │       │   │   │   │   ├── cluster-autoscaler-operator
# │       │   │   │   │   │   └── cluster-autoscaler-operator
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── cluster-autoscaler-operator-68bc4d6bd-jcrwv.yaml
# │       │   │   │   ├── machine-api-controllers-cf7d74945-zftzx
# │       │   │   │   │   ├── controller-manager
# │       │   │   │   │   │   └── controller-manager
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── machine-api-controllers-cf7d74945-zftzx.yaml
# │       │   │   │   │   ├── machine-controller
# │       │   │   │   │   │   └── machine-controller
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── nodelink-controller
# │       │   │   │   │       └── nodelink-controller
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   └── machine-api-operator-68cbcfbf9-fzwsf
# │       │   │   │       ├── machine-api-operator
# │       │   │   │       │   └── machine-api-operator
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── machine-api-operator-68cbcfbf9-fzwsf.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-machine-config-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-machine-config-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── etcd-quorum-guard-777d76fbdd-4vqq5
# │       │   │   │   │   ├── etcd-quorum-guard-777d76fbdd-4vqq5.yaml
# │       │   │   │   │   └── guard
# │       │   │   │   │       └── guard
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── etcd-quorum-guard-777d76fbdd-fnzkh
# │       │   │   │   │   ├── etcd-quorum-guard-777d76fbdd-fnzkh.yaml
# │       │   │   │   │   └── guard
# │       │   │   │   │       └── guard
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── etcd-quorum-guard-777d76fbdd-p9vhn
# │       │   │   │   │   ├── etcd-quorum-guard-777d76fbdd-p9vhn.yaml
# │       │   │   │   │   └── guard
# │       │   │   │   │       └── guard
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── kubelet-bootstrap-cred-manager-cg8c2
# │       │   │   │   │   ├── kubelet-bootstrap-cred-manager
# │       │   │   │   │   │   └── kubelet-bootstrap-cred-manager
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── kubelet-bootstrap-cred-manager-cg8c2.yaml
# │       │   │   │   ├── kubelet-bootstrap-cred-manager-mdztn
# │       │   │   │   │   ├── kubelet-bootstrap-cred-manager
# │       │   │   │   │   │   └── kubelet-bootstrap-cred-manager
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── kubelet-bootstrap-cred-manager-mdztn.yaml
# │       │   │   │   ├── kubelet-bootstrap-cred-manager-mnmhl
# │       │   │   │   │   ├── kubelet-bootstrap-cred-manager
# │       │   │   │   │   │   └── kubelet-bootstrap-cred-manager
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── kubelet-bootstrap-cred-manager-mnmhl.yaml
# │       │   │   │   ├── machine-config-controller-84495957cf-lk4wb
# │       │   │   │   │   ├── machine-config-controller
# │       │   │   │   │   │   └── machine-config-controller
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-controller-84495957cf-lk4wb.yaml
# │       │   │   │   ├── machine-config-daemon-bn76l
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-bn76l.yaml
# │       │   │   │   ├── machine-config-daemon-bqtht
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-bqtht.yaml
# │       │   │   │   ├── machine-config-daemon-fjqbt
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-fjqbt.yaml
# │       │   │   │   ├── machine-config-daemon-hdm27
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-hdm27.yaml
# │       │   │   │   ├── machine-config-daemon-lfwnq
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-lfwnq.yaml
# │       │   │   │   ├── machine-config-daemon-ml44b
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-ml44b.yaml
# │       │   │   │   ├── machine-config-daemon-p9zd6
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-p9zd6.yaml
# │       │   │   │   ├── machine-config-daemon-ps76x
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-ps76x.yaml
# │       │   │   │   ├── machine-config-daemon-qqhvz
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-qqhvz.yaml
# │       │   │   │   ├── machine-config-daemon-s7f54
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-s7f54.yaml
# │       │   │   │   ├── machine-config-daemon-sb8sr
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-sb8sr.yaml
# │       │   │   │   ├── machine-config-daemon-sn8f9
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-sn8f9.yaml
# │       │   │   │   ├── machine-config-daemon-v6s2m
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-v6s2m.yaml
# │       │   │   │   ├── machine-config-daemon-x4nrm
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-x4nrm.yaml
# │       │   │   │   ├── machine-config-daemon-zrx7r
# │       │   │   │   │   ├── machine-config-daemon
# │       │   │   │   │   │   └── machine-config-daemon
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-daemon-zrx7r.yaml
# │       │   │   │   ├── machine-config-operator-db69f9f4b-llnrn
# │       │   │   │   │   ├── machine-config-operator
# │       │   │   │   │   │   └── machine-config-operator
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-operator-db69f9f4b-llnrn.yaml
# │       │   │   │   ├── machine-config-server-68v65
# │       │   │   │   │   ├── machine-config-server
# │       │   │   │   │   │   └── machine-config-server
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-server-68v65.yaml
# │       │   │   │   ├── machine-config-server-dw77g
# │       │   │   │   │   ├── machine-config-server
# │       │   │   │   │   │   └── machine-config-server
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── machine-config-server-dw77g.yaml
# │       │   │   │   └── machine-config-server-v8l9q
# │       │   │   │       ├── machine-config-server
# │       │   │   │       │   └── machine-config-server
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── machine-config-server-v8l9q.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-marketplace
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-marketplace.yaml
# │       │   │   ├── operators.coreos.com
# │       │   │   │   ├── catalogsources
# │       │   │   │   │   ├── certified-operators.yaml
# │       │   │   │   │   ├── community-operators.yaml
# │       │   │   │   │   └── redhat-operators.yaml
# │       │   │   │   └── operatorsources
# │       │   │   │       ├── certified-operators.yaml
# │       │   │   │       ├── community-operators.yaml
# │       │   │   │       └── redhat-operators.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── certified-operators-fd9b95b9f-htd9p
# │       │   │   │   │   ├── certified-operators
# │       │   │   │   │   │   └── certified-operators
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── certified-operators-fd9b95b9f-htd9p.yaml
# │       │   │   │   ├── community-operators-7db6b74f69-76rmm
# │       │   │   │   │   ├── community-operators
# │       │   │   │   │   │   └── community-operators
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── community-operators-7db6b74f69-76rmm.yaml
# │       │   │   │   ├── marketplace-operator-78775467c7-ldb6m
# │       │   │   │   │   ├── marketplace-operator
# │       │   │   │   │   │   └── marketplace-operator
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── marketplace-operator-78775467c7-ldb6m.yaml
# │       │   │   │   └── redhat-operators-5bddbf4558-9qh4x
# │       │   │   │       ├── redhat-operators
# │       │   │   │       │   └── redhat-operators
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── redhat-operators-5bddbf4558-9qh4x.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-monitoring
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-monitoring.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── alertmanager-main-0
# │       │   │   │   │   ├── alertmanager
# │       │   │   │   │   │   └── alertmanager
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── alertmanager-main-0.yaml
# │       │   │   │   │   ├── alertmanager-proxy
# │       │   │   │   │   │   └── alertmanager-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       ├── logs
# │       │   │   │   │   │       │   ├── current.log
# │       │   │   │   │   │       │   └── previous.log
# │       │   │   │   │   │       └── metrics.json
# │       │   │   │   │   └── config-reloader
# │       │   │   │   │       └── config-reloader
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── alertmanager-main-1
# │       │   │   │   │   ├── alertmanager
# │       │   │   │   │   │   └── alertmanager
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── alertmanager-main-1.yaml
# │       │   │   │   │   ├── alertmanager-proxy
# │       │   │   │   │   │   └── alertmanager-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       ├── logs
# │       │   │   │   │   │       │   ├── current.log
# │       │   │   │   │   │       │   └── previous.log
# │       │   │   │   │   │       └── metrics.json
# │       │   │   │   │   └── config-reloader
# │       │   │   │   │       └── config-reloader
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── alertmanager-main-2
# │       │   │   │   │   ├── alertmanager
# │       │   │   │   │   │   └── alertmanager
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── alertmanager-main-2.yaml
# │       │   │   │   │   ├── alertmanager-proxy
# │       │   │   │   │   │   └── alertmanager-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       ├── logs
# │       │   │   │   │   │       │   ├── current.log
# │       │   │   │   │   │       │   └── previous.log
# │       │   │   │   │   │       └── metrics.json
# │       │   │   │   │   └── config-reloader
# │       │   │   │   │       └── config-reloader
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── cluster-monitoring-operator-585c8c44d9-hzmnw
# │       │   │   │   │   ├── cluster-monitoring-operator
# │       │   │   │   │   │   └── cluster-monitoring-operator
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── cluster-monitoring-operator-585c8c44d9-hzmnw.yaml
# │       │   │   │   ├── grafana-69685f986d-rl757
# │       │   │   │   │   ├── grafana
# │       │   │   │   │   │   └── grafana
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── grafana-69685f986d-rl757.yaml
# │       │   │   │   │   └── grafana-proxy
# │       │   │   │   │       └── grafana-proxy
# │       │   │   │   │           ├── healthz
# │       │   │   │   │           ├── logs
# │       │   │   │   │           │   ├── current.log
# │       │   │   │   │           │   └── previous.log
# │       │   │   │   │           └── metrics.json
# │       │   │   │   ├── kube-state-metrics-7c884764fd-jdf57
# │       │   │   │   │   ├── kube-rbac-proxy-main
# │       │   │   │   │   │   └── kube-rbac-proxy-main
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-rbac-proxy-self
# │       │   │   │   │   │   └── kube-rbac-proxy-self
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-state-metrics
# │       │   │   │   │   │   └── kube-state-metrics
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── kube-state-metrics-7c884764fd-jdf57.yaml
# │       │   │   │   ├── node-exporter-86k7k
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-86k7k.yaml
# │       │   │   │   ├── node-exporter-d49v5
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-d49v5.yaml
# │       │   │   │   ├── node-exporter-fk267
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-fk267.yaml
# │       │   │   │   ├── node-exporter-gqgjk
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-gqgjk.yaml
# │       │   │   │   ├── node-exporter-hgtjd
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-hgtjd.yaml
# │       │   │   │   ├── node-exporter-lkxvx
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-lkxvx.yaml
# │       │   │   │   ├── node-exporter-nbkxw
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-nbkxw.yaml
# │       │   │   │   ├── node-exporter-p7vrw
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-p7vrw.yaml
# │       │   │   │   ├── node-exporter-pl95f
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-pl95f.yaml
# │       │   │   │   ├── node-exporter-prbc5
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-prbc5.yaml
# │       │   │   │   ├── node-exporter-r9ckd
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-r9ckd.yaml
# │       │   │   │   ├── node-exporter-vm4np
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-vm4np.yaml
# │       │   │   │   ├── node-exporter-wn9hs
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-wn9hs.yaml
# │       │   │   │   ├── node-exporter-xddfk
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-xddfk.yaml
# │       │   │   │   ├── node-exporter-xhwb8
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── node-exporter
# │       │   │   │   │   │   └── node-exporter
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── node-exporter-xhwb8.yaml
# │       │   │   │   ├── openshift-state-metrics-7c76b98c77-kxdjp
# │       │   │   │   │   ├── kube-rbac-proxy-main
# │       │   │   │   │   │   └── kube-rbac-proxy-main
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-rbac-proxy-self
# │       │   │   │   │   │   └── kube-rbac-proxy-self
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── openshift-state-metrics
# │       │   │   │   │   │   └── openshift-state-metrics
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── openshift-state-metrics-7c76b98c77-kxdjp.yaml
# │       │   │   │   ├── prometheus-adapter-986777885-d7rjf
# │       │   │   │   │   ├── prometheus-adapter
# │       │   │   │   │   │   └── prometheus-adapter
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── prometheus-adapter-986777885-d7rjf.yaml
# │       │   │   │   ├── prometheus-adapter-986777885-vdcmj
# │       │   │   │   │   ├── prometheus-adapter
# │       │   │   │   │   │   └── prometheus-adapter
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── prometheus-adapter-986777885-vdcmj.yaml
# │       │   │   │   ├── prometheus-k8s-0
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── prometheus
# │       │   │   │   │   │   └── prometheus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── prometheus-config-reloader
# │       │   │   │   │   │   └── prometheus-config-reloader
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── prometheus-k8s-0.yaml
# │       │   │   │   │   ├── prometheus-proxy
# │       │   │   │   │   │   └── prometheus-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       ├── logs
# │       │   │   │   │   │       │   ├── current.log
# │       │   │   │   │   │       │   └── previous.log
# │       │   │   │   │   │       └── metrics.json
# │       │   │   │   │   ├── prom-label-proxy
# │       │   │   │   │   │   └── prom-label-proxy
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── rules-configmap-reloader
# │       │   │   │   │       └── rules-configmap-reloader
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── prometheus-k8s-1
# │       │   │   │   │   ├── kube-rbac-proxy
# │       │   │   │   │   │   └── kube-rbac-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── prometheus
# │       │   │   │   │   │   └── prometheus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── prometheus-config-reloader
# │       │   │   │   │   │   └── prometheus-config-reloader
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── prometheus-k8s-1.yaml
# │       │   │   │   │   ├── prometheus-proxy
# │       │   │   │   │   │   └── prometheus-proxy
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       ├── logs
# │       │   │   │   │   │       │   ├── current.log
# │       │   │   │   │   │       │   └── previous.log
# │       │   │   │   │   │       └── metrics.json
# │       │   │   │   │   ├── prom-label-proxy
# │       │   │   │   │   │   └── prom-label-proxy
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── rules-configmap-reloader
# │       │   │   │   │       └── rules-configmap-reloader
# │       │   │   │   │           └── logs
# │       │   │   │   │               ├── current.log
# │       │   │   │   │               └── previous.log
# │       │   │   │   ├── prometheus-operator-7c8568cc64-x5vtt
# │       │   │   │   │   ├── prometheus-operator
# │       │   │   │   │   │   └── prometheus-operator
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── prometheus-operator-7c8568cc64-x5vtt.yaml
# │       │   │   │   └── telemeter-client-944599596-c5jzs
# │       │   │   │       ├── kube-rbac-proxy
# │       │   │   │       │   └── kube-rbac-proxy
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── reload
# │       │   │   │       │   └── reload
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── telemeter-client
# │       │   │   │       │   └── telemeter-client
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── telemeter-client-944599596-c5jzs.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-multus
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets
# │       │   │   │   │   ├── multus-admission-controller.yaml
# │       │   │   │   │   └── multus.yaml
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   ├── serviceaccounts
# │       │   │   │   │   └── multus.yaml
# │       │   │   │   ├── services
# │       │   │   │   │   └── multus-admission-controller.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-multus.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── multus-66xjb
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-66xjb.yaml
# │       │   │   │   ├── multus-8c89h
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-8c89h.yaml
# │       │   │   │   ├── multus-9n5xm
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-9n5xm.yaml
# │       │   │   │   ├── multus-admission-controller-lrvhs
# │       │   │   │   │   ├── multus-admission-controller
# │       │   │   │   │   │   └── multus-admission-controller
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-admission-controller-lrvhs.yaml
# │       │   │   │   ├── multus-admission-controller-nhpzv
# │       │   │   │   │   ├── multus-admission-controller
# │       │   │   │   │   │   └── multus-admission-controller
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-admission-controller-nhpzv.yaml
# │       │   │   │   ├── multus-admission-controller-pkrr8
# │       │   │   │   │   ├── multus-admission-controller
# │       │   │   │   │   │   └── multus-admission-controller
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-admission-controller-pkrr8.yaml
# │       │   │   │   ├── multus-bftmf
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-bftmf.yaml
# │       │   │   │   ├── multus-cqsb7
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-cqsb7.yaml
# │       │   │   │   ├── multus-dzl7n
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-dzl7n.yaml
# │       │   │   │   ├── multus-h5zhb
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-h5zhb.yaml
# │       │   │   │   ├── multus-kr7xv
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-kr7xv.yaml
# │       │   │   │   ├── multus-mm2lb
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-mm2lb.yaml
# │       │   │   │   ├── multus-nbkgp
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-nbkgp.yaml
# │       │   │   │   ├── multus-pwcvp
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-pwcvp.yaml
# │       │   │   │   ├── multus-s5sw5
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-s5sw5.yaml
# │       │   │   │   ├── multus-sfdmt
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-sfdmt.yaml
# │       │   │   │   ├── multus-t7l56
# │       │   │   │   │   ├── cni-plugins
# │       │   │   │   │   │   └── cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── kube-multus
# │       │   │   │   │   │   └── kube-multus
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── multus-t7l56.yaml
# │       │   │   │   └── multus-zfcl4
# │       │   │   │       ├── cni-plugins
# │       │   │   │       │   └── cni-plugins
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── kube-multus
# │       │   │   │       │   └── kube-multus
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── multus-zfcl4.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-network-operator
# │       │   │   └── core
# │       │   │       └── configmaps
# │       │   │           ├── applied-cluster.yaml
# │       │   │           └── openshift-service-ca.yaml
# │       │   ├── openshift-operator-lifecycle-manager
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-operator-lifecycle-manager.yaml
# │       │   │   ├── operators.coreos.com
# │       │   │   │   └── clusterserviceversions
# │       │   │   │       └── packageserver.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── catalog-operator-5555bbb485-w279j
# │       │   │   │   │   ├── catalog-operator
# │       │   │   │   │   │   └── catalog-operator
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── catalog-operator-5555bbb485-w279j.yaml
# │       │   │   │   ├── olm-operator-84c545cfdc-mrmfd
# │       │   │   │   │   ├── olm-operator
# │       │   │   │   │   │   └── olm-operator
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── olm-operator-84c545cfdc-mrmfd.yaml
# │       │   │   │   ├── packageserver-7448d988bc-bw6n2
# │       │   │   │   │   ├── packageserver
# │       │   │   │   │   │   └── packageserver
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── packageserver-7448d988bc-bw6n2.yaml
# │       │   │   │   └── packageserver-7448d988bc-ljqdf
# │       │   │   │       ├── packageserver
# │       │   │   │       │   └── packageserver
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── packageserver-7448d988bc-ljqdf.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-sdn
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets
# │       │   │   │   │   ├── ovs.yaml
# │       │   │   │   │   ├── sdn-controller.yaml
# │       │   │   │   │   └── sdn.yaml
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps
# │       │   │   │   │   └── sdn-config.yaml
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   ├── serviceaccounts
# │       │   │   │   │   ├── sdn-controller.yaml
# │       │   │   │   │   └── sdn.yaml
# │       │   │   │   ├── services
# │       │   │   │   │   └── sdn.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── monitoring.coreos.com
# │       │   │   │   ├── prometheusrules
# │       │   │   │   │   └── networking-rules.yaml
# │       │   │   │   └── servicemonitors
# │       │   │   │       └── monitor-sdn.yaml
# │       │   │   ├── openshift-sdn.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── ovs-2zbhf
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-2zbhf.yaml
# │       │   │   │   ├── ovs-5g7tz
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-5g7tz.yaml
# │       │   │   │   ├── ovs-8bfdp
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-8bfdp.yaml
# │       │   │   │   ├── ovs-8k6gp
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-8k6gp.yaml
# │       │   │   │   ├── ovs-95smf
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-95smf.yaml
# │       │   │   │   ├── ovs-9s55d
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-9s55d.yaml
# │       │   │   │   ├── ovs-b6h8n
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-b6h8n.yaml
# │       │   │   │   ├── ovs-c2drb
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-c2drb.yaml
# │       │   │   │   ├── ovs-c5khk
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-c5khk.yaml
# │       │   │   │   ├── ovs-kzvdb
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-kzvdb.yaml
# │       │   │   │   ├── ovs-m5x6s
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-m5x6s.yaml
# │       │   │   │   ├── ovs-mmjdt
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-mmjdt.yaml
# │       │   │   │   ├── ovs-pjmpg
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-pjmpg.yaml
# │       │   │   │   ├── ovs-vxcxx
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-vxcxx.yaml
# │       │   │   │   ├── ovs-z5cx4
# │       │   │   │   │   ├── openvswitch
# │       │   │   │   │   │   └── openvswitch
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── ovs-z5cx4.yaml
# │       │   │   │   ├── sdn-5gp57
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-5gp57.yaml
# │       │   │   │   ├── sdn-78qkj
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-78qkj.yaml
# │       │   │   │   ├── sdn-9zftm
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-9zftm.yaml
# │       │   │   │   ├── sdn-controller-dgtx5
# │       │   │   │   │   ├── sdn-controller
# │       │   │   │   │   │   └── sdn-controller
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-controller-dgtx5.yaml
# │       │   │   │   ├── sdn-controller-xcz2l
# │       │   │   │   │   ├── sdn-controller
# │       │   │   │   │   │   └── sdn-controller
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-controller-xcz2l.yaml
# │       │   │   │   ├── sdn-controller-ztg6b
# │       │   │   │   │   ├── sdn-controller
# │       │   │   │   │   │   └── sdn-controller
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-controller-ztg6b.yaml
# │       │   │   │   ├── sdn-gdx8p
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-gdx8p.yaml
# │       │   │   │   ├── sdn-hv6h9
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-hv6h9.yaml
# │       │   │   │   ├── sdn-hwq7n
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-hwq7n.yaml
# │       │   │   │   ├── sdn-lhmr5
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-lhmr5.yaml
# │       │   │   │   ├── sdn-mp57c
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-mp57c.yaml
# │       │   │   │   ├── sdn-n8b8n
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-n8b8n.yaml
# │       │   │   │   ├── sdn-nznzf
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-nznzf.yaml
# │       │   │   │   ├── sdn-p59fb
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-p59fb.yaml
# │       │   │   │   ├── sdn-qvnt5
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-qvnt5.yaml
# │       │   │   │   ├── sdn-xlxs7
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-xlxs7.yaml
# │       │   │   │   ├── sdn-zlwrw
# │       │   │   │   │   ├── install-cni-plugins
# │       │   │   │   │   │   └── install-cni-plugins
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   ├── sdn
# │       │   │   │   │   │   └── sdn
# │       │   │   │   │   │       ├── healthz
# │       │   │   │   │   │       └── logs
# │       │   │   │   │   │           ├── current.log
# │       │   │   │   │   │           └── previous.log
# │       │   │   │   │   └── sdn-zlwrw.yaml
# │       │   │   │   └── sdn-zzxgm
# │       │   │   │       ├── install-cni-plugins
# │       │   │   │       │   └── install-cni-plugins
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       ├── sdn
# │       │   │   │       │   └── sdn
# │       │   │   │       │       ├── healthz
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── sdn-zzxgm.yaml
# │       │   │   ├── rbac.authorization.k8s.io
# │       │   │   │   ├── rolebindings
# │       │   │   │   │   ├── openshift-sdn-controller-leaderelection.yaml
# │       │   │   │   │   └── prometheus-k8s.yaml
# │       │   │   │   └── roles
# │       │   │   │       ├── openshift-sdn-controller-leaderelection.yaml
# │       │   │   │       └── prometheus-k8s.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-service-ca
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-service-ca.yaml
# │       │   │   ├── pods
# │       │   │   │   ├── apiservice-cabundle-injector-545f9c779-d2jj2
# │       │   │   │   │   ├── apiservice-cabundle-injector-545f9c779-d2jj2.yaml
# │       │   │   │   │   └── apiservice-cabundle-injector-controller
# │       │   │   │   │       └── apiservice-cabundle-injector-controller
# │       │   │   │   │           ├── healthz
# │       │   │   │   │           │   ├── index
# │       │   │   │   │           │   ├── log
# │       │   │   │   │           │   └── ping
# │       │   │   │   │           ├── logs
# │       │   │   │   │           │   ├── current.log
# │       │   │   │   │           │   └── previous.log
# │       │   │   │   │           └── metrics.json
# │       │   │   │   ├── configmap-cabundle-injector-b7f4cd66c-4g9sk
# │       │   │   │   │   ├── configmap-cabundle-injector-b7f4cd66c-4g9sk.yaml
# │       │   │   │   │   └── configmap-cabundle-injector-controller
# │       │   │   │   │       └── configmap-cabundle-injector-controller
# │       │   │   │   │           ├── healthz
# │       │   │   │   │           │   ├── index
# │       │   │   │   │           │   ├── log
# │       │   │   │   │           │   └── ping
# │       │   │   │   │           ├── logs
# │       │   │   │   │           │   ├── current.log
# │       │   │   │   │           │   └── previous.log
# │       │   │   │   │           └── metrics.json
# │       │   │   │   └── service-serving-cert-signer-6c8f5dcdf7-8xsjz
# │       │   │   │       ├── service-serving-cert-signer-6c8f5dcdf7-8xsjz.yaml
# │       │   │   │       └── service-serving-cert-signer-controller
# │       │   │   │           └── service-serving-cert-signer-controller
# │       │   │   │               ├── healthz
# │       │   │   │               │   ├── index
# │       │   │   │               │   ├── log
# │       │   │   │               │   └── ping
# │       │   │   │               ├── logs
# │       │   │   │               │   ├── current.log
# │       │   │   │               │   └── previous.log
# │       │   │   │               └── metrics.json
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-service-ca-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-service-ca-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── service-ca-operator-5bdc97dfcd-s42ds
# │       │   │   │       ├── operator
# │       │   │   │       │   └── operator
# │       │   │   │       │       └── logs
# │       │   │   │       │           ├── current.log
# │       │   │   │       │           └── previous.log
# │       │   │   │       └── service-ca-operator-5bdc97dfcd-s42ds.yaml
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   ├── openshift-service-catalog-apiserver-operator
# │       │   │   ├── apps
# │       │   │   │   ├── daemonsets.yaml
# │       │   │   │   ├── deployments.yaml
# │       │   │   │   ├── replicasets.yaml
# │       │   │   │   └── statefulsets.yaml
# │       │   │   ├── apps.openshift.io
# │       │   │   │   └── deploymentconfigs.yaml
# │       │   │   ├── autoscaling
# │       │   │   │   └── horizontalpodautoscalers.yaml
# │       │   │   ├── batch
# │       │   │   │   ├── cronjobs.yaml
# │       │   │   │   └── jobs.yaml
# │       │   │   ├── build.openshift.io
# │       │   │   │   ├── buildconfigs.yaml
# │       │   │   │   └── builds.yaml
# │       │   │   ├── core
# │       │   │   │   ├── configmaps.yaml
# │       │   │   │   ├── events.yaml
# │       │   │   │   ├── persistentvolumeclaims.yaml
# │       │   │   │   ├── pods.yaml
# │       │   │   │   ├── replicationcontrollers.yaml
# │       │   │   │   ├── secrets.yaml
# │       │   │   │   └── services.yaml
# │       │   │   ├── image.openshift.io
# │       │   │   │   └── imagestreams.yaml
# │       │   │   ├── openshift-service-catalog-apiserver-operator.yaml
# │       │   │   ├── pods
# │       │   │   │   └── openshift-service-catalog-apiserver-operator-5879db865f-r2gdq
# │       │   │   │       ├── openshift-service-catalog-apiserver-operator-5879db865f-r2gdq.yaml
# │       │   │   │       └── operator
# │       │   │   │           └── operator
# │       │   │   │               ├── healthz
# │       │   │   │               └── logs
# │       │   │   │                   ├── current.log
# │       │   │   │                   └── previous.log
# │       │   │   └── route.openshift.io
# │       │   │       └── routes.yaml
# │       │   └── openshift-service-catalog-controller-manager-operator
# │       │       ├── apps
# │       │       │   ├── daemonsets.yaml
# │       │       │   ├── deployments.yaml
# │       │       │   ├── replicasets.yaml
# │       │       │   └── statefulsets.yaml
# │       │       ├── apps.openshift.io
# │       │       │   └── deploymentconfigs.yaml
# │       │       ├── autoscaling
# │       │       │   └── horizontalpodautoscalers.yaml
# │       │       ├── batch
# │       │       │   ├── cronjobs.yaml
# │       │       │   └── jobs.yaml
# │       │       ├── build.openshift.io
# │       │       │   ├── buildconfigs.yaml
# │       │       │   └── builds.yaml
# │       │       ├── core
# │       │       │   ├── configmaps.yaml
# │       │       │   ├── events.yaml
# │       │       │   ├── persistentvolumeclaims.yaml
# │       │       │   ├── pods.yaml
# │       │       │   ├── replicationcontrollers.yaml
# │       │       │   ├── secrets.yaml
# │       │       │   └── services.yaml
# │       │       ├── image.openshift.io
# │       │       │   └── imagestreams.yaml
# │       │       ├── openshift-service-catalog-controller-manager-operator.yaml
# │       │       ├── pods
# │       │       │   └── openshift-service-catalog-controller-manager-operator-5478wjbwg
# │       │       │       ├── openshift-service-catalog-controller-manager-operator-5478wjbwg.yaml
# │       │       │       └── operator
# │       │       │           └── operator
# │       │       │               ├── healthz
# │       │       │               └── logs
# │       │       │                   ├── current.log
# │       │       │                   └── previous.log
# │       │       └── route.openshift.io
# │       │           └── routes.yaml
# │       └── version
# ├── noobaa
# ├── #ops-road-show.org#
# ├── ops-road-show.org
# ├── ops-road-show.org~
# ├── packer
# │   ├── ocp-cns-aio.json
# │   ├── README.md
# │   ├── roles
# │   │   ├── idm-install
# │   │   │   ├── files
# │   │   │   │   └── idm-install
# │   │   │   └── tasks
# │   │   │       └── main.yml
# │   │   ├── instructor-user
# │   │   │   └── tasks
# │   │   │       └── main.yml
# │   │   └── registered-host
# │   │       ├── files
# │   │       │   └── product-id.conf
# │   │       ├── tasks
# │   │       │   └── main.yml
# │   │       └── vars
# │   │           └── main.yml
# │   └── site.yml
# ├── packer-host
# │   ├── ec2.ini
# │   ├── ec2.py
# │   ├── environment.yaml
# │   ├── README.md
# │   ├── terminate.yaml
# │   └── vars.yaml
# ├── README.md
# ├── support
# │   ├── ca.crt
# │   ├── cluster-monitoring-configmap.yaml
# │   ├── create-net-projects.sh
# │   ├── cr_project_request.yaml
# │   ├── groupsync.yaml
# │   ├── infra-nodes.sh
# │   ├── ingresscontroller.yaml
# │   ├── netproj-template.yaml
# │   ├── network-policy-allow-all-from-netproj-a.yaml
# │   ├── network-policy-block-all.yaml
# │   ├── oauth-cluster.yaml
# │   ├── ocslab_cluster-monitoring-noinfra.yaml
# │   ├── ocslab_cluster-monitoring-withinfra.yaml
# │   ├── ocslab_cluster-workerocs.yaml
# │   ├── ocslab_deploy-with-olm.yaml
# │   ├── ocslab_obc-app-example.yaml
# │   ├── ocslab_rails-app.yaml
# │   ├── ocslab_toolbox.yaml
# │   ├── openshift_logging_namespace.yaml
# │   ├── project_request_template.yaml
# │   └── test-connectivity.sh
# ├── testdir
# │   └── test.html
# └── test.html
# 
# 1852 directories, 2084 files
#+end_example

#+begin_src bash
oc adm must-gather --image=quay.io/rhceph-dev/ocs-must-gather:latest-4.2
#+end_src

#+begin_example
# [must-gather      ] OUT Using must-gather plugin-in image: quay.io/rhceph-dev/ocs-must-gather:latest-4.2
# [must-gather      ] OUT namespace/openshift-must-gather-8gkg5 created
# [must-gather      ] OUT clusterrolebinding.rbac.authorization.k8s.io/must-gather-2v5sh created
# [must-gather      ] OUT pod for plug-in image quay.io/rhceph-dev/ocs-must-gather:latest-4.2 created
# [must-gather-m2ggn] POD Collecting operator pod logs
# [must-gather-m2ggn] POD Collection dump of storageclusters
# [must-gather-m2ggn] POD 2020/02/21 11:50:34 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collection dump of objectbucketclaims
# [must-gather-m2ggn] POD 2020/02/21 11:50:34 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collection dump of objectbuckets
# [must-gather-m2ggn] POD 2020/02/21 11:50:34 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collecting dump of noobaa
# [must-gather-m2ggn] POD 2020/02/21 11:50:34 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collecting dump of backingstore
# [must-gather-m2ggn] POD 2020/02/21 11:50:34 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collecting dump of bucketclass
# [must-gather-m2ggn] POD 2020/02/21 11:50:34 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collecting dump cephblockpools
# [must-gather-m2ggn] POD 2020/02/21 11:50:35 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collecting dump cephclusters
# [must-gather-m2ggn] POD 2020/02/21 11:50:35 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collecting dump cephfilesystems
# [must-gather-m2ggn] POD 2020/02/21 11:50:35 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collecting dump cephobjectstores
# [must-gather-m2ggn] POD 2020/02/21 11:50:35 Finished successfully with no errors.
# [must-gather-m2ggn] POD Collecting dump cephobjectstoreusers
# [must-gather-m2ggn] POD 2020/02/21 11:50:35 Finished successfully with no errors.
# [must-gather-m2ggn] POD pod/must-gather-m2ggn-helper created
# [must-gather-m2ggn] POD Collecting dump of namespace
# [must-gather-m2ggn] POD 2020/02/21 11:50:35 Gathering data for ns/openshift-storage...
# [must-gather-m2ggn] POD 2020/02/21 11:50:35     Collecting resources for namespace "openshift-storage"...
# [must-gather-m2ggn] POD 2020/02/21 11:50:36     Gathering pod data for namespace "openshift-storage"...
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Gathering data for pod "csi-cephfsplugin-b8pfk"
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-b8pfk" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Skipping container endpoint collection for pod "csi-cephfsplugin-b8pfk" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-b8pfk" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Skipping container endpoint collection for pod "csi-cephfsplugin-b8pfk" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-b8pfk" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Skipping container endpoint collection for pod "csi-cephfsplugin-b8pfk" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Gathering data for pod "csi-cephfsplugin-c9k9s"
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-c9k9s" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Skipping container endpoint collection for pod "csi-cephfsplugin-c9k9s" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-c9k9s" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Skipping container endpoint collection for pod "csi-cephfsplugin-c9k9s" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-c9k9s" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Skipping container endpoint collection for pod "csi-cephfsplugin-c9k9s" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Gathering data for pod "csi-cephfsplugin-f9hcs"
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-f9hcs" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:36         Skipping container endpoint collection for pod "csi-cephfsplugin-f9hcs" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:37         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-f9hcs" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:37         Skipping container endpoint collection for pod "csi-cephfsplugin-f9hcs" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:37         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-f9hcs" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:37         Skipping container endpoint collection for pod "csi-cephfsplugin-f9hcs" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:37         Gathering data for pod "csi-cephfsplugin-j4fkc"
# [must-gather-m2ggn] POD 2020/02/21 11:50:38         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-j4fkc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:38         Skipping container endpoint collection for pod "csi-cephfsplugin-j4fkc" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:38         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-j4fkc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:38         Skipping container endpoint collection for pod "csi-cephfsplugin-j4fkc" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:38         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-j4fkc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:38         Skipping container endpoint collection for pod "csi-cephfsplugin-j4fkc" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:38         Gathering data for pod "csi-cephfsplugin-kblgw"
# [must-gather-m2ggn] POD 2020/02/21 11:50:39         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-kblgw" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:39         Skipping container endpoint collection for pod "csi-cephfsplugin-kblgw" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:39         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-kblgw" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:39         Skipping container endpoint collection for pod "csi-cephfsplugin-kblgw" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:40         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-kblgw" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:40         Skipping container endpoint collection for pod "csi-cephfsplugin-kblgw" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:40         Gathering data for pod "csi-cephfsplugin-kv2nw"
# [must-gather-m2ggn] POD 2020/02/21 11:50:40         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-kv2nw" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:40         Skipping container endpoint collection for pod "csi-cephfsplugin-kv2nw" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:40         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-kv2nw" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:40         Skipping container endpoint collection for pod "csi-cephfsplugin-kv2nw" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:41         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-kv2nw" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:41         Skipping container endpoint collection for pod "csi-cephfsplugin-kv2nw" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:41         Gathering data for pod "csi-cephfsplugin-lbjj6"
# [must-gather-m2ggn] POD 2020/02/21 11:50:41         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-lbjj6" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:41         Skipping container endpoint collection for pod "csi-cephfsplugin-lbjj6" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:42         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-lbjj6" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:42         Skipping container endpoint collection for pod "csi-cephfsplugin-lbjj6" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:42         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-lbjj6" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:42         Skipping container endpoint collection for pod "csi-cephfsplugin-lbjj6" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:42         Gathering data for pod "csi-cephfsplugin-provisioner-647cd6996c-jdsf4"
# [must-gather-m2ggn] POD 2020/02/21 11:50:42         Unable to gather previous container logs: previous terminated container "csi-attacher" in pod "csi-cephfsplugin-provisioner-647cd6996c-jdsf4" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:42         Skipping container endpoint collection for pod "csi-cephfsplugin-provisioner-647cd6996c-jdsf4" container "csi-attacher": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:43         Unable to gather previous container logs: previous terminated container "csi-provisioner" in pod "csi-cephfsplugin-provisioner-647cd6996c-jdsf4" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:43         Skipping container endpoint collection for pod "csi-cephfsplugin-provisioner-647cd6996c-jdsf4" container "csi-provisioner": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:43         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-provisioner-647cd6996c-jdsf4" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:43         Skipping container endpoint collection for pod "csi-cephfsplugin-provisioner-647cd6996c-jdsf4" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:44         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-provisioner-647cd6996c-jdsf4" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:44         Skipping container endpoint collection for pod "csi-cephfsplugin-provisioner-647cd6996c-jdsf4" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:44         Gathering data for pod "csi-cephfsplugin-provisioner-647cd6996c-q45l9"
# [must-gather-m2ggn] POD 2020/02/21 11:50:44         Unable to gather previous container logs: previous terminated container "csi-attacher" in pod "csi-cephfsplugin-provisioner-647cd6996c-q45l9" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:44         Skipping container endpoint collection for pod "csi-cephfsplugin-provisioner-647cd6996c-q45l9" container "csi-attacher": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:44         Unable to gather previous container logs: previous terminated container "csi-provisioner" in pod "csi-cephfsplugin-provisioner-647cd6996c-q45l9" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:44         Skipping container endpoint collection for pod "csi-cephfsplugin-provisioner-647cd6996c-q45l9" container "csi-provisioner": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:45         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-provisioner-647cd6996c-q45l9" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:45         Skipping container endpoint collection for pod "csi-cephfsplugin-provisioner-647cd6996c-q45l9" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:45         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-provisioner-647cd6996c-q45l9" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:45         Skipping container endpoint collection for pod "csi-cephfsplugin-provisioner-647cd6996c-q45l9" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:45         Gathering data for pod "csi-cephfsplugin-q52sm"
# [must-gather-m2ggn] POD 2020/02/21 11:50:46         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-q52sm" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:46         Skipping container endpoint collection for pod "csi-cephfsplugin-q52sm" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:46         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-q52sm" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:46         Skipping container endpoint collection for pod "csi-cephfsplugin-q52sm" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:46         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-q52sm" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:46         Skipping container endpoint collection for pod "csi-cephfsplugin-q52sm" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:46         Gathering data for pod "csi-cephfsplugin-rkh44"
# [must-gather-m2ggn] POD 2020/02/21 11:50:47         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-rkh44" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:47         Skipping container endpoint collection for pod "csi-cephfsplugin-rkh44" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:47         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-rkh44" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:47         Skipping container endpoint collection for pod "csi-cephfsplugin-rkh44" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:48         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-rkh44" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:48         Skipping container endpoint collection for pod "csi-cephfsplugin-rkh44" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:48         Gathering data for pod "csi-cephfsplugin-thzlr"
# [must-gather-m2ggn] POD 2020/02/21 11:50:48         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-thzlr" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:48         Skipping container endpoint collection for pod "csi-cephfsplugin-thzlr" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:48         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-thzlr" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:48         Skipping container endpoint collection for pod "csi-cephfsplugin-thzlr" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:49         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-thzlr" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:49         Skipping container endpoint collection for pod "csi-cephfsplugin-thzlr" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:49         Gathering data for pod "csi-cephfsplugin-xbb8b"
# [must-gather-m2ggn] POD 2020/02/21 11:50:49         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-xbb8b" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:49         Skipping container endpoint collection for pod "csi-cephfsplugin-xbb8b" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:50         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-xbb8b" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:50         Skipping container endpoint collection for pod "csi-cephfsplugin-xbb8b" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:50         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-xbb8b" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:50         Skipping container endpoint collection for pod "csi-cephfsplugin-xbb8b" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:50         Gathering data for pod "csi-cephfsplugin-xwmrc"
# [must-gather-m2ggn] POD 2020/02/21 11:50:50         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-cephfsplugin-xwmrc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:50         Skipping container endpoint collection for pod "csi-cephfsplugin-xwmrc" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:51         Unable to gather previous container logs: previous terminated container "csi-cephfsplugin" in pod "csi-cephfsplugin-xwmrc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:51         Skipping container endpoint collection for pod "csi-cephfsplugin-xwmrc" container "csi-cephfsplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:51         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-cephfsplugin-xwmrc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:51         Skipping container endpoint collection for pod "csi-cephfsplugin-xwmrc" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:51         Gathering data for pod "csi-rbdplugin-2f7zj"
# [must-gather-m2ggn] POD 2020/02/21 11:50:52         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-2f7zj" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:52         Skipping container endpoint collection for pod "csi-rbdplugin-2f7zj" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:52         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-2f7zj" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:52         Skipping container endpoint collection for pod "csi-rbdplugin-2f7zj" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:52         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-2f7zj" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:52         Skipping container endpoint collection for pod "csi-rbdplugin-2f7zj" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:52         Gathering data for pod "csi-rbdplugin-6tr5t"
# [must-gather-m2ggn] POD 2020/02/21 11:50:53         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-6tr5t" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:53         Skipping container endpoint collection for pod "csi-rbdplugin-6tr5t" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:53         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-6tr5t" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:53         Skipping container endpoint collection for pod "csi-rbdplugin-6tr5t" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:54         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-6tr5t" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:54         Skipping container endpoint collection for pod "csi-rbdplugin-6tr5t" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:54         Gathering data for pod "csi-rbdplugin-9tgfg"
# [must-gather-m2ggn] POD 2020/02/21 11:50:54         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-9tgfg" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:54         Skipping container endpoint collection for pod "csi-rbdplugin-9tgfg" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:54         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-9tgfg" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:54         Skipping container endpoint collection for pod "csi-rbdplugin-9tgfg" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:55         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-9tgfg" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:55         Skipping container endpoint collection for pod "csi-rbdplugin-9tgfg" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:55         Gathering data for pod "csi-rbdplugin-b8sbd"
# [must-gather-m2ggn] POD 2020/02/21 11:50:55         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-b8sbd" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:55         Skipping container endpoint collection for pod "csi-rbdplugin-b8sbd" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:56         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-b8sbd" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:56         Skipping container endpoint collection for pod "csi-rbdplugin-b8sbd" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:56         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-b8sbd" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:56         Skipping container endpoint collection for pod "csi-rbdplugin-b8sbd" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:56         Gathering data for pod "csi-rbdplugin-cxrwp"
# [must-gather-m2ggn] POD 2020/02/21 11:50:56         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-cxrwp" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:56         Skipping container endpoint collection for pod "csi-rbdplugin-cxrwp" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:57         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-cxrwp" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:57         Skipping container endpoint collection for pod "csi-rbdplugin-cxrwp" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:57         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-cxrwp" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:57         Skipping container endpoint collection for pod "csi-rbdplugin-cxrwp" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:57         Gathering data for pod "csi-rbdplugin-fh6xz"
# [must-gather-m2ggn] POD 2020/02/21 11:50:58         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-fh6xz" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:58         Skipping container endpoint collection for pod "csi-rbdplugin-fh6xz" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:58         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-fh6xz" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:58         Skipping container endpoint collection for pod "csi-rbdplugin-fh6xz" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:58         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-fh6xz" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:58         Skipping container endpoint collection for pod "csi-rbdplugin-fh6xz" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:58         Gathering data for pod "csi-rbdplugin-grd57"
# [must-gather-m2ggn] POD 2020/02/21 11:50:59         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-grd57" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:59         Skipping container endpoint collection for pod "csi-rbdplugin-grd57" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:50:59         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-grd57" not found
# [must-gather-m2ggn] POD 2020/02/21 11:50:59         Skipping container endpoint collection for pod "csi-rbdplugin-grd57" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:00         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-grd57" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:00         Skipping container endpoint collection for pod "csi-rbdplugin-grd57" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:00         Gathering data for pod "csi-rbdplugin-m29tc"
# [must-gather-m2ggn] POD 2020/02/21 11:51:00         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-m29tc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:00         Skipping container endpoint collection for pod "csi-rbdplugin-m29tc" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:00         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-m29tc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:00         Skipping container endpoint collection for pod "csi-rbdplugin-m29tc" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:01         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-m29tc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:01         Skipping container endpoint collection for pod "csi-rbdplugin-m29tc" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:01         Gathering data for pod "csi-rbdplugin-provisioner-6b8ff67dc4-4xjql"
# [must-gather-m2ggn] POD 2020/02/21 11:51:01         Unable to gather previous container logs: previous terminated container "csi-provisioner" in pod "csi-rbdplugin-provisioner-6b8ff67dc4-4xjql" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:01         Skipping container endpoint collection for pod "csi-rbdplugin-provisioner-6b8ff67dc4-4xjql" container "csi-provisioner": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:02         Unable to gather previous container logs: previous terminated container "csi-rbdplugin-attacher" in pod "csi-rbdplugin-provisioner-6b8ff67dc4-4xjql" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:02         Skipping container endpoint collection for pod "csi-rbdplugin-provisioner-6b8ff67dc4-4xjql" container "csi-rbdplugin-attacher": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:02         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-provisioner-6b8ff67dc4-4xjql" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:02         Skipping container endpoint collection for pod "csi-rbdplugin-provisioner-6b8ff67dc4-4xjql" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:02         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-provisioner-6b8ff67dc4-4xjql" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:02         Skipping container endpoint collection for pod "csi-rbdplugin-provisioner-6b8ff67dc4-4xjql" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:02         Gathering data for pod "csi-rbdplugin-provisioner-6b8ff67dc4-5z97f"
# [must-gather-m2ggn] POD 2020/02/21 11:51:03         Unable to gather previous container logs: previous terminated container "csi-provisioner" in pod "csi-rbdplugin-provisioner-6b8ff67dc4-5z97f" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:03         Skipping container endpoint collection for pod "csi-rbdplugin-provisioner-6b8ff67dc4-5z97f" container "csi-provisioner": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:03         Unable to gather previous container logs: previous terminated container "csi-rbdplugin-attacher" in pod "csi-rbdplugin-provisioner-6b8ff67dc4-5z97f" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:03         Skipping container endpoint collection for pod "csi-rbdplugin-provisioner-6b8ff67dc4-5z97f" container "csi-rbdplugin-attacher": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:04         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-provisioner-6b8ff67dc4-5z97f" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:04         Skipping container endpoint collection for pod "csi-rbdplugin-provisioner-6b8ff67dc4-5z97f" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:04         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-provisioner-6b8ff67dc4-5z97f" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:04         Skipping container endpoint collection for pod "csi-rbdplugin-provisioner-6b8ff67dc4-5z97f" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:04         Gathering data for pod "csi-rbdplugin-qwx8d"
# [must-gather-m2ggn] POD 2020/02/21 11:51:04         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-qwx8d" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:04         Skipping container endpoint collection for pod "csi-rbdplugin-qwx8d" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:05         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-qwx8d" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:05         Skipping container endpoint collection for pod "csi-rbdplugin-qwx8d" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:05         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-qwx8d" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:05         Skipping container endpoint collection for pod "csi-rbdplugin-qwx8d" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:05         Gathering data for pod "csi-rbdplugin-s8j9r"
# [must-gather-m2ggn] POD 2020/02/21 11:51:06         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-s8j9r" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:06         Skipping container endpoint collection for pod "csi-rbdplugin-s8j9r" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:06         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-s8j9r" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:06         Skipping container endpoint collection for pod "csi-rbdplugin-s8j9r" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:06         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-s8j9r" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:06         Skipping container endpoint collection for pod "csi-rbdplugin-s8j9r" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:06         Gathering data for pod "csi-rbdplugin-wljmn"
# [must-gather-m2ggn] POD 2020/02/21 11:51:07         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-wljmn" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:07         Skipping container endpoint collection for pod "csi-rbdplugin-wljmn" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:07         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-wljmn" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:07         Skipping container endpoint collection for pod "csi-rbdplugin-wljmn" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:08         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-wljmn" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:08         Skipping container endpoint collection for pod "csi-rbdplugin-wljmn" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:08         Gathering data for pod "csi-rbdplugin-xvr92"
# [must-gather-m2ggn] POD 2020/02/21 11:51:08         Unable to gather previous container logs: previous terminated container "driver-registrar" in pod "csi-rbdplugin-xvr92" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:08         Skipping container endpoint collection for pod "csi-rbdplugin-xvr92" container "driver-registrar": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:08         Unable to gather previous container logs: previous terminated container "csi-rbdplugin" in pod "csi-rbdplugin-xvr92" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:08         Skipping container endpoint collection for pod "csi-rbdplugin-xvr92" container "csi-rbdplugin": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:09         Unable to gather previous container logs: previous terminated container "liveness-prometheus" in pod "csi-rbdplugin-xvr92" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:09         Skipping container endpoint collection for pod "csi-rbdplugin-xvr92" container "liveness-prometheus": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:09         Gathering data for pod "must-gather-m2ggn-helper"
# [must-gather-m2ggn] POD 2020/02/21 11:51:09         Skipping container data collection for pod "must-gather-m2ggn-helper": Pod not running
# [must-gather-m2ggn] POD 2020/02/21 11:51:09         Gathering data for pod "noobaa-core-0"
# [must-gather-m2ggn] POD 2020/02/21 11:51:09         Unable to gather previous container logs: previous terminated container "core" in pod "noobaa-core-0" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Unable to gather previous container logs: previous terminated container "db" in pod "noobaa-core-0" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Skipping container endpoint collection for pod "noobaa-core-0" container "db": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Unable to gather previous container logs: previous terminated container "init" in pod "noobaa-core-0" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Skipping container endpoint collection for pod "noobaa-core-0" container "init": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Gathering data for pod "noobaa-operator-7697b7b488-w7fqk"
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Unable to gather previous container logs: previous terminated container "noobaa-operator" in pod "noobaa-operator-7697b7b488-w7fqk" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Skipping container endpoint collection for pod "noobaa-operator-7697b7b488-w7fqk" container "noobaa-operator": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Gathering data for pod "ocs-operator-55b5dd4d79-j9z75"
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Unable to gather previous container logs: previous terminated container "ocs-operator" in pod "ocs-operator-55b5dd4d79-j9z75" not found
# [must-gather-m2ggn] POD E0221 11:51:11.608925     755 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod 1cdb5945906b6ff55c1309af4111bb83c9fdf514618d0264209ddda6a55db835, uid : exit status 1: 2020/02/21 11:51:11 socat[326252] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:11.645532     755 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod 1cdb5945906b6ff55c1309af4111bb83c9fdf514618d0264209ddda6a55db835, uid : exit status 1: 2020/02/21 11:51:11 socat[326259] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:11.674926     755 portforward.go:331] an error occurred forwarding 37587 -> 60000: error forwarding port 60000 to pod 1cdb5945906b6ff55c1309af4111bb83c9fdf514618d0264209ddda6a55db835, uid : exit status 1: 2020/02/21 11:51:11 socat[326266] E connect(5, AF=2 127.0.0.1:60000, 16): Connection refused
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Gathering data for pod "rook-ceph-drain-canary-394b22bae7457d3ad4d30d2cc3859f93-84mqgzx"
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Unable to gather previous container logs: previous terminated container "sleep" in pod "rook-ceph-drain-canary-394b22bae7457d3ad4d30d2cc3859f93-84mqgzx" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Skipping container endpoint collection for pod "rook-ceph-drain-canary-394b22bae7457d3ad4d30d2cc3859f93-84mqgzx" container "sleep": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:11         Gathering data for pod "rook-ceph-drain-canary-3b436e65e844960e668d0958f62a708b-59h4qqq"
# [must-gather-m2ggn] POD 2020/02/21 11:51:12         Unable to gather previous container logs: previous terminated container "sleep" in pod "rook-ceph-drain-canary-3b436e65e844960e668d0958f62a708b-59h4qqq" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:12         Skipping container endpoint collection for pod "rook-ceph-drain-canary-3b436e65e844960e668d0958f62a708b-59h4qqq" container "sleep": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:12         Gathering data for pod "rook-ceph-drain-canary-8e1c5b3e6d550f869e0df268323763a9-6bptpb4"
# [must-gather-m2ggn] POD 2020/02/21 11:51:12         Unable to gather previous container logs: previous terminated container "sleep" in pod "rook-ceph-drain-canary-8e1c5b3e6d550f869e0df268323763a9-6bptpb4" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:12         Skipping container endpoint collection for pod "rook-ceph-drain-canary-8e1c5b3e6d550f869e0df268323763a9-6bptpb4" container "sleep": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:12         Gathering data for pod "rook-ceph-drain-canary-b09e5f36ba368752334f340d47e24fee-96r2wmt"
# [must-gather-m2ggn] POD 2020/02/21 11:51:12         Unable to gather previous container logs: previous terminated container "sleep" in pod "rook-ceph-drain-canary-b09e5f36ba368752334f340d47e24fee-96r2wmt" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:12         Skipping container endpoint collection for pod "rook-ceph-drain-canary-b09e5f36ba368752334f340d47e24fee-96r2wmt" container "sleep": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:12         Gathering data for pod "rook-ceph-drain-canary-b3da251966d747eebc14ed058dcd1838-59j7bnq"
# [must-gather-m2ggn] POD 2020/02/21 11:51:13         Unable to gather previous container logs: previous terminated container "sleep" in pod "rook-ceph-drain-canary-b3da251966d747eebc14ed058dcd1838-59j7bnq" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:13         Skipping container endpoint collection for pod "rook-ceph-drain-canary-b3da251966d747eebc14ed058dcd1838-59j7bnq" container "sleep": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:13         Gathering data for pod "rook-ceph-drain-canary-fe039eeb15c3a68dbed7c98506964678-c5kdnr6"
# [must-gather-m2ggn] POD 2020/02/21 11:51:13         Unable to gather previous container logs: previous terminated container "sleep" in pod "rook-ceph-drain-canary-fe039eeb15c3a68dbed7c98506964678-c5kdnr6" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:13         Skipping container endpoint collection for pod "rook-ceph-drain-canary-fe039eeb15c3a68dbed7c98506964678-c5kdnr6" container "sleep": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:13         Gathering data for pod "rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg"
# [must-gather-m2ggn] POD 2020/02/21 11:51:14         Unable to gather previous container logs: previous terminated container "mds" in pod "rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:14         Skipping container endpoint collection for pod "rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg" container "mds": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:14         Gathering data for pod "rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk"
# [must-gather-m2ggn] POD 2020/02/21 11:51:14         Unable to gather previous container logs: previous terminated container "mds" in pod "rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:14         Skipping container endpoint collection for pod "rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk" container "mds": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:14         Gathering data for pod "rook-ceph-mgr-a-5bcdb86666-gxw5f"
# [must-gather-m2ggn] POD 2020/02/21 11:51:14         Unable to gather previous container logs: previous terminated container "mgr" in pod "rook-ceph-mgr-a-5bcdb86666-gxw5f" not found
# [must-gather-m2ggn] POD E0221 11:51:15.039126     755 portforward.go:331] an error occurred forwarding 37587 -> 6800: error forwarding port 6800 to pod 293f58b69bcc539fe68209e310da2e1c1b67d3d5264f46caa1581b178b7fd5f7, uid : exit status 1: 2020/02/21 11:51:15 socat[344382] E connect(5, AF=2 127.0.0.1:6800, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:15.076845     755 portforward.go:331] an error occurred forwarding 37587 -> 6800: error forwarding port 6800 to pod 293f58b69bcc539fe68209e310da2e1c1b67d3d5264f46caa1581b178b7fd5f7, uid : exit status 1: 2020/02/21 11:51:15 socat[344389] E connect(5, AF=2 127.0.0.1:6800, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:15.116871     755 portforward.go:331] an error occurred forwarding 37587 -> 6800: error forwarding port 6800 to pod 293f58b69bcc539fe68209e310da2e1c1b67d3d5264f46caa1581b178b7fd5f7, uid : exit status 1: 2020/02/21 11:51:15 socat[344397] E connect(5, AF=2 127.0.0.1:6800, 16): Connection refused
# [must-gather-m2ggn] POD 2020/02/21 11:51:15         Gathering data for pod "rook-ceph-mon-a-6c7d8bbc5b-fv6g5"
# [must-gather-m2ggn] POD 2020/02/21 11:51:15         Unable to gather previous container logs: previous terminated container "mon" in pod "rook-ceph-mon-a-6c7d8bbc5b-fv6g5" not found
# [must-gather-m2ggn] POD E0221 11:51:15.429608     755 portforward.go:331] an error occurred forwarding 37587 -> 6789: error forwarding port 6789 to pod 42ca62d5b889b5694aa8c3520aa9dc6483d043820d5f305b431731f81efa28d3, uid : exit status 1: 2020/02/21 11:51:15 socat[326393] E connect(5, AF=2 127.0.0.1:6789, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:15.458730     755 portforward.go:331] an error occurred forwarding 37587 -> 6789: error forwarding port 6789 to pod 42ca62d5b889b5694aa8c3520aa9dc6483d043820d5f305b431731f81efa28d3, uid : exit status 1: 2020/02/21 11:51:15 socat[326400] E connect(5, AF=2 127.0.0.1:6789, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:15.491770     755 portforward.go:331] an error occurred forwarding 37587 -> 6789: error forwarding port 6789 to pod 42ca62d5b889b5694aa8c3520aa9dc6483d043820d5f305b431731f81efa28d3, uid : exit status 1: 2020/02/21 11:51:15 socat[326407] E connect(5, AF=2 127.0.0.1:6789, 16): Connection refused
# [must-gather-m2ggn] POD 2020/02/21 11:51:15         Unable to gather previous container logs: previous terminated container "chown-container-data-dir" in pod "rook-ceph-mon-a-6c7d8bbc5b-fv6g5" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:15         Skipping container endpoint collection for pod "rook-ceph-mon-a-6c7d8bbc5b-fv6g5" container "chown-container-data-dir": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:16         Unable to gather previous container logs: previous terminated container "init-mon-fs" in pod "rook-ceph-mon-a-6c7d8bbc5b-fv6g5" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:16         Skipping container endpoint collection for pod "rook-ceph-mon-a-6c7d8bbc5b-fv6g5" container "init-mon-fs": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:16         Gathering data for pod "rook-ceph-mon-b-657d966977-ds9pb"
# [must-gather-m2ggn] POD 2020/02/21 11:51:16         Unable to gather previous container logs: previous terminated container "mon" in pod "rook-ceph-mon-b-657d966977-ds9pb" not found
# [must-gather-m2ggn] POD E0221 11:51:16.627334     755 portforward.go:331] an error occurred forwarding 37587 -> 6789: error forwarding port 6789 to pod 970eb100f8786062b81cc74add674f56a67b7c4f09057239bca935cb491f464b, uid : exit status 1: 2020/02/21 11:51:16 socat[344485] E connect(5, AF=2 127.0.0.1:6789, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:16.660122     755 portforward.go:331] an error occurred forwarding 37587 -> 6789: error forwarding port 6789 to pod 970eb100f8786062b81cc74add674f56a67b7c4f09057239bca935cb491f464b, uid : exit status 1: 2020/02/21 11:51:16 socat[344492] E connect(5, AF=2 127.0.0.1:6789, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:16.690978     755 portforward.go:331] an error occurred forwarding 37587 -> 6789: error forwarding port 6789 to pod 970eb100f8786062b81cc74add674f56a67b7c4f09057239bca935cb491f464b, uid : exit status 1: 2020/02/21 11:51:16 socat[344499] E connect(5, AF=2 127.0.0.1:6789, 16): Connection refused
# [must-gather-m2ggn] POD 2020/02/21 11:51:16         Unable to gather previous container logs: previous terminated container "chown-container-data-dir" in pod "rook-ceph-mon-b-657d966977-ds9pb" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:16         Skipping container endpoint collection for pod "rook-ceph-mon-b-657d966977-ds9pb" container "chown-container-data-dir": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:17         Unable to gather previous container logs: previous terminated container "init-mon-fs" in pod "rook-ceph-mon-b-657d966977-ds9pb" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:17         Skipping container endpoint collection for pod "rook-ceph-mon-b-657d966977-ds9pb" container "init-mon-fs": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:17         Gathering data for pod "rook-ceph-mon-c-58c5f47974-bdm2g"
# [must-gather-m2ggn] POD 2020/02/21 11:51:17         Unable to gather previous container logs: previous terminated container "mon" in pod "rook-ceph-mon-c-58c5f47974-bdm2g" not found
# [must-gather-m2ggn] POD E0221 11:51:17.832298     755 portforward.go:331] an error occurred forwarding 37587 -> 6789: error forwarding port 6789 to pod 7b83c43af3b5aed597061e67bdfd3f6f47ba357f269af541108af3ec026701b0, uid : exit status 1: 2020/02/21 11:51:17 socat[299511] E connect(5, AF=2 127.0.0.1:6789, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:17.867884     755 portforward.go:331] an error occurred forwarding 37587 -> 6789: error forwarding port 6789 to pod 7b83c43af3b5aed597061e67bdfd3f6f47ba357f269af541108af3ec026701b0, uid : exit status 1: 2020/02/21 11:51:17 socat[299518] E connect(5, AF=2 127.0.0.1:6789, 16): Connection refused
# [must-gather-m2ggn] POD E0221 11:51:17.904009     755 portforward.go:331] an error occurred forwarding 37587 -> 6789: error forwarding port 6789 to pod 7b83c43af3b5aed597061e67bdfd3f6f47ba357f269af541108af3ec026701b0, uid : exit status 1: 2020/02/21 11:51:17 socat[299525] E connect(5, AF=2 127.0.0.1:6789, 16): Connection refused
# [must-gather-m2ggn] POD 2020/02/21 11:51:18         Unable to gather previous container logs: previous terminated container "chown-container-data-dir" in pod "rook-ceph-mon-c-58c5f47974-bdm2g" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:18         Skipping container endpoint collection for pod "rook-ceph-mon-c-58c5f47974-bdm2g" container "chown-container-data-dir": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:18         Unable to gather previous container logs: previous terminated container "init-mon-fs" in pod "rook-ceph-mon-c-58c5f47974-bdm2g" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:18         Skipping container endpoint collection for pod "rook-ceph-mon-c-58c5f47974-bdm2g" container "init-mon-fs": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:18         Gathering data for pod "rook-ceph-operator-7c6c4fd77-k76jl"
# [must-gather-m2ggn] POD 2020/02/21 11:51:18         Unable to gather previous container logs: previous terminated container "rook-ceph-operator" in pod "rook-ceph-operator-7c6c4fd77-k76jl" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:18         Skipping container endpoint collection for pod "rook-ceph-operator-7c6c4fd77-k76jl" container "rook-ceph-operator": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:18         Gathering data for pod "rook-ceph-osd-0-54d86bc475-kbjdb"
# [must-gather-m2ggn] POD 2020/02/21 11:51:19         Unable to gather previous container logs: previous terminated container "osd" in pod "rook-ceph-osd-0-54d86bc475-kbjdb" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:19         Skipping container endpoint collection for pod "rook-ceph-osd-0-54d86bc475-kbjdb" container "osd": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:19         Unable to gather previous container logs: previous terminated container "config-init" in pod "rook-ceph-osd-0-54d86bc475-kbjdb" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:19         Skipping container endpoint collection for pod "rook-ceph-osd-0-54d86bc475-kbjdb" container "config-init": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:20         Unable to gather previous container logs: previous terminated container "copy-bins" in pod "rook-ceph-osd-0-54d86bc475-kbjdb" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:20         Skipping container endpoint collection for pod "rook-ceph-osd-0-54d86bc475-kbjdb" container "copy-bins": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:20         Unable to gather previous container logs: previous terminated container "blkdevmapper" in pod "rook-ceph-osd-0-54d86bc475-kbjdb" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:20         Skipping container endpoint collection for pod "rook-ceph-osd-0-54d86bc475-kbjdb" container "blkdevmapper": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:20         Gathering data for pod "rook-ceph-osd-1-5f56fcff97-cnbrg"
# [must-gather-m2ggn] POD 2020/02/21 11:51:20         Unable to gather previous container logs: previous terminated container "osd" in pod "rook-ceph-osd-1-5f56fcff97-cnbrg" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:20         Skipping container endpoint collection for pod "rook-ceph-osd-1-5f56fcff97-cnbrg" container "osd": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:21         Unable to gather previous container logs: previous terminated container "config-init" in pod "rook-ceph-osd-1-5f56fcff97-cnbrg" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:21         Skipping container endpoint collection for pod "rook-ceph-osd-1-5f56fcff97-cnbrg" container "config-init": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:21         Unable to gather previous container logs: previous terminated container "copy-bins" in pod "rook-ceph-osd-1-5f56fcff97-cnbrg" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:21         Skipping container endpoint collection for pod "rook-ceph-osd-1-5f56fcff97-cnbrg" container "copy-bins": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:22         Unable to gather previous container logs: previous terminated container "blkdevmapper" in pod "rook-ceph-osd-1-5f56fcff97-cnbrg" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:22         Skipping container endpoint collection for pod "rook-ceph-osd-1-5f56fcff97-cnbrg" container "blkdevmapper": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:22         Gathering data for pod "rook-ceph-osd-2-5d6b876dd7-hw29d"
# [must-gather-m2ggn] POD 2020/02/21 11:51:22         Unable to gather previous container logs: previous terminated container "osd" in pod "rook-ceph-osd-2-5d6b876dd7-hw29d" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:22         Skipping container endpoint collection for pod "rook-ceph-osd-2-5d6b876dd7-hw29d" container "osd": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:22         Unable to gather previous container logs: previous terminated container "config-init" in pod "rook-ceph-osd-2-5d6b876dd7-hw29d" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:22         Skipping container endpoint collection for pod "rook-ceph-osd-2-5d6b876dd7-hw29d" container "config-init": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:23         Unable to gather previous container logs: previous terminated container "copy-bins" in pod "rook-ceph-osd-2-5d6b876dd7-hw29d" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:23         Skipping container endpoint collection for pod "rook-ceph-osd-2-5d6b876dd7-hw29d" container "copy-bins": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:23         Unable to gather previous container logs: previous terminated container "blkdevmapper" in pod "rook-ceph-osd-2-5d6b876dd7-hw29d" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:23         Skipping container endpoint collection for pod "rook-ceph-osd-2-5d6b876dd7-hw29d" container "blkdevmapper": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:23         Gathering data for pod "rook-ceph-osd-3-67cb6f9499-zdsr7"
# [must-gather-m2ggn] POD 2020/02/21 11:51:24         Unable to gather previous container logs: previous terminated container "osd" in pod "rook-ceph-osd-3-67cb6f9499-zdsr7" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:24         Skipping container endpoint collection for pod "rook-ceph-osd-3-67cb6f9499-zdsr7" container "osd": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:24         Unable to gather previous container logs: previous terminated container "config-init" in pod "rook-ceph-osd-3-67cb6f9499-zdsr7" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:24         Skipping container endpoint collection for pod "rook-ceph-osd-3-67cb6f9499-zdsr7" container "config-init": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:24         Unable to gather previous container logs: previous terminated container "copy-bins" in pod "rook-ceph-osd-3-67cb6f9499-zdsr7" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:24         Skipping container endpoint collection for pod "rook-ceph-osd-3-67cb6f9499-zdsr7" container "copy-bins": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:25         Unable to gather previous container logs: previous terminated container "blkdevmapper" in pod "rook-ceph-osd-3-67cb6f9499-zdsr7" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:25         Skipping container endpoint collection for pod "rook-ceph-osd-3-67cb6f9499-zdsr7" container "blkdevmapper": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:25         Gathering data for pod "rook-ceph-osd-4-595ff56bb-tmmdx"
# [must-gather-m2ggn] POD 2020/02/21 11:51:25         Unable to gather previous container logs: previous terminated container "osd" in pod "rook-ceph-osd-4-595ff56bb-tmmdx" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:25         Skipping container endpoint collection for pod "rook-ceph-osd-4-595ff56bb-tmmdx" container "osd": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:26         Unable to gather previous container logs: previous terminated container "config-init" in pod "rook-ceph-osd-4-595ff56bb-tmmdx" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:26         Skipping container endpoint collection for pod "rook-ceph-osd-4-595ff56bb-tmmdx" container "config-init": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:26         Unable to gather previous container logs: previous terminated container "copy-bins" in pod "rook-ceph-osd-4-595ff56bb-tmmdx" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:26         Skipping container endpoint collection for pod "rook-ceph-osd-4-595ff56bb-tmmdx" container "copy-bins": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:26         Unable to gather previous container logs: previous terminated container "blkdevmapper" in pod "rook-ceph-osd-4-595ff56bb-tmmdx" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:26         Skipping container endpoint collection for pod "rook-ceph-osd-4-595ff56bb-tmmdx" container "blkdevmapper": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:26         Gathering data for pod "rook-ceph-osd-5-748b96b4d8-nzndk"
# [must-gather-m2ggn] POD 2020/02/21 11:51:27         Unable to gather previous container logs: previous terminated container "osd" in pod "rook-ceph-osd-5-748b96b4d8-nzndk" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:27         Skipping container endpoint collection for pod "rook-ceph-osd-5-748b96b4d8-nzndk" container "osd": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:27         Unable to gather previous container logs: previous terminated container "config-init" in pod "rook-ceph-osd-5-748b96b4d8-nzndk" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:27         Skipping container endpoint collection for pod "rook-ceph-osd-5-748b96b4d8-nzndk" container "config-init": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Unable to gather previous container logs: previous terminated container "copy-bins" in pod "rook-ceph-osd-5-748b96b4d8-nzndk" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Skipping container endpoint collection for pod "rook-ceph-osd-5-748b96b4d8-nzndk" container "copy-bins": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Unable to gather previous container logs: previous terminated container "blkdevmapper" in pod "rook-ceph-osd-5-748b96b4d8-nzndk" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Skipping container endpoint collection for pod "rook-ceph-osd-5-748b96b4d8-nzndk" container "blkdevmapper": No ports
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Gathering data for pod "rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8"
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Skipping container data collection for pod "rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8": Pod not running
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Gathering data for pod "rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb"
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Skipping container data collection for pod "rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb": Pod not running
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Gathering data for pod "rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g"
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Skipping container data collection for pod "rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g": Pod not running
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Gathering data for pod "rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n"
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Skipping container data collection for pod "rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n": Pod not running
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Gathering data for pod "rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl"
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Skipping container data collection for pod "rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl": Pod not running
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Gathering data for pod "rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m"
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Skipping container data collection for pod "rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m": Pod not running
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Gathering data for pod "rook-ceph-tools-859dfcff6f-f98sc"
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Unable to gather previous container logs: previous terminated container "rook-ceph-tools" in pod "rook-ceph-tools-859dfcff6f-f98sc" not found
# [must-gather-m2ggn] POD 2020/02/21 11:51:28         Skipping container endpoint collection for pod "rook-ceph-tools-859dfcff6f-f98sc" container "rook-ceph-tools": No ports
# [must-gather-m2ggn] POD Error: one or more errors ocurred while gathering pod-specific data for namespace: openshift-storage
# [must-gather-m2ggn] POD 
# [must-gather-m2ggn] POD     [one or more errors ocurred while gathering container data for pod noobaa-core-0:
# [must-gather-m2ggn] POD 
# [must-gather-m2ggn] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod ocs-operator-55b5dd4d79-j9z75:
# [must-gather-m2ggn] POD 
# [must-gather-m2ggn] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod rook-ceph-mgr-a-5bcdb86666-gxw5f:
# [must-gather-m2ggn] POD 
# [must-gather-m2ggn] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod rook-ceph-mon-a-6c7d8bbc5b-fv6g5:
# [must-gather-m2ggn] POD 
# [must-gather-m2ggn] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod rook-ceph-mon-b-657d966977-ds9pb:
# [must-gather-m2ggn] POD 
# [must-gather-m2ggn] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF], one or more errors ocurred while gathering container data for pod rook-ceph-mon-c-58c5f47974-bdm2g:
# [must-gather-m2ggn] POD 
# [must-gather-m2ggn] POD     [unable to gather container /healthz: Get https://localhost:37587/: EOF, unable to gather container /version: Get https://localhost:37587/: EOF, unable to gather container /metrics: Get https://localhost:37587/metrics: EOF]]
# [must-gather-m2ggn] POD Collecting dump of clusterresourceversion
# [must-gather-m2ggn] POD 2020/02/21 11:51:30 Finished successfully with no errors.
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "copy-bins" in pod "rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "blkdevmapper" in pod "rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "provision" in pod "rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "copy-bins" in pod "rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "blkdevmapper" in pod "rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "provision" in pod "rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "copy-bins" in pod "rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "blkdevmapper" in pod "rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "provision" in pod "rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "copy-bins" in pod "rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "blkdevmapper" in pod "rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "provision" in pod "rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "copy-bins" in pod "rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "blkdevmapper" in pod "rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "provision" in pod "rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "copy-bins" in pod "rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "blkdevmapper" in pod "rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m" not found
# [must-gather-m2ggn] POD Error from server (BadRequest): previous terminated container "provision" in pod "rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m" not found
# [must-gather-m2ggn] POD Helper pod got deployed successfully.
# [must-gather-m2ggn] POD collecting command output for: ceph status
# [must-gather-m2ggn] POD collecting command output for: ceph health detail
# [must-gather-m2ggn] POD collecting command output for: ceph osd tree
# [must-gather-m2ggn] POD collecting command output for: ceph osd stat
# [must-gather-m2ggn] POD collecting command output for: ceph osd dump
# [must-gather-m2ggn] POD collecting command output for: ceph mon stat
# [must-gather-m2ggn] POD collecting command output for: ceph mon dump
# [must-gather-m2ggn] POD collecting command output for: ceph df
# [must-gather-m2ggn] POD collecting command output for: ceph report
# [must-gather-m2ggn] POD collecting command output for: ceph osd df tree
# [must-gather-m2ggn] POD collecting command output for: ceph fs ls
# [must-gather-m2ggn] POD collecting command output for: ceph pg dump
# [must-gather-m2ggn] POD collecting command output for: ceph osd crush show-tunables
# [must-gather-m2ggn] POD collecting command output for: ceph osd crush dump
# [must-gather-m2ggn] POD collecting command output for: ceph mgr dump
# [must-gather-m2ggn] POD collecting command output for: ceph mds stat
# [must-gather-m2ggn] POD collecting command output for: ceph versions
# [must-gather-m2ggn] POD collecting command output for: ceph fs dump
# [must-gather-m2ggn] POD collecting command output for: ceph auth list
# [must-gather-m2ggn] POD collecting command output for: ceph-volume lvm list
# [must-gather-m2ggn] POD collecting prepare volume logs from node ip-10-0-131-190.us-east-2.compute.internal 
# [must-gather-m2ggn] POD collecting prepare volume logs from node ip-10-0-143-103.us-east-2.compute.internal 
# [must-gather-m2ggn] POD collecting prepare volume logs from node ip-10-0-147-203.us-east-2.compute.internal 
# [must-gather-m2ggn] POD collecting prepare volume logs from node ip-10-0-153-85.us-east-2.compute.internal 
# [must-gather-m2ggn] POD collecting prepare volume logs from node ip-10-0-163-36.us-east-2.compute.internal 
# [must-gather-m2ggn] POD collecting prepare volume logs from node ip-10-0-174-204.us-east-2.compute.internal 
# [must-gather-m2ggn] POD pod "must-gather-m2ggn-helper" deleted
# [must-gather-m2ggn] OUT waiting for gather to complete
# [must-gather-m2ggn] OUT downloading gather output
# [must-gather-m2ggn] OUT receiving incremental file list
# [must-gather-m2ggn] OUT ./
# [must-gather-m2ggn] OUT ceph/
# [must-gather-m2ggn] OUT ceph/namespaces/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/openshift-storage.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/apps.openshift.io/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/apps.openshift.io/deploymentconfigs.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/apps/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/apps/daemonsets.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/apps/deployments.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/apps/replicasets.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/apps/statefulsets.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/autoscaling/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/autoscaling/horizontalpodautoscalers.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/batch/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/batch/cronjobs.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/batch/jobs.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/build.openshift.io/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/build.openshift.io/buildconfigs.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/build.openshift.io/builds.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/ceph.rook.io/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/ceph.rook.io/cephblockpools/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/ceph.rook.io/cephblockpools/ocs-storagecluster-cephblockpool.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/ceph.rook.io/cephclusters/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/ceph.rook.io/cephclusters/ocs-storagecluster-cephcluster.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/ceph.rook.io/cephfilesystems/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/ceph.rook.io/cephfilesystems/ocs-storagecluster-cephfilesystem.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/core/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/core/configmaps.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/core/events.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/core/persistentvolumeclaims.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/core/pods.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/core/replicationcontrollers.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/core/secrets.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/core/services.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/image.openshift.io/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/image.openshift.io/imagestreams.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph-volume_lvm_list
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_auth_list
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_df
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_fs_dump
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_fs_ls
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_health_detail
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_mds_stat
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_mgr_dump
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_mon_dump
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_mon_stat
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_osd_crush_dump
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_osd_crush_show-tunables
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_osd_df_tree
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_osd_dump
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_osd_stat
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_osd_tree
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_pg_dump
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_report
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_status
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/ceph_versions
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_auth_list_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_df_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_fs_dump_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_fs_ls_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_health_detail_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_mds_stat_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_mgr_dump_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_mon_dump_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_mon_stat_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_osd_crush_dump_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_osd_crush_show-tunables_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_osd_df_tree_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_osd_dump_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_osd_stat_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_osd_tree_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_pg_dump_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_report_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_status_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/must_gather_commands/json_output/ceph_versions_--format_json-pretty
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/operators.coreos.com/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/operators.coreos.com/clusterserviceversions/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/operators.coreos.com/clusterserviceversions/ocs-operator.v4.2.1.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-131-190.us-east-2.compute.internal/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-131-190.us-east-2.compute.internal/ceph-volume.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-143-103.us-east-2.compute.internal/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-143-103.us-east-2.compute.internal/ceph-volume.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-147-203.us-east-2.compute.internal/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-147-203.us-east-2.compute.internal/ceph-volume.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-153-85.us-east-2.compute.internal/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-153-85.us-east-2.compute.internal/ceph-volume.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-163-36.us-east-2.compute.internal/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-163-36.us-east-2.compute.internal/ceph-volume.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-174-204.us-east-2.compute.internal/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/osd_prepare_volume_logs/ip-10-0-174-204.us-east-2.compute.internal/ceph-volume.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/csi-cephfsplugin-b8pfk.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-b8pfk/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/csi-cephfsplugin-c9k9s.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-c9k9s/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/csi-cephfsplugin-f9hcs.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-f9hcs/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/csi-cephfsplugin-j4fkc.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-j4fkc/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/csi-cephfsplugin-kblgw.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kblgw/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/csi-cephfsplugin-kv2nw.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-kv2nw/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/csi-cephfsplugin-lbjj6.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-lbjj6/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-cephfsplugin-provisioner-647cd6996c-jdsf4.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-attacher/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-attacher/csi-attacher/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-attacher/csi-attacher/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-attacher/csi-attacher/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-provisioner/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-provisioner/csi-provisioner/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-provisioner/csi-provisioner/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/csi-provisioner/csi-provisioner/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-jdsf4/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-cephfsplugin-provisioner-647cd6996c-q45l9.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-attacher/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-attacher/csi-attacher/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-attacher/csi-attacher/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-attacher/csi-attacher/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-provisioner/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-provisioner/csi-provisioner/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-provisioner/csi-provisioner/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/csi-provisioner/csi-provisioner/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-provisioner-647cd6996c-q45l9/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/csi-cephfsplugin-q52sm.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-q52sm/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/csi-cephfsplugin-rkh44.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-rkh44/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/csi-cephfsplugin-thzlr.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-thzlr/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/csi-cephfsplugin-xbb8b.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xbb8b/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/csi-cephfsplugin-xwmrc.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/csi-cephfsplugin/csi-cephfsplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/csi-cephfsplugin/csi-cephfsplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/csi-cephfsplugin/csi-cephfsplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-cephfsplugin-xwmrc/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/csi-rbdplugin-2f7zj.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-2f7zj/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/csi-rbdplugin-6tr5t.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-6tr5t/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/csi-rbdplugin-9tgfg.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-9tgfg/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/csi-rbdplugin-b8sbd.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-b8sbd/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/csi-rbdplugin-cxrwp.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-cxrwp/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/csi-rbdplugin-fh6xz.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-fh6xz/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/csi-rbdplugin-grd57.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-grd57/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/csi-rbdplugin-m29tc.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-m29tc/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-provisioner/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-provisioner/csi-provisioner/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-provisioner/csi-provisioner/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-provisioner/csi-provisioner/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-rbdplugin-attacher/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-rbdplugin-attacher/csi-rbdplugin-attacher/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-rbdplugin-attacher/csi-rbdplugin-attacher/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-rbdplugin-attacher/csi-rbdplugin-attacher/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-4xjql/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-provisioner/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-provisioner/csi-provisioner/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-provisioner/csi-provisioner/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-provisioner/csi-provisioner/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-rbdplugin-attacher/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-rbdplugin-attacher/csi-rbdplugin-attacher/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-rbdplugin-attacher/csi-rbdplugin-attacher/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-rbdplugin-attacher/csi-rbdplugin-attacher/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-provisioner-6b8ff67dc4-5z97f/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/csi-rbdplugin-qwx8d.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-qwx8d/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/csi-rbdplugin-s8j9r.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-s8j9r/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/csi-rbdplugin-wljmn.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-wljmn/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/csi-rbdplugin-xvr92.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/csi-rbdplugin/csi-rbdplugin/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/csi-rbdplugin/csi-rbdplugin/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/csi-rbdplugin/csi-rbdplugin/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/driver-registrar/driver-registrar/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/driver-registrar/driver-registrar/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/driver-registrar/driver-registrar/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/liveness-prometheus/liveness-prometheus/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/liveness-prometheus/liveness-prometheus/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/csi-rbdplugin-xvr92/liveness-prometheus/liveness-prometheus/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/noobaa-core-0.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/core/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/core/core/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/core/core/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/core/core/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/db/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/db/db/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/db/db/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/db/db/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/init/init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/init/init/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-core-0/init/init/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-operator-7697b7b488-w7fqk/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-operator-7697b7b488-w7fqk/noobaa-operator-7697b7b488-w7fqk.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-operator-7697b7b488-w7fqk/noobaa-operator/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-operator-7697b7b488-w7fqk/noobaa-operator/noobaa-operator/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-operator-7697b7b488-w7fqk/noobaa-operator/noobaa-operator/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/noobaa-operator-7697b7b488-w7fqk/noobaa-operator/noobaa-operator/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/ocs-operator-55b5dd4d79-j9z75/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/ocs-operator-55b5dd4d79-j9z75/ocs-operator-55b5dd4d79-j9z75.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/ocs-operator-55b5dd4d79-j9z75/ocs-operator/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/ocs-operator-55b5dd4d79-j9z75/ocs-operator/ocs-operator/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/ocs-operator-55b5dd4d79-j9z75/ocs-operator/ocs-operator/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/ocs-operator-55b5dd4d79-j9z75/ocs-operator/ocs-operator/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-394b22bae7457d3ad4d30d2cc3859f93-84mqgzx/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-394b22bae7457d3ad4d30d2cc3859f93-84mqgzx/rook-ceph-drain-canary-394b22bae7457d3ad4d30d2cc3859f93-84mqgzx.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-3b436e65e844960e668d0958f62a708b-59h4qqq/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-3b436e65e844960e668d0958f62a708b-59h4qqq/rook-ceph-drain-canary-3b436e65e844960e668d0958f62a708b-59h4qqq.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-8e1c5b3e6d550f869e0df268323763a9-6bptpb4/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-8e1c5b3e6d550f869e0df268323763a9-6bptpb4/rook-ceph-drain-canary-8e1c5b3e6d550f869e0df268323763a9-6bptpb4.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-b09e5f36ba368752334f340d47e24fee-96r2wmt/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-b09e5f36ba368752334f340d47e24fee-96r2wmt/rook-ceph-drain-canary-b09e5f36ba368752334f340d47e24fee-96r2wmt.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-b3da251966d747eebc14ed058dcd1838-59j7bnq/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-b3da251966d747eebc14ed058dcd1838-59j7bnq/rook-ceph-drain-canary-b3da251966d747eebc14ed058dcd1838-59j7bnq.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-fe039eeb15c3a68dbed7c98506964678-c5kdnr6/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-drain-canary-fe039eeb15c3a68dbed7c98506964678-c5kdnr6/rook-ceph-drain-canary-fe039eeb15c3a68dbed7c98506964678-c5kdnr6.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg/mds/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg/mds/mds/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg/mds/mds/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg/mds/mds/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk/mds/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk/mds/mds/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk/mds/mds/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk/mds/mds/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mgr-a-5bcdb86666-gxw5f/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mgr-a-5bcdb86666-gxw5f/rook-ceph-mgr-a-5bcdb86666-gxw5f.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mgr-a-5bcdb86666-gxw5f/mgr/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mgr-a-5bcdb86666-gxw5f/mgr/mgr/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mgr-a-5bcdb86666-gxw5f/mgr/mgr/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mgr-a-5bcdb86666-gxw5f/mgr/mgr/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/rook-ceph-mon-a-6c7d8bbc5b-fv6g5.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/chown-container-data-dir/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/chown-container-data-dir/chown-container-data-dir/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/chown-container-data-dir/chown-container-data-dir/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/chown-container-data-dir/chown-container-data-dir/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/init-mon-fs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/init-mon-fs/init-mon-fs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/init-mon-fs/init-mon-fs/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/init-mon-fs/init-mon-fs/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/mon/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/mon/mon/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/mon/mon/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-a-6c7d8bbc5b-fv6g5/mon/mon/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/rook-ceph-mon-b-657d966977-ds9pb.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/chown-container-data-dir/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/chown-container-data-dir/chown-container-data-dir/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/chown-container-data-dir/chown-container-data-dir/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/chown-container-data-dir/chown-container-data-dir/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/init-mon-fs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/init-mon-fs/init-mon-fs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/init-mon-fs/init-mon-fs/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/init-mon-fs/init-mon-fs/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/mon/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/mon/mon/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/mon/mon/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-b-657d966977-ds9pb/mon/mon/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/rook-ceph-mon-c-58c5f47974-bdm2g.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/chown-container-data-dir/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/chown-container-data-dir/chown-container-data-dir/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/chown-container-data-dir/chown-container-data-dir/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/chown-container-data-dir/chown-container-data-dir/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/init-mon-fs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/init-mon-fs/init-mon-fs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/init-mon-fs/init-mon-fs/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/init-mon-fs/init-mon-fs/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/mon/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/mon/mon/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/mon/mon/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-mon-c-58c5f47974-bdm2g/mon/mon/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-operator-7c6c4fd77-k76jl/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-operator-7c6c4fd77-k76jl/rook-ceph-operator-7c6c4fd77-k76jl.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-operator-7c6c4fd77-k76jl/rook-ceph-operator/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-operator-7c6c4fd77-k76jl/rook-ceph-operator/rook-ceph-operator/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-operator-7c6c4fd77-k76jl/rook-ceph-operator/rook-ceph-operator/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-operator-7c6c4fd77-k76jl/rook-ceph-operator/rook-ceph-operator/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/rook-ceph-osd-0-54d86bc475-kbjdb.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/config-init/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/config-init/config-init/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/config-init/config-init/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/osd/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/osd/osd/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-0-54d86bc475-kbjdb/osd/osd/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/rook-ceph-osd-1-5f56fcff97-cnbrg.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/config-init/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/config-init/config-init/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/config-init/config-init/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/osd/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/osd/osd/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-1-5f56fcff97-cnbrg/osd/osd/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/rook-ceph-osd-2-5d6b876dd7-hw29d.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/config-init/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/config-init/config-init/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/config-init/config-init/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/osd/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/osd/osd/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-2-5d6b876dd7-hw29d/osd/osd/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/rook-ceph-osd-3-67cb6f9499-zdsr7.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/config-init/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/config-init/config-init/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/config-init/config-init/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/osd/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/osd/osd/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-3-67cb6f9499-zdsr7/osd/osd/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/rook-ceph-osd-4-595ff56bb-tmmdx.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/config-init/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/config-init/config-init/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/config-init/config-init/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/osd/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/osd/osd/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-4-595ff56bb-tmmdx/osd/osd/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/rook-ceph-osd-5-748b96b4d8-nzndk.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/config-init/config-init/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/config-init/config-init/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/config-init/config-init/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/osd/osd/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/osd/osd/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-5-748b96b4d8-nzndk/osd/osd/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/provision/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/provision/provision/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8/provision/provision/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/provision/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/provision/provision/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb/provision/provision/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/provision/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/provision/provision/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g/provision/provision/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/provision/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/provision/provision/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n/provision/provision/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/provision/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/provision/provision/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl/provision/provision/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/copy-bins/copy-bins/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/copy-bins/copy-bins/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/copy-bins/copy-bins/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/provision/provision/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/provision/provision/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m/provision/provision/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-tools-859dfcff6f-f98sc/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-tools-859dfcff6f-f98sc/rook-ceph-tools-859dfcff6f-f98sc.yaml
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-tools-859dfcff6f-f98sc/rook-ceph-tools/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-tools-859dfcff6f-f98sc/rook-ceph-tools/rook-ceph-tools/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-tools-859dfcff6f-f98sc/rook-ceph-tools/rook-ceph-tools/logs/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/pods/rook-ceph-tools-859dfcff6f-f98sc/rook-ceph-tools/rook-ceph-tools/logs/current.log
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/route.openshift.io/
# [must-gather-m2ggn] OUT ceph/namespaces/openshift-storage/route.openshift.io/routes.yaml
# [must-gather-m2ggn] OUT cluster-scoped-resources/
# [must-gather-m2ggn] OUT cluster-scoped-resources/objectbucket.io/
# [must-gather-m2ggn] OUT cluster-scoped-resources/objectbucket.io/objectbuckets/
# [must-gather-m2ggn] OUT cluster-scoped-resources/objectbucket.io/objectbuckets/obc-obc-test-obc-test.yaml
# [must-gather-m2ggn] OUT cluster-scoped-resources/objectbucket.io/objectbuckets/obc-openshift-storage-test21obc.yaml
# [must-gather-m2ggn] OUT namespaces/
# [must-gather-m2ggn] OUT namespaces/obc-test/
# [must-gather-m2ggn] OUT namespaces/obc-test/objectbucket.io/
# [must-gather-m2ggn] OUT namespaces/obc-test/objectbucket.io/objectbucketclaims/
# [must-gather-m2ggn] OUT namespaces/obc-test/objectbucket.io/objectbucketclaims/obc-test.yaml
# [must-gather-m2ggn] OUT namespaces/openshift-storage/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/noobaa-core-0/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/noobaa-core-0/noobaa-core-0-previous.log
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/noobaa-core-0/noobaa-core-0.log
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/noobaa-operator-7697b7b488-w7fqk/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/noobaa-operator-7697b7b488-w7fqk/noobaa-operator-7697b7b488-w7fqk-previous.log
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/noobaa-operator-7697b7b488-w7fqk/noobaa-operator-7697b7b488-w7fqk.log
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/ocs-operator-55b5dd4d79-j9z75/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/ocs-operator-55b5dd4d79-j9z75/ocs-operator-55b5dd4d79-j9z75-previous.log
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/ocs-operator-55b5dd4d79-j9z75/ocs-operator-55b5dd4d79-j9z75.log
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/rook-ceph-operator-7c6c4fd77-k76jl/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/rook-ceph-operator-7c6c4fd77-k76jl/rook-ceph-operator-7c6c4fd77-k76jl-previous.log
# [must-gather-m2ggn] OUT namespaces/openshift-storage/logs/rook-ceph-operator-7c6c4fd77-k76jl/rook-ceph-operator-7c6c4fd77-k76jl.log
# [must-gather-m2ggn] OUT namespaces/openshift-storage/objectbucket.io/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/objectbucket.io/objectbucketclaims/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/objectbucket.io/objectbucketclaims/test21obc.yaml
# [must-gather-m2ggn] OUT namespaces/openshift-storage/ocs.openshift.io/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/ocs.openshift.io/storageclusters/
# [must-gather-m2ggn] OUT namespaces/openshift-storage/ocs.openshift.io/storageclusters/ocs-storagecluster.yaml
# [must-gather-m2ggn] OUT noobaa/
# [must-gather-m2ggn] OUT noobaa/logs/
# [must-gather-m2ggn] OUT noobaa/logs/openshift-storage/
# [must-gather-m2ggn] OUT noobaa/logs/openshift-storage/noobaa-core-0.log
# [must-gather-m2ggn] OUT noobaa/logs/openshift-storage/noobaa-operator-7697b7b488-w7fqk.log
# [must-gather-m2ggn] OUT noobaa/namespaces/
# [must-gather-m2ggn] OUT noobaa/namespaces/openshift-storage/
# [must-gather-m2ggn] OUT noobaa/namespaces/openshift-storage/noobaa.io/
# [must-gather-m2ggn] OUT noobaa/namespaces/openshift-storage/noobaa.io/backingstores/
# [must-gather-m2ggn] OUT noobaa/namespaces/openshift-storage/noobaa.io/backingstores/noobaa-default-backing-store.yaml
# [must-gather-m2ggn] OUT noobaa/namespaces/openshift-storage/noobaa.io/bucketclasses/
# [must-gather-m2ggn] OUT noobaa/namespaces/openshift-storage/noobaa.io/bucketclasses/noobaa-default-bucket-class.yaml
# [must-gather-m2ggn] OUT noobaa/namespaces/openshift-storage/noobaa.io/noobaas/
# [must-gather-m2ggn] OUT noobaa/namespaces/openshift-storage/noobaa.io/noobaas/noobaa.yaml
# [must-gather-m2ggn] OUT 
# [must-gather-m2ggn] OUT sent 7,622 bytes  received 52,728,251 bytes  35,157,248.67 bytes/sec
# [must-gather-m2ggn] OUT total size is 52,671,645  speedup is 1.00
# [must-gather      ] OUT clusterrolebinding.rbac.authorization.k8s.io/must-gather-2v5sh deleted
# [must-gather      ] OUT namespace/openshift-must-gather-8gkg5 deleted
#+end_example

#+begin_src bash
tree must-gather.local.5716477790504308840
#+end_src

#+begin_example
# must-gather.local.5716477790504308840
# └── quay-io-rhceph-dev-ocs-must-gather-sha256-da1a76a963e73da276c04060bd3d222b14b0dbff642aec0dd2d939d455c53565
#     ├── ceph
#     │   └── namespaces
#     │       └── openshift-storage
#     │           ├── apps
#     │           │   ├── daemonsets.yaml
#     │           │   ├── deployments.yaml
#     │           │   ├── replicasets.yaml
#     │           │   └── statefulsets.yaml
#     │           ├── apps.openshift.io
#     │           │   └── deploymentconfigs.yaml
#     │           ├── autoscaling
#     │           │   └── horizontalpodautoscalers.yaml
#     │           ├── batch
#     │           │   ├── cronjobs.yaml
#     │           │   └── jobs.yaml
#     │           ├── build.openshift.io
#     │           │   ├── buildconfigs.yaml
#     │           │   └── builds.yaml
#     │           ├── ceph.rook.io
#     │           │   ├── cephblockpools
#     │           │   │   └── ocs-storagecluster-cephblockpool.yaml
#     │           │   ├── cephclusters
#     │           │   │   └── ocs-storagecluster-cephcluster.yaml
#     │           │   └── cephfilesystems
#     │           │       └── ocs-storagecluster-cephfilesystem.yaml
#     │           ├── core
#     │           │   ├── configmaps.yaml
#     │           │   ├── events.yaml
#     │           │   ├── persistentvolumeclaims.yaml
#     │           │   ├── pods.yaml
#     │           │   ├── replicationcontrollers.yaml
#     │           │   ├── secrets.yaml
#     │           │   └── services.yaml
#     │           ├── image.openshift.io
#     │           │   └── imagestreams.yaml
#     │           ├── must_gather_commands
#     │           │   ├── ceph_auth_list
#     │           │   ├── ceph_df
#     │           │   ├── ceph_fs_dump
#     │           │   ├── ceph_fs_ls
#     │           │   ├── ceph_health_detail
#     │           │   ├── ceph_mds_stat
#     │           │   ├── ceph_mgr_dump
#     │           │   ├── ceph_mon_dump
#     │           │   ├── ceph_mon_stat
#     │           │   ├── ceph_osd_crush_dump
#     │           │   ├── ceph_osd_crush_show-tunables
#     │           │   ├── ceph_osd_df_tree
#     │           │   ├── ceph_osd_dump
#     │           │   ├── ceph_osd_stat
#     │           │   ├── ceph_osd_tree
#     │           │   ├── ceph_pg_dump
#     │           │   ├── ceph_report
#     │           │   ├── ceph_status
#     │           │   ├── ceph_versions
#     │           │   ├── ceph-volume_lvm_list
#     │           │   └── json_output
#     │           │       ├── ceph_auth_list_--format_json-pretty
#     │           │       ├── ceph_df_--format_json-pretty
#     │           │       ├── ceph_fs_dump_--format_json-pretty
#     │           │       ├── ceph_fs_ls_--format_json-pretty
#     │           │       ├── ceph_health_detail_--format_json-pretty
#     │           │       ├── ceph_mds_stat_--format_json-pretty
#     │           │       ├── ceph_mgr_dump_--format_json-pretty
#     │           │       ├── ceph_mon_dump_--format_json-pretty
#     │           │       ├── ceph_mon_stat_--format_json-pretty
#     │           │       ├── ceph_osd_crush_dump_--format_json-pretty
#     │           │       ├── ceph_osd_crush_show-tunables_--format_json-pretty
#     │           │       ├── ceph_osd_df_tree_--format_json-pretty
#     │           │       ├── ceph_osd_dump_--format_json-pretty
#     │           │       ├── ceph_osd_stat_--format_json-pretty
#     │           │       ├── ceph_osd_tree_--format_json-pretty
#     │           │       ├── ceph_pg_dump_--format_json-pretty
#     │           │       ├── ceph_report_--format_json-pretty
#     │           │       ├── ceph_status_--format_json-pretty
#     │           │       └── ceph_versions_--format_json-pretty
#     │           ├── openshift-storage.yaml
#     │           ├── operators.coreos.com
#     │           │   └── clusterserviceversions
#     │           │       └── ocs-operator.v4.2.1.yaml
#     │           ├── osd_prepare_volume_logs
#     │           │   ├── ip-10-0-131-190.us-east-2.compute.internal
#     │           │   │   └── ceph-volume.log
#     │           │   ├── ip-10-0-143-103.us-east-2.compute.internal
#     │           │   │   └── ceph-volume.log
#     │           │   ├── ip-10-0-147-203.us-east-2.compute.internal
#     │           │   │   └── ceph-volume.log
#     │           │   ├── ip-10-0-153-85.us-east-2.compute.internal
#     │           │   │   └── ceph-volume.log
#     │           │   ├── ip-10-0-163-36.us-east-2.compute.internal
#     │           │   │   └── ceph-volume.log
#     │           │   └── ip-10-0-174-204.us-east-2.compute.internal
#     │           │       └── ceph-volume.log
#     │           ├── pods
#     │           │   ├── csi-cephfsplugin-b8pfk
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-b8pfk.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-c9k9s
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-c9k9s.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-f9hcs
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-f9hcs.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-j4fkc
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-j4fkc.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-kblgw
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-kblgw.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-kv2nw
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-kv2nw.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-lbjj6
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-lbjj6.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-provisioner-647cd6996c-jdsf4
#     │           │   │   ├── csi-attacher
#     │           │   │   │   └── csi-attacher
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-provisioner-647cd6996c-jdsf4.yaml
#     │           │   │   ├── csi-provisioner
#     │           │   │   │   └── csi-provisioner
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-provisioner-647cd6996c-q45l9
#     │           │   │   ├── csi-attacher
#     │           │   │   │   └── csi-attacher
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-provisioner-647cd6996c-q45l9.yaml
#     │           │   │   ├── csi-provisioner
#     │           │   │   │   └── csi-provisioner
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-q52sm
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-q52sm.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-rkh44
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-rkh44.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-thzlr
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-thzlr.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-xbb8b
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-xbb8b.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-cephfsplugin-xwmrc
#     │           │   │   ├── csi-cephfsplugin
#     │           │   │   │   └── csi-cephfsplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-cephfsplugin-xwmrc.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-2f7zj
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-2f7zj.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-6tr5t
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-6tr5t.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-9tgfg
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-9tgfg.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-b8sbd
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-b8sbd.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-cxrwp
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-cxrwp.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-fh6xz
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-fh6xz.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-grd57
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-grd57.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-m29tc
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-m29tc.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-provisioner-6b8ff67dc4-4xjql
#     │           │   │   ├── csi-provisioner
#     │           │   │   │   └── csi-provisioner
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-attacher
#     │           │   │   │   └── csi-rbdplugin-attacher
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-provisioner-6b8ff67dc4-4xjql.yaml
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-provisioner-6b8ff67dc4-5z97f
#     │           │   │   ├── csi-provisioner
#     │           │   │   │   └── csi-provisioner
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-attacher
#     │           │   │   │   └── csi-rbdplugin-attacher
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-provisioner-6b8ff67dc4-5z97f.yaml
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-qwx8d
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-qwx8d.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-s8j9r
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-s8j9r.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-wljmn
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-wljmn.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── csi-rbdplugin-xvr92
#     │           │   │   ├── csi-rbdplugin
#     │           │   │   │   └── csi-rbdplugin
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── csi-rbdplugin-xvr92.yaml
#     │           │   │   ├── driver-registrar
#     │           │   │   │   └── driver-registrar
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── liveness-prometheus
#     │           │   │       └── liveness-prometheus
#     │           │   │           └── logs
#     │           │   │               └── current.log
#     │           │   ├── noobaa-core-0
#     │           │   │   ├── core
#     │           │   │   │   └── core
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── db
#     │           │   │   │   └── db
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── init
#     │           │   │   │   └── init
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── noobaa-core-0.yaml
#     │           │   ├── noobaa-operator-7697b7b488-w7fqk
#     │           │   │   ├── noobaa-operator
#     │           │   │   │   └── noobaa-operator
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── noobaa-operator-7697b7b488-w7fqk.yaml
#     │           │   ├── ocs-operator-55b5dd4d79-j9z75
#     │           │   │   ├── ocs-operator
#     │           │   │   │   └── ocs-operator
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── ocs-operator-55b5dd4d79-j9z75.yaml
#     │           │   ├── rook-ceph-drain-canary-394b22bae7457d3ad4d30d2cc3859f93-84mqgzx
#     │           │   │   └── rook-ceph-drain-canary-394b22bae7457d3ad4d30d2cc3859f93-84mqgzx.yaml
#     │           │   ├── rook-ceph-drain-canary-3b436e65e844960e668d0958f62a708b-59h4qqq
#     │           │   │   └── rook-ceph-drain-canary-3b436e65e844960e668d0958f62a708b-59h4qqq.yaml
#     │           │   ├── rook-ceph-drain-canary-8e1c5b3e6d550f869e0df268323763a9-6bptpb4
#     │           │   │   └── rook-ceph-drain-canary-8e1c5b3e6d550f869e0df268323763a9-6bptpb4.yaml
#     │           │   ├── rook-ceph-drain-canary-b09e5f36ba368752334f340d47e24fee-96r2wmt
#     │           │   │   └── rook-ceph-drain-canary-b09e5f36ba368752334f340d47e24fee-96r2wmt.yaml
#     │           │   ├── rook-ceph-drain-canary-b3da251966d747eebc14ed058dcd1838-59j7bnq
#     │           │   │   └── rook-ceph-drain-canary-b3da251966d747eebc14ed058dcd1838-59j7bnq.yaml
#     │           │   ├── rook-ceph-drain-canary-fe039eeb15c3a68dbed7c98506964678-c5kdnr6
#     │           │   │   └── rook-ceph-drain-canary-fe039eeb15c3a68dbed7c98506964678-c5kdnr6.yaml
#     │           │   ├── rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg
#     │           │   │   ├── mds
#     │           │   │   │   └── mds
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-mds-ocs-storagecluster-cephfilesystem-a-675c6476m5bhg.yaml
#     │           │   ├── rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk
#     │           │   │   ├── mds
#     │           │   │   │   └── mds
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-mds-ocs-storagecluster-cephfilesystem-b-5cc7fd7fvvvkk.yaml
#     │           │   ├── rook-ceph-mgr-a-5bcdb86666-gxw5f
#     │           │   │   ├── mgr
#     │           │   │   │   └── mgr
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-mgr-a-5bcdb86666-gxw5f.yaml
#     │           │   ├── rook-ceph-mon-a-6c7d8bbc5b-fv6g5
#     │           │   │   ├── chown-container-data-dir
#     │           │   │   │   └── chown-container-data-dir
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── init-mon-fs
#     │           │   │   │   └── init-mon-fs
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── mon
#     │           │   │   │   └── mon
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-mon-a-6c7d8bbc5b-fv6g5.yaml
#     │           │   ├── rook-ceph-mon-b-657d966977-ds9pb
#     │           │   │   ├── chown-container-data-dir
#     │           │   │   │   └── chown-container-data-dir
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── init-mon-fs
#     │           │   │   │   └── init-mon-fs
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── mon
#     │           │   │   │   └── mon
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-mon-b-657d966977-ds9pb.yaml
#     │           │   ├── rook-ceph-mon-c-58c5f47974-bdm2g
#     │           │   │   ├── chown-container-data-dir
#     │           │   │   │   └── chown-container-data-dir
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── init-mon-fs
#     │           │   │   │   └── init-mon-fs
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── mon
#     │           │   │   │   └── mon
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-mon-c-58c5f47974-bdm2g.yaml
#     │           │   ├── rook-ceph-operator-7c6c4fd77-k76jl
#     │           │   │   ├── rook-ceph-operator
#     │           │   │   │   └── rook-ceph-operator
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-operator-7c6c4fd77-k76jl.yaml
#     │           │   ├── rook-ceph-osd-0-54d86bc475-kbjdb
#     │           │   │   ├── config-init
#     │           │   │   │   └── config-init
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── osd
#     │           │   │   │   └── osd
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-0-54d86bc475-kbjdb.yaml
#     │           │   ├── rook-ceph-osd-1-5f56fcff97-cnbrg
#     │           │   │   ├── config-init
#     │           │   │   │   └── config-init
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── osd
#     │           │   │   │   └── osd
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-1-5f56fcff97-cnbrg.yaml
#     │           │   ├── rook-ceph-osd-2-5d6b876dd7-hw29d
#     │           │   │   ├── config-init
#     │           │   │   │   └── config-init
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── osd
#     │           │   │   │   └── osd
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-2-5d6b876dd7-hw29d.yaml
#     │           │   ├── rook-ceph-osd-3-67cb6f9499-zdsr7
#     │           │   │   ├── config-init
#     │           │   │   │   └── config-init
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── osd
#     │           │   │   │   └── osd
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-3-67cb6f9499-zdsr7.yaml
#     │           │   ├── rook-ceph-osd-4-595ff56bb-tmmdx
#     │           │   │   ├── config-init
#     │           │   │   │   └── config-init
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── osd
#     │           │   │   │   └── osd
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-4-595ff56bb-tmmdx.yaml
#     │           │   ├── rook-ceph-osd-5-748b96b4d8-nzndk
#     │           │   │   ├── config-init
#     │           │   │   │   └── config-init
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── osd
#     │           │   │   │   └── osd
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-5-748b96b4d8-nzndk.yaml
#     │           │   ├── rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── provision
#     │           │   │   │   └── provision
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-prepare-ocs-deviceset-0-0-qdn6p-t7bn8.yaml
#     │           │   ├── rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── provision
#     │           │   │   │   └── provision
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-prepare-ocs-deviceset-0-1-jqljq-459cb.yaml
#     │           │   ├── rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── provision
#     │           │   │   │   └── provision
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-prepare-ocs-deviceset-1-0-x7hnz-q7f6g.yaml
#     │           │   ├── rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── provision
#     │           │   │   │   └── provision
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-prepare-ocs-deviceset-1-1-wbjp9-lr77n.yaml
#     │           │   ├── rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── provision
#     │           │   │   │   └── provision
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-prepare-ocs-deviceset-2-0-rxlj7-sgrhl.yaml
#     │           │   ├── rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m
#     │           │   │   ├── copy-bins
#     │           │   │   │   └── copy-bins
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   ├── provision
#     │           │   │   │   └── provision
#     │           │   │   │       └── logs
#     │           │   │   │           └── current.log
#     │           │   │   └── rook-ceph-osd-prepare-ocs-deviceset-2-1-zjt7w-d658m.yaml
#     │           │   └── rook-ceph-tools-859dfcff6f-f98sc
#     │           │       ├── rook-ceph-tools
#     │           │       │   └── rook-ceph-tools
#     │           │       │       └── logs
#     │           │       │           └── current.log
#     │           │       └── rook-ceph-tools-859dfcff6f-f98sc.yaml
#     │           └── route.openshift.io
#     │               └── routes.yaml
#     ├── cluster-scoped-resources
#     │   └── objectbucket.io
#     │       └── objectbuckets
#     │           ├── obc-obc-test-obc-test.yaml
#     │           └── obc-openshift-storage-test21obc.yaml
#     ├── namespaces
#     │   ├── obc-test
#     │   │   └── objectbucket.io
#     │   │       └── objectbucketclaims
#     │   │           └── obc-test.yaml
#     │   └── openshift-storage
#     │       ├── logs
#     │       │   ├── noobaa-core-0
#     │       │   │   ├── noobaa-core-0.log
#     │       │   │   └── noobaa-core-0-previous.log
#     │       │   ├── noobaa-operator-7697b7b488-w7fqk
#     │       │   │   ├── noobaa-operator-7697b7b488-w7fqk.log
#     │       │   │   └── noobaa-operator-7697b7b488-w7fqk-previous.log
#     │       │   ├── ocs-operator-55b5dd4d79-j9z75
#     │       │   │   ├── ocs-operator-55b5dd4d79-j9z75.log
#     │       │   │   └── ocs-operator-55b5dd4d79-j9z75-previous.log
#     │       │   └── rook-ceph-operator-7c6c4fd77-k76jl
#     │       │       ├── rook-ceph-operator-7c6c4fd77-k76jl.log
#     │       │       └── rook-ceph-operator-7c6c4fd77-k76jl-previous.log
#     │       ├── objectbucket.io
#     │       │   └── objectbucketclaims
#     │       │       └── test21obc.yaml
#     │       └── ocs.openshift.io
#     │           └── storageclusters
#     │               └── ocs-storagecluster.yaml
#     └── noobaa
#         ├── logs
#         │   └── openshift-storage
#         │       ├── noobaa-core-0.log
#         │       └── noobaa-operator-7697b7b488-w7fqk.log
#         └── namespaces
#             └── openshift-storage
#                 └── noobaa.io
#                     ├── backingstores
#                     │   └── noobaa-default-backing-store.yaml
#                     ├── bucketclasses
#                     │   └── noobaa-default-bucket-class.yaml
#                     └── noobaas
#                         └── noobaa.yaml
# 
# 522 directories, 281 files
#+end_example

#+begin_src bash
oc adm must-gather -h
#+end_src

#+begin_example
# Launch a pod to gather debugging information
# 
#  This command will launch a pod in a temporary namespace on your cluster that gathers debugging information and then
# downloads the gathered information.
# 
#  Experimental: This command is under active development and may change without notice.
# 
# Usage:
#   oc adm must-gather [flags]
# 
# Examples:
#   # gather information using the default plug-in image and command, writing into ./must-gather.local.<rand>
#   oc adm must-gather
#   
#   # gather information with a specific local folder to copy to
#   oc adm must-gather --dest-dir=/local/directory
#   
#   # gather information using multiple plug-in images
#   oc adm must-gather --image=quay.io/kubevirt/must-gather --image=quay.io/openshift/origin-must-gather
#   
#   # gather information using a specific image stream plug-in
#   oc adm must-gather --image-stream=openshift/must-gather:latest
#   
#   # gather information using a specific image, command, and pod-dir
#   oc adm must-gather --image=my/image:tag --source-dir=/pod/directory -- myspecial-command.sh
# 
# Options:
#       --dest-dir='': Set a specific directory on the local machine to write gathered data to.
#       --image=[]: Specify a must-gather plugin image to run. If not specified, OpenShift's default must-gather image
# will be used.
#       --image-stream=[]: Specify an image stream (namespace/name:tag) containing a must-gather plugin image to run.
#       --node-name='': Set a specific node to use - by default a random master will be used
#       --source-dir='/must-gather/': Set the specific directory on the pod copy the gathered data from.
# 
# Use "oc adm options" for a list of global command-line options (applies to all commands).
#+end_example

** OpenShift Log Aggregation
*** OpenShift Log Aggregation

https://docs.openshift.com/container-platform/4.1/logging/efk-logging.html

*** Deploying OpenShift Logging

*** Create the openshift-logging namespace

#+begin_src bash
cat ~/test/support/openshift_logging_namespace.yaml
#+end_src

#+begin_example
# apiVersion: v1
# kind: Namespace
# metadata:
#   name: openshift-logging
#   annotations:
#     openshift.io/node-selector: "" 
#   labels:
#     openshift.io/cluster-logging: "true"
#+end_example

#+begin_src bash
oc create -f ~/test/support/openshift_logging_namespace.yaml
#+end_src

#+begin_example
# namespace/openshift-logging created
#+end_example

*** Install the Elasticsearch and Cluster Logging Operators in the cluster

#+begin_example
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogging"
metadata:
  name: "instance"
  namespace: "openshift-logging"
spec:
  managementState: "Managed"
  logStore:
    type: "elasticsearch"
    elasticsearch:
      nodeCount: 3
      storage:
         storageClassName: ocs-storagecluster-ceph-rbd
         size: 100Gi
      redundancyPolicy: "SingleRedundancy"
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      resources:
        request:
          memory: 4G
  visualization:
    type: "kibana"
    kibana:
      replicas: 1
      nodeSelector:
        node-role.kubernetes.io/infra: ""
  curation:
    type: "curator"
    curator:
      schedule: "30 3 * * *"
      nodeSelector:
        node-role.kubernetes.io/infra: ""
  collection:
    logs:
      type: "fluentd"
      fluentd: {}
      nodeSelector:
        node-role.kubernetes.io/infra: ""
#+end_example

#+begin_src bash
oc get pods -n openshift-logging
#+end_src

#+begin_example
# NAME                                            READY   STATUS              RESTARTS   AGE
# cluster-logging-operator-7bfd9895c9-hglc7       1/1     Running             0          5m53s
# elasticsearch-cdm-cfsd60sz-1-97986d555-7rb52    0/2     ContainerCreating   0          39s
# elasticsearch-cdm-cfsd60sz-2-f4896798b-65jfl    0/2     ContainerCreating   0          36s
# elasticsearch-cdm-cfsd60sz-3-6f4c858b5d-x558b   0/2     ContainerCreating   0          33s
# fluentd-5l7tr                                   1/1     Running             0          38s
# fluentd-5nhpr                                   1/1     Running             0          38s
# fluentd-5nxxb                                   1/1     Running             0          38s
# fluentd-72bgk                                   1/1     Running             0          39s
# fluentd-84tk4                                   1/1     Running             0          38s
# fluentd-9m44j                                   1/1     Running             0          39s
# fluentd-9tvd8                                   1/1     Running             0          38s
# fluentd-f7nfv                                   1/1     Running             0          39s
# fluentd-hqmfz                                   1/1     Running             0          38s
# fluentd-l8w7l                                   1/1     Running             0          39s
# fluentd-lp2lt                                   1/1     Running             0          38s
# fluentd-mbw6x                                   1/1     Running             0          38s
# fluentd-q7pqf                                   1/1     Running             0          39s
# fluentd-rfmm4                                   1/1     Running             0          39s
# fluentd-zxx9h                                   1/1     Running             0          39s
# kibana-6ff8fd577b-x8ktn                         2/2     Running             0          40s
#+end_example

#+begin_src bash
oc get pods -n openshift-logging
#+end_src

#+begin_example
# NAME                                            READY   STATUS    RESTARTS   AGE
# cluster-logging-operator-7bfd9895c9-hglc7       1/1     Running   0          6m41s
# elasticsearch-cdm-cfsd60sz-1-97986d555-7rb52    2/2     Running   0          87s
# elasticsearch-cdm-cfsd60sz-2-f4896798b-65jfl    2/2     Running   0          84s
# elasticsearch-cdm-cfsd60sz-3-6f4c858b5d-x558b   2/2     Running   0          81s
# fluentd-5l7tr                                   1/1     Running   0          86s
# fluentd-5nhpr                                   1/1     Running   0          86s
# fluentd-5nxxb                                   1/1     Running   0          86s
# fluentd-72bgk                                   1/1     Running   0          87s
# fluentd-84tk4                                   1/1     Running   0          86s
# fluentd-9m44j                                   1/1     Running   0          87s
# fluentd-9tvd8                                   1/1     Running   0          86s
# fluentd-f7nfv                                   1/1     Running   0          87s
# fluentd-hqmfz                                   1/1     Running   0          86s
# fluentd-l8w7l                                   1/1     Running   0          87s
# fluentd-lp2lt                                   1/1     Running   0          86s
# fluentd-mbw6x                                   1/1     Running   0          86s
# fluentd-q7pqf                                   1/1     Running   0          87s
# fluentd-rfmm4                                   1/1     Running   0          87s
# fluentd-zxx9h                                   1/1     Running   0          87s
# kibana-6ff8fd577b-x8ktn                         2/2     Running   0          88s
#+end_example

#+begin_src bash
oc get daemonset -n openshift-logging
#+end_src

#+begin_example
# NAME      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
# fluentd   15        15        15      15           15          kubernetes.io/os=linux   99s
#+end_example

#+begin_src bash
oc get pvc -n openshift-logging
#+end_src

#+begin_example
# NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                  AGE
# elasticsearch-elasticsearch-cdm-cfsd60sz-1   Bound    pvc-80b1776f-54a4-11ea-99d2-0a84b9320ef6   100Gi      RWO            ocs-storagecluster-ceph-rbd   3m12s
# elasticsearch-elasticsearch-cdm-cfsd60sz-2   Bound    pvc-80b2fb28-54a4-11ea-99d2-0a84b9320ef6   100Gi      RWO            ocs-storagecluster-ceph-rbd   3m12s
# elasticsearch-elasticsearch-cdm-cfsd60sz-3   Bound    pvc-80b47559-54a4-11ea-99d2-0a84b9320ef6   100Gi      RWO            ocs-storagecluster-ceph-rbd   3m12s
#+end_example

*** Accessing Kibana

#+begin_src bash
oc get route -n openshift-logging
#+end_src

#+begin_example
# NAME     HOST/PORT                                                                   PATH   SERVICES   PORT    TERMINATION          WILDCARD
# kibana   kibana-openshift-logging.apps.cluster-munich-e7ab.sandbox1596.opentlc.com          kibana     <all>   reencrypt/Redirect   None
#+end_example

** External (LDAP) Authentication Providers, Users, and Groups
*** Examine the OAuth configuration

#+begin_src bash
oc get oauth cluster -o yaml
#+end_src

#+begin_example
# apiVersion: config.openshift.io/v1
# kind: OAuth
# metadata:
#   annotations:
#     release.openshift.io/create-only: "true"
#   creationTimestamp: "2020-02-19T17:56:51Z"
#   generation: 1
#   name: cluster
#   resourceVersion: "1738"
#   selfLink: /apis/config.openshift.io/v1/oauths/cluster
#   uid: 34bf32d6-5341-11ea-8707-027cbd288fd4
# spec: {}
#+end_example

#+begin_src bash
oc get secret -n kube-system kubeadmin -o yaml
#+end_src

#+begin_example
# apiVersion: v1
# data:
#   kubeadmin: JDJhJDEwJGQ0bkRqQzFkamdhem01cnFldm9PSGVObHYxSzdDNTQ2QUMyb0Q1TjdwUnpSbUIzOHNNQ0xh
# kind: Secret
# metadata:
#   creationTimestamp: "2020-02-19T17:55:47Z"
#   name: kubeadmin
#   namespace: kube-system
#   resourceVersion: "53"
#   selfLink: /api/v1/namespaces/kube-system/secrets/kubeadmin
#   uid: 0eb17e0f-5341-11ea-8707-027cbd288fd4
# type: Opaque
#+end_example

#+begin_src bash
oc create secret generic ldap-secret --from-literal=bindPassword=bindingpassword -n openshift-config
#+end_src

#+begin_example
# secret/ldap-secret created
#+end_example

#+begin_src bash
wget http://idm.openshiftworkshop.com/pub/ca.crt -O ~/test/ca.crt
#+end_src

#+begin_example
# --2020-02-21 13:01:52--  http://idm.openshiftworkshop.com/pub/ca.crt
# Resolving idm.openshiftworkshop.com (idm.openshiftworkshop.com)... 54.172.16.6
# Connecting to idm.openshiftworkshop.com (idm.openshiftworkshop.com)|54.172.16.6|:80... connected.
# HTTP request sent, awaiting response... 200 OK
# Length: 1350 (1.3K) [application/x-x509-ca-cert]
# Saving to: ‘/home/lab-user/test/ca.crt’
# 
# 100%[====================================================================================>] 1,350       --.-K/s   in 0s      
# 
# 2020-02-21 13:01:53 (186 MB/s) - ‘/home/lab-user/test/ca.crt’ saved [1350/1350]
# 
#+end_example

#+begin_src bash
oc create configmap ca-config-map --from-file=/home/lab-user/test/ca.crt -n openshift-config
#+end_src

#+begin_example
# configmap/ca-config-map created
#+end_example

#+begin_src bash
cat ~/test/support/oauth-cluster.yaml
#+end_src

#+begin_example
# apiVersion: config.openshift.io/v1
# kind: OAuth
# metadata:
#   name: cluster
# spec:
#   identityProviders:
#   - name: idm
#     challenge: false
#     login: true
#     mappingMethod: claim
#     type: LDAP
#     ldap:
#       attributes:
#         id:
#         - dn
#         email:
#         - mail
#         name:
#         - cn
#         preferredUsername:
#         - uid
#       bindDN: "uid=system,cn=sysaccounts,cn=etc,dc=auth,dc=openshiftworkshop,dc=com"
#       bindPassword:
#         name: ldap-secret
#       ca:
#         name: ca-config-map
#       insecure: false
#       url: "ldap://idm.openshiftworkshop.com/cn=users,cn=accounts,dc=auth,dc=openshiftworkshop,dc=com?uid?sub?(memberOf=cn=ose-user,cn=groups,cn=accounts,dc=auth,dc=openshiftworkshop,dc=com)"
#   tokenConfig:
#+end_example

#+begin_src bash
oc apply -f ~/test/support/oauth-cluster.yaml
#+end_src

#+begin_example
# Warning: oc apply should be used on resource created by either oc create --save-config or oc apply
# oauth.config.openshift.io/cluster configured
#+end_example

*** Syncing LDAP Groups to OpenShift Groups

Change ca: to point to the downloaded ca.crt file

#+begin_src bash
cat ~/test/support/groupsync.yaml
#+end_src

#+begin_example
# kind: LDAPSyncConfig
# apiVersion: v1
# url: ldap://idm.openshiftworkshop.com
# ca: /home/lab-user/test/ca.crt
# bindDN: uid=system,cn=sysaccounts,cn=etc,dc=auth,dc=openshiftworkshop,dc=com
# bindPassword: bindingpassword
# rfc2307:
#   groupsQuery:
#     baseDN: cn=groups,cn=accounts,dc=auth,dc=openshiftworkshop,dc=com
#     derefAliases: never
#     filter: '(|(cn=ose-*))'
#   groupUIDAttribute: dn
#   groupNameAttributes:
#   - cn
#   groupMembershipAttributes:
#   - member
#   usersQuery:
#     baseDN: cn=users,cn=accounts,dc=auth,dc=openshiftworkshop,dc=com
#     derefAliases: never
#   userUIDAttribute: dn
#   userNameAttributes:
#   - uid
# 
#+end_example

#+begin_src bash
oc adm groups sync --sync-config=/home/lab-user/test/support/groupsync.yaml --confirm
#+end_src

#+begin_example
# group/ose-user
# group/ose-normal-dev
# group/ose-fancy-dev
# group/ose-teamed-app
#+end_example

#+begin_src bash
oc adm groups sync -h
#+end_src

#+begin_example
# Sync OpenShift Groups with records from an external provider.
# 
#  In order to sync OpenShift Group records with those from an external provider, determine which Groups you wish to sync
# and where their records live. For instance, all or some groups may be selected from the current Groups stored in
# OpenShift that have been synced previously, or similarly all or some groups may be selected from those stored on an LDAP
# server. The path to a sync configuration file is required in order to describe how data is requested from the external
# record store and migrated to OpenShift records. Default behavior is to do a dry-run without changing OpenShift records.
# Passing '--confirm' will sync all groups from the LDAP server returned by the LDAP query templates.
# 
# Usage:
#   oc adm groups sync [--type=TYPE] [WHITELIST] [--whitelist=WHITELIST-FILE] --sync-config=CONFIG-FILE [--confirm]
# [flags]
# 
# Examples:
#   # Sync all groups from an LDAP server
#   oc adm groups sync --sync-config=/path/to/ldap-sync-config.yaml --confirm
#   
#   # Sync all groups except the ones from the blacklist file from an LDAP server
#   oc adm groups sync --blacklist=/path/to/blacklist.txt --sync-config=/path/to/ldap-sync-config.yaml --confirm
#   
#   # Sync specific groups specified in a whitelist file with an LDAP server
#   oc adm groups sync --whitelist=/path/to/whitelist.txt --sync-config=/path/to/sync-config.yaml --confirm
#   
#   # Sync all OpenShift Groups that have been synced previously with an LDAP server
#   oc adm groups sync --type=openshift --sync-config=/path/to/ldap-sync-config.yaml --confirm
#   
#   # Sync specific OpenShift Groups if they have been synced previously with an LDAP server
#   oc adm groups sync groups/group1 groups/group2 groups/group3 --sync-config=/path/to/sync-config.yaml --confirm
# 
# Options:
#       --blacklist='': path to the group blacklist file
#       --confirm=false: if true, modify OpenShift groups; if false, display results of a dry-run
#       --sync-config='': path to the sync config
#       --type='ldap': which groups white- and blacklist entries refer to: ldap,openshift
#       --whitelist='': path to the group whitelist file
# 
# Use "oc adm options" for a list of global command-line options (applies to all commands).
#+end_example

#+begin_src bash
oc get groups
#+end_src

#+begin_example
# NAME             USERS
# ose-fancy-dev    fancyuser1, fancyuser2
# ose-normal-dev   normaluser1, teamuser1, teamuser2
# ose-teamed-app   teamuser1, teamuser2
# ose-user         normaluser1, fancyuser1, fancyuser2, teamuser1, teamuser2
#+end_example

#+begin_src bash
oc get group ose-fancy-dev -o yaml
#+end_src

#+begin_example
# apiVersion: user.openshift.io/v1
# kind: Group
# metadata:
#   annotations:
#     openshift.io/ldap.sync-time: "2020-02-21T13:14:55Z"
#     openshift.io/ldap.uid: cn=ose-fancy-dev,cn=groups,cn=accounts,dc=auth,dc=openshiftworkshop,dc=com
#     openshift.io/ldap.url: idm.openshiftworkshop.com:389
#   creationTimestamp: "2020-02-21T13:14:55Z"
#   labels:
#     openshift.io/ldap.host: idm.openshiftworkshop.com
#   name: ose-fancy-dev
#   resourceVersion: "832003"
#   selfLink: /apis/user.openshift.io/v1/groups/ose-fancy-dev
#   uid: 271480fd-54ac-11ea-b0e7-0a580a800016
# users:
# - fancyuser1
# - fancyuser2
#+end_example

#+begin_src bash
oc get user
#+end_src

#+begin_example
# No resources found.
#+end_example

*** Change Group Policy

#+begin_src bash
oc adm policy add-cluster-role-to-group cluster-reader ose-fancy-dev
#+end_src

#+begin_example
# clusterrole.rbac.authorization.k8s.io/cluster-reader added: "ose-fancy-dev"
#+end_example

*** Examine cluster-reader policy

#+begin_src bash
oc login -u normaluser1 -p openshift
#+end_src

#+begin_example
# Login successful.
# 
# You don't have any projects. You can try to create a new project, by running
# 
#     oc new-project <projectname>
# 
#+end_example

#+begin_src bash
oc get projects
#+end_src

#+begin_example
# No resources found.
#+end_example

#+begin_src bash
oc login -u fancyuser1 -p openshift
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "default".
#+end_example

#+begin_src bash
oc get projects
#+end_src

#+begin_example
# NAME                                                    DISPLAY NAME   STATUS
# app-management                                                         Active
# default                                                                Active
# kube-node-lease                                                        Active
# kube-public                                                            Active
# kube-system                                                            Active
# lab-ocp-cns                                                            Active
# my-database-app                                                        Active
# my-shared-storage                                                      Active
# obc-test                                                               Active
# openshift                                                              Active
# openshift-apiserver                                                    Active
# openshift-apiserver-operator                                           Active
# openshift-authentication                                               Active
# openshift-authentication-operator                                      Active
# openshift-cloud-credential-operator                                    Active
# openshift-cluster-machine-approver                                     Active
# openshift-cluster-node-tuning-operator                                 Active
# openshift-cluster-samples-operator                                     Active
# openshift-cluster-storage-operator                                     Active
# openshift-cluster-version                                              Active
# openshift-config                                                       Active
# openshift-config-managed                                               Active
# openshift-console                                                      Active
# openshift-console-operator                                             Active
# openshift-controller-manager                                           Active
# openshift-controller-manager-operator                                  Active
# openshift-dns                                                          Active
# openshift-dns-operator                                                 Active
# openshift-etcd                                                         Active
# openshift-image-registry                                               Active
# openshift-infra                                                        Active
# openshift-ingress                                                      Active
# openshift-ingress-operator                                             Active
# openshift-insights                                                     Active
# openshift-kni-infra                                                    Active
# openshift-kube-apiserver                                               Active
# openshift-kube-apiserver-operator                                      Active
# openshift-kube-controller-manager                                      Active
# openshift-kube-controller-manager-operator                             Active
# openshift-kube-scheduler                                               Active
# openshift-kube-scheduler-operator                                      Active
# openshift-logging                                                      Active
# openshift-machine-api                                                  Active
# openshift-machine-config-operator                                      Active
# openshift-marketplace                                                  Active
# openshift-monitoring                                                   Active
# openshift-multus                                                       Active
# openshift-must-gather-td7bl                                            Active
# openshift-network-operator                                             Active
# openshift-node                                                         Active
# openshift-openstack-infra                                              Active
# openshift-operator-lifecycle-manager                                   Active
# openshift-operators                                                    Active
# openshift-sdn                                                          Active
# openshift-service-ca                                                   Active
# openshift-service-ca-operator                                          Active
# openshift-service-catalog-apiserver-operator                           Active
# openshift-service-catalog-controller-manager-operator                  Active
# openshift-storage                                                      Active
#+end_example

*** Create Projects for Collaboration

#+begin_src bash
oc login -u system:serviceaccount:lab-ocp-cns:dashboard-user
#+end_src

#+begin_example
# Authentication required for https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443 (openshift)
# Username: system:serviceaccount:lab-ocp-cns:dashboard-user
# Password: 
# error: username system:serviceaccount:lab-ocp-cns:dashboard-user is invalid for basic auth
#+end_example

#+begin_src bash
oc login -u kubeadmin
#+end_src

#+begin_example
# Authentication required for https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443 (openshift)
# Username: kubeadmin
# Password: 
# Login successful.
# 
# You have access to 59 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "default".
#+end_example

RmEAm-FN5EH-XhpAH-YYbf8

#+begin_src bash
oc adm new-project app-dev --display-name="Application Development"
oc adm new-project app-test --display-name="Application Testing"
oc adm new-project app-prod --display-name="Application Production"
#+end_src

#+begin_example
# Created project app-dev
# Created project app-test
# Created project app-prod
#+end_example

*** Map Groups to Projects

#+begin_src bash
oc adm policy add-role-to-group edit ose-teamed-app -n app-dev
oc adm policy add-role-to-group edit ose-teamed-app -n app-test
#+end_src

#+begin_example
# clusterrole.rbac.authorization.k8s.io/edit added: "ose-teamed-app"
# clusterrole.rbac.authorization.k8s.io/edit added: "ose-teamed-app"
#+end_example

#+begin_src bash
oc adm policy add-role-to-group view ose-teamed-app -n app-prod
#+end_src

#+begin_example
# clusterrole.rbac.authorization.k8s.io/view added: "ose-teamed-app"
#+end_example

#+begin_src bash
oc adm policy add-role-to-group edit ose-fancy-dev -n app-prod
#+end_src

#+begin_example
# clusterrole.rbac.authorization.k8s.io/edit added: "ose-fancy-dev"
#+end_example

*** Examine Group Access

#+begin_src bash
oc login -u normaluser1 -p openshift
#+end_src

#+begin_src bash
oc get projects
#+end_src

#+begin_example
# No resources found.
#+end_example

#+begin_src bash
oc login -u teamuser1 -p openshift
#+end_src

#+begin_example
# Login successful.
# 
# You have access to the following projects and can switch between them with 'oc project <projectname>':
# 
#   * app-dev
#     app-prod
#     app-test
# 
# Using project "app-dev".
#+end_example

#+begin_src bash
oc get projects
#+end_src

#+begin_example
# NAME       DISPLAY NAME              STATUS
# app-dev    Application Development   Active
# app-prod   Application Production    Active
# app-test   Application Testing       Active
#+end_example

#+begin_src bash
oc project app-prod
#+end_src

#+begin_example
# Now using project "app-prod" on server "https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443".
#+end_example

#+begin_src bash
oc new-app docker.io/siamaksade/mapit
#+end_src

#+begin_example
# error: can't lookup images: imagestreamimports.image.openshift.io is forbidden: User "teamuser1" cannot create resource "imagestreamimports" in API group "image.openshift.io" in the namespace "app-prod"
# error:  local file access failed with: stat docker.io/siamaksade/mapit: no such file or directory
# error: unable to locate any images in image streams, templates loaded in accessible projects, template files, local docker images with name "docker.io/siamaksade/mapit"
# 
# Argument 'docker.io/siamaksade/mapit' was classified as an image, image~source, or loaded template reference.
# 
# The 'oc new-app' command will match arguments to the following types:
# 
#   1. Images tagged into image streams in the current project or the 'openshift' project
#      - if you don't specify a tag, we'll add ':latest'
#   2. Images in the Docker Hub, on remote registries, or on the local Docker engine
#   3. Templates in the current project or the 'openshift' project
#   4. Git repository URLs or local paths that point to Git repositories
# 
# --allow-missing-images can be used to point to an image that does not exist yet.
# 
# See 'oc new-app -h' for examples.
#+end_example

*** Prometheus

#+begin_src bash
oc login -u fancyuser1 -p openshift
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 62 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "app-prod".
#+end_example

#+begin_src bash
oc get route prometheus-k8s -n openshift-monitoring
#+end_src

#+begin_example
# NAME             HOST/PORT                                                                              PATH   SERVICES         PORT   TERMINATION          WILDCARD
# prometheus-k8s   prometheus-k8s-openshift-monitoring.apps.cluster-munich-e7ab.sandbox1596.opentlc.com          prometheus-k8s   web    reencrypt/Redirect   None
#+end_example

Before continuing, make sure to go to the OpenShift web console and log out by
using the dropdown menu at the upper right where it says kube:admin. Otherwise
Prometheus will try to use your kubeadmin user to pass through
authentication. While it will work, it doesn’t demonstrate the cluster-reader
role.

https://prometheus-k8s-openshift-monitoring.apps.cluster-munich-e7ab.sandbox1596.opentlc.com

#+begin_src bash
oc login -u system:serviceaccount:lab-ocp-cns:dashboard-user
#+end_src

#+begin_src bash
oc login -u system:serviceaccount:lab-ocp-cns:dashboard-user -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# error: username system:serviceaccount:lab-ocp-cns:dashboard-user is invalid for basic auth
#+end_example

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 62 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "app-prod".
#+end_example

** OpenShift Monitoring with Prometheus

*** Running Prometheus Queries

sum(kube_pod_container_resource_limits_memory_bytes)/(1024^3)

cluster:cpu_usage_cores:sum

** Project Template, Quota, and Limits

*** View the Default Project Request Template

#+begin_src bash
oc adm create-bootstrap-project-template -o yaml
#+end_src

#+begin_example
# apiVersion: template.openshift.io/v1
# kind: Template
# metadata:
#   creationTimestamp: null
#   name: project-request
# objects:
# - apiVersion: project.openshift.io/v1
#   kind: Project
#   metadata:
#     annotations:
#       openshift.io/description: ${PROJECT_DESCRIPTION}
#       openshift.io/display-name: ${PROJECT_DISPLAYNAME}
#       openshift.io/requester: ${PROJECT_REQUESTING_USER}
#     creationTimestamp: null
#     name: ${PROJECT_NAME}
#   spec: {}
#   status: {}
# - apiVersion: rbac.authorization.k8s.io/v1
#   kind: RoleBinding
#   metadata:
#     creationTimestamp: null
#     name: admin
#     namespace: ${PROJECT_NAME}
#   roleRef:
#     apiGroup: rbac.authorization.k8s.io
#     kind: ClusterRole
#     name: admin
#   subjects:
#   - apiGroup: rbac.authorization.k8s.io
#     kind: User
#     name: ${PROJECT_ADMIN_USER}
# parameters:
# - name: PROJECT_NAME
# - name: PROJECT_DISPLAYNAME
# - name: PROJECT_DESCRIPTION
# - name: PROJECT_ADMIN_USER
# - name: PROJECT_REQUESTING_USER
#+end_example

#+begin_src bash
oc new-project -h
#+end_src

#+begin_example
# Create a new project for yourself
# 
#  If your administrator allows self-service, this command will create a new project for you and assign you as the project
# admin.
# 
#  After your project is created it will become the default project in your config.
# 
# Usage:
#   oc new-project NAME [--display-name=DISPLAYNAME] [--description=DESCRIPTION] [flags]
# 
# Examples:
#   # Create a new project with minimal information
#   oc new-project web-team-dev
#   
#   # Create a new project with a display name and description
#   oc new-project web-team-dev --display-name="Web Team Development" --description="Development project for the web
# team."
# 
# Options:
#       --description='': Project description
#       --display-name='': Project display name
#       --skip-config-write=false: If true, the project will not be set as a cluster entry in kubeconfig after being
# created
# 
# Use "oc options" for a list of global command-line options (applies to all commands).
#+end_example

*** Modify the Project Request Template

#+begin_src bash
cat ~/test/support/project_request_template.yaml
#+end_src

#+begin_example
# apiVersion: v1
# kind: Template
# metadata:
#   creationTimestamp: null
#   name: project-request
# objects:
# - apiVersion: v1
#   kind: Project
#   metadata:
#     annotations:
#       openshift.io/description: ${PROJECT_DESCRIPTION}
#       openshift.io/display-name: ${PROJECT_DISPLAYNAME}
#       openshift.io/requester: ${PROJECT_REQUESTING_USER}
#     creationTimestamp: null
#     name: ${PROJECT_NAME}
#   spec: {}
#   status: {}
# - apiVersion: v1
#   kind: ResourceQuota
#   metadata:
#     name: ${PROJECT_NAME}-quota
#   spec:
#     hard:
#       pods: 10
#       requests.cpu: 4000m
#       requests.memory: 8Gi
#       resourcequotas: 1
#       requests.storage: 50Gi
#       persistentvolumeclaims: 5
# - apiVersion: v1
#   kind: LimitRange
#   metadata:
#     name: ${PROJECT_NAME}-limits
#     creationTimestamp: null
#   spec:
#     limits:
#       -
#         type: Container
#         max:
#           cpu: 4000m
#           memory: 1024Mi
#         min:
#           cpu: 10m
#           memory: 5Mi
#         default:
#           cpu: 4000m
#           memory: 1024Mi
#         defaultRequest:
#           cpu: 100m
#           memory: 512Mi
# - apiVersion: v1
#   groupNames:
#   - system:serviceaccounts:${PROJECT_NAME}
#   kind: RoleBinding
#   metadata:
#     creationTimestamp: null
#     name: system:image-pullers
#     namespace: ${PROJECT_NAME}
#   roleRef:
#     name: system:image-puller
#   subjects:
#   - kind: SystemGroup
#     name: system:serviceaccounts:${PROJECT_NAME}
#   userNames: null
# - apiVersion: v1
#   groupNames: null
#   kind: RoleBinding
#   metadata:
#     creationTimestamp: null
#     name: system:image-builders
#     namespace: ${PROJECT_NAME}
#   roleRef:
#     name: system:image-builder
#   subjects:
#   - kind: ServiceAccount
#     name: builder
#   userNames:
#   - system:serviceaccount:${PROJECT_NAME}:builder
# - apiVersion: v1
#   groupNames: null
#   kind: RoleBinding
#   metadata:
#     creationTimestamp: null
#     name: system:deployers
#     namespace: ${PROJECT_NAME}
#   roleRef:
#     name: system:deployer
#   subjects:
#   - kind: ServiceAccount
#     name: deployer
#   userNames:
#   - system:serviceaccount:${PROJECT_NAME}:deployer
# - apiVersion: v1
#   groupNames: null
#   kind: RoleBinding
#   metadata:
#     creationTimestamp: null
#     name: admin
#     namespace: ${PROJECT_NAME}
#   roleRef:
#     name: admin
#   subjects:
#   - kind: User
#     name: ${PROJECT_ADMIN_USER}
#   userNames:
#   - ${PROJECT_ADMIN_USER}
# parameters:
# - name: PROJECT_NAME
# - name: PROJECT_DISPLAYNAME
# - name: PROJECT_DESCRIPTION
# - name: PROJECT_ADMIN_USER
# - name: PROJECT_REQUESTING_USER
#+end_example

*** Create the Template

#+begin_src bash
oc create -f ~/test/support/project_request_template.yaml -n openshift-config
#+end_src

#+begin_example
# template.template.openshift.io/project-request created
#+end_example

#+begin_src bash
oc get template -n openshift-config
#+end_src

#+begin_example
# NAME              DESCRIPTION   PARAMETERS    OBJECTS
# project-request                 5 (5 blank)   7
#+end_example
*** Setting the Default ProjectRequestTemplate

#+begin_src bash
oc get cm config -n openshift-apiserver -o jsonpath --template="{.data.config\.yaml}" | jq
#+end_src

#+begin_example
# {
#   "aggregatorConfig": {
#     "allowedNames": [
#       "kube-apiserver-proxy",
#       "system:kube-apiserver-proxy",
#       "system:openshift-aggregator"
#     ],
#     "clientCA": "/var/run/configmaps/aggregator-client-ca/ca-bundle.crt",
#     "extraHeaderPrefixes": [
#       "X-Remote-Extra-"
#     ],
#     "groupHeaders": [
#       "X-Remote-Group"
#     ],
#     "usernameHeaders": [
#       "X-Remote-User"
#     ]
#   },
#   "apiServerArguments": {
#     "minimal-shutdown-duration": [
#       "3s"
#     ]
#   },
#   "apiVersion": "openshiftcontrolplane.config.openshift.io/v1",
#   "auditConfig": {
#     "auditFilePath": "/var/log/openshift-apiserver/audit.log",
#     "enabled": true,
#     "logFormat": "json",
#     "maximumFileSizeMegabytes": 100,
#     "maximumRetainedFiles": 10,
#     "policyConfiguration": {
#       "apiVersion": "audit.k8s.io/v1beta1",
#       "kind": "Policy",
#       "omitStages": [
#         "RequestReceived"
#       ],
#       "rules": [
#         {
#           "level": "None",
#           "resources": [
#             {
#               "group": "",
#               "resources": [
#                 "events"
#               ]
#             }
#           ]
#         },
#         {
#           "level": "None",
#           "resources": [
#             {
#               "group": "oauth.openshift.io",
#               "resources": [
#                 "oauthaccesstokens",
#                 "oauthauthorizetokens"
#               ]
#             }
#           ]
#         },
#         {
#           "level": "None",
#           "nonResourceURLs": [
#             "/api*",
#             "/version",
#             "/healthz"
#           ],
#           "userGroups": [
#             "system:authenticated",
#             "system:unauthenticated"
#           ]
#         },
#         {
#           "level": "Metadata",
#           "omitStages": [
#             "RequestReceived"
#           ]
#         }
#       ]
#     }
#   },
#   "imagePolicyConfig": {
#     "internalRegistryHostname": "image-registry.openshift-image-registry.svc:5000"
#   },
#   "kind": "OpenShiftAPIServerConfig",
#   "projectConfig": {
#     "projectRequestMessage": ""
#   },
#   "routingConfig": {
#     "subdomain": "apps.cluster-munich-e7ab.sandbox1596.opentlc.com"
#   },
#   "storageConfig": {
#     "urls": [
#       "https://etcd.openshift-etcd.svc:2379"
#     ]
#   }
# }
#+end_example

#+begin_src bash
cat ~/test/support/cr_project_request.yaml
#+end_src

#+begin_example
# apiVersion: "config.openshift.io/v1"
# kind: "Project"
# metadata: 
#   name: "cluster"
#   namespace: ""
# spec: 
#   projectRequestMessage: ""
#   projectRequestTemplate: 
#     name: "project-request"
#+end_example

#+begin_src bash
oc apply -f ~/test/support/cr_project_request.yaml -n openshift-config
#+end_src

#+begin_example
# Warning: oc apply should be used on resource created by either oc create --save-config or oc apply
# project.config.openshift.io/cluster configured
#+end_example

#+begin_src bash
oc get cm config -n openshift-apiserver -o jsonpath --template="{.data.config\.yaml}" | jq
#+end_src

#+begin_example
# {
#   "aggregatorConfig": {
#     "allowedNames": [
#       "kube-apiserver-proxy",
#       "system:kube-apiserver-proxy",
#       "system:openshift-aggregator"
#     ],
#     "clientCA": "/var/run/configmaps/aggregator-client-ca/ca-bundle.crt",
#     "extraHeaderPrefixes": [
#       "X-Remote-Extra-"
#     ],
#     "groupHeaders": [
#       "X-Remote-Group"
#     ],
#     "usernameHeaders": [
#       "X-Remote-User"
#     ]
#   },
#   "apiServerArguments": {
#     "minimal-shutdown-duration": [
#       "3s"
#     ]
#   },
#   "apiVersion": "openshiftcontrolplane.config.openshift.io/v1",
#   "auditConfig": {
#     "auditFilePath": "/var/log/openshift-apiserver/audit.log",
#     "enabled": true,
#     "logFormat": "json",
#     "maximumFileSizeMegabytes": 100,
#     "maximumRetainedFiles": 10,
#     "policyConfiguration": {
#       "apiVersion": "audit.k8s.io/v1beta1",
#       "kind": "Policy",
#       "omitStages": [
#         "RequestReceived"
#       ],
#       "rules": [
#         {
#           "level": "None",
#           "resources": [
#             {
#               "group": "",
#               "resources": [
#                 "events"
#               ]
#             }
#           ]
#         },
#         {
#           "level": "None",
#           "resources": [
#             {
#               "group": "oauth.openshift.io",
#               "resources": [
#                 "oauthaccesstokens",
#                 "oauthauthorizetokens"
#               ]
#             }
#           ]
#         },
#         {
#           "level": "None",
#           "nonResourceURLs": [
#             "/api*",
#             "/version",
#             "/healthz"
#           ],
#           "userGroups": [
#             "system:authenticated",
#             "system:unauthenticated"
#           ]
#         },
#         {
#           "level": "Metadata",
#           "omitStages": [
#             "RequestReceived"
#           ]
#         }
#       ]
#     }
#   },
#   "imagePolicyConfig": {
#     "internalRegistryHostname": "image-registry.openshift-image-registry.svc:5000"
#   },
#   "kind": "OpenShiftAPIServerConfig",
#   "projectConfig": {
#     "projectRequestMessage": "",
#     "projectRequestTemplate": "openshift-config/project-request"
#   },
#   "routingConfig": {
#     "subdomain": "apps.cluster-munich-e7ab.sandbox1596.opentlc.com"
#   },
#   "storageConfig": {
#     "urls": [
#       "https://etcd.openshift-etcd.svc:2379"
#     ]
#   }
# }
#+end_example

*** Create a New Project

#+begin_src bash
oc new-project template-test
#+end_src

#+begin_example
# Now using project "template-test" on server "https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443".
# 
# You can add applications to this project with the 'new-app' command. For example, try:
# 
#     oc new-app django-psql-example
# 
# to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:
# 
#     kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
# 
#+end_example

#+begin_src bash
oc describe project template-test
#+end_src

#+begin_example
# Name:			template-test
# Created:		22 seconds ago
# Labels:			<none>
# Annotations:		openshift.io/description=
# 			openshift.io/display-name=
# 			openshift.io/requester=kube:admin
# 			openshift.io/sa.scc.mcs=s0:c27,c14
# 			openshift.io/sa.scc.supplemental-groups=1000730000/10000
# 			openshift.io/sa.scc.uid-range=1000730000/10000
# Display Name:		<none>
# Description:		<none>
# Status:			Active
# Node Selector:		<none>
# Quota:			<none>
# Resource limits:	<none>
#+end_example

#+begin_src bash
oc delete project template-test
#+end_src

#+begin_example
# project.project.openshift.io "template-test" deleted
#+end_example

#+begin_src bash
oc new-project template-test
#+end_src

#+begin_src bash
oc describe project template-test
#+end_src

#+begin_example
# Name:		template-test
# Created:	18 seconds ago
# Labels:		<none>
# Annotations:	openshift.io/description=
# 		openshift.io/display-name=
# 		openshift.io/requester=kube:admin
# 		openshift.io/sa.scc.mcs=s0:c27,c24
# 		openshift.io/sa.scc.supplemental-groups=1000750000/10000
# 		openshift.io/sa.scc.uid-range=1000750000/10000
# Display Name:	<none>
# Description:	<none>
# Status:		Active
# Node Selector:	<none>
# Quota:
# 	Name:			template-test-quota
# 	Resource		Used	Hard
# 	--------		----	----
# 	persistentvolumeclaims	0	5
# 	pods			0	10
# 	requests.cpu		0	4
# 	requests.memory		0	8Gi
# 	requests.storage	0	50Gi
# 	resourcequotas		1	1
# Resource limits:
# 	Name:		template-test-limits
# 	Type		Resource	Min	Max	Default Request	Default Limit	Max Limit/Request Ratio
# 	----		--------	---	---	---------------	-------------	-----------------------
# 	Container	memory		5Mi	1Gi	512Mi		1Gi		-
# 	Container	cpu		10m	4	100m		4		-
#+end_example

#+begin_src bash
oc get quota -n template-test
#+end_src

#+begin_example
# NAME                  CREATED AT
# template-test-quota   2020-02-21T14:32:32Z
#+end_example

#+begin_src bash
oc get limitrange -n template-test
#+end_src

#+begin_example
# NAME                   CREATED AT
# template-test-limits   2020-02-21T14:32:32Z
#+end_example

*** Clean Up

#+begin_src bash
oc delete project template-test
#+end_src

*** Redeploy the "Basics" section

Deploy the application from the Application Management Basics lab again inside
this template-test project to observe how the Quota and LimitRange are
applied. If you do, be sure to look at the JSON/YAML output (oc get …​ -o yaml)
for things like the DeploymentConfig and the Pod.

** OpenShift Networking and NetworkPolicy
*** Switch Your Project

#+begin_src bash
oc project default
#+end_src

#+begin_example
# Now using project "default" on server "https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443".
#+end_example

** Execute the Creation Script

#+begin_src bash
cat ~/test/support/create-net-projects.sh
#+end_src

#+begin_example
# #!/bin/bash
# 
# # We want to bail if an error occurs.
# set -e
# 
# 
# cd $(dirname $0)
# 
# 
# image=$(oc get deployments -n openshift-image-registry image-registry --template '{{ range $x := .spec.template.spec.containers }} {{- $x.image -}} {{ end }}')
# 
# # deploy the DC definition into the projects
# oc process -f netproj-template.yaml NAMESPACE=netproj-a IMAGE="$image" | oc apply -n netproj-a -f -
# oc process -f netproj-template.yaml NAMESPACE=netproj-b IMAGE="$image" | oc apply -n netproj-b -f -
#+end_example

#+begin_src bash
bash ~/test/support/create-net-projects.sh
#+end_src

#+begin_example
# namespace/netproj-a created
# deploymentconfig.apps.openshift.io/ose created
# namespace/netproj-b created
# deploymentconfig.apps.openshift.io/ose created
#+end_example

*** Examine the created infrastructure

#+begin_src bash
oc get pods -n netproj-a
#+end_src

#+begin_example
# NAME           READY   STATUS      RESTARTS   AGE
# ose-1-deploy   0/1     Completed   0          33s
# ose-1-r2rml    1/1     Running     0          24s
#+end_example

#+begin_src bash
oc get pods -n netproj-b
#+end_src

#+begin_example
# NAME           READY   STATUS      RESTARTS   AGE
# ose-1-54mtk    1/1     Running     0          57s
# ose-1-deploy   0/1     Completed   0          67s
#+end_example

*** Test Connectivity (should work)

#+begin_src bash
cat ~/test/support/test-connectivity.sh
#+end_src

#+begin_example
# #!/bin/bash
# 
# normal_color=$(echo -e "\e[0m")
# red_color=$(echo -e "\e[31m")
# green_color=$(echo -e "\e[32m")
# max_tries=3
# sleep_between_tries=1
# 
# 
# echo -n "Getting Pod B's IP... "
# pod_b_ip=$(oc get pod -n netproj-b $(oc get pod -n netproj-b | grep -v deploy | awk '/ose-/ {print $1}') -o jsonpath='{.status.podIP}{"\n"}')
# echo $pod_b_ip
# 
# 
# echo -n "Getting Pod A's Name... "
# pod_a_name=$(oc get pod -n netproj-a | grep -v deploy | awk '/ose-/ {print $1}')
# echo $pod_a_name
# 
# 
# echo -n "Checking connectivity between Pod A and Pod B..."
# 
# 
# i=1
# while [ $i -le ${max_tries} ]; do
#   if [ $i -gt 1 ]; then
#     # Don't sleep on first loop
#     echo -n "."
#     sleep ${sleep_between_tries}
#   fi
# 
#   if oc exec -n netproj-a $pod_a_name -- timeout 2 bash -c "</dev/tcp/$pod_b_ip/5000" 2>/dev/null ; then
#     break
#   fi
# 
#   i=$((i + 1))
# done
# 
# if [ $i -ge ${max_tries} ] ; then
#   # Failed the maximum amount of times.
#   echo " ${red_color}FAILED!${normal_color}"
#   exit 1
# else
#   echo " ${green_color}worked${normal_color}"
# fi
# 
#+end_example

#+begin_src bash
bash ~/test/support/test-connectivity.sh
#+end_src

#+begin_example
# Getting Pod B's IP... 10.130.6.12
# Getting Pod A's Name... ose-1-r2rml
# Checking connectivity between Pod A and Pod B... worked
#+end_example

*** Restricting Access

#+begin_src bash
cat ~/test/support/network-policy-block-all.yaml
#+end_src

#+begin_example
# # deny everything into the project
# # even resources in project can't reach each other
# ---
# kind: NetworkPolicy
# apiVersion: networking.k8s.io/v1
# metadata:
#   name: deny-by-default
# spec:
#   podSelector:
#   ingress: []
#+end_example

#+begin_src bash
oc create -n netproj-b -f ~/test/support/network-policy-block-all.yaml
#+end_src

#+begin_example
# networkpolicy.networking.k8s.io/deny-by-default created
#+end_example

*** Test Connectivity #2 (should fail)

#+begin_src bash
bash ~/test/support/test-connectivity.sh
#+end_src

#+begin_example
# Getting Pod B's IP... 10.130.6.12
# Getting Pod A's Name... ose-1-r2rml
# Checking connectivity between Pod A and Pod B..... FAILED!
#+end_example

*** Allow Access

#+begin_src bash
cat ~/test/support/network-policy-allow-all-from-netproj-a.yaml
#+end_src

#+begin_example
# # allow access to TCP port 5000 for pods with the label "run: ose" specifically
# # from projects with the label "name: netproj-a".
# ---
# kind: NetworkPolicy
# apiVersion: networking.k8s.io/v1
# metadata:
#   name: allow-tcp-5000-from-netproj-a-namespace
# spec:
#   podSelector:
#     matchLabels:
#       run: ose
#   ingress:
#   - ports:
#     - protocol: TCP
#       port: 5000
#     from:
#     - namespaceSelector:
#         matchLabels:
#           name: netproj-a
#+end_example

#+begin_src bash
oc create -n netproj-b -f ~/test/support/network-policy-allow-all-from-netproj-a.yaml
#+end_src

#+begin_example
# networkpolicy.networking.k8s.io/allow-tcp-5000-from-netproj-a-namespace created
#+end_example

*** Test Connectivity #3 (should work again)

#+begin_src bash
bash ~/test/support/test-connectivity.sh
#+end_src

#+begin_example
# Getting Pod B's IP... 10.130.6.12
# Getting Pod A's Name... ose-1-r2rml
# Checking connectivity between Pod A and Pod B... worked
#+end_example

** Disabling Project Self-Provisioning

*** Examine the Clusterrolebinding

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 64 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "default".
#+end_example

#+begin_src bash
oc describe clusterrolebinding.rbac self-provisioners
#+end_src

#+begin_example
# Name:         self-provisioners
# Labels:       <none>
# Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
# Role:
#   Kind:  ClusterRole
#   Name:  self-provisioner
# Subjects:
#   Kind   Name                        Namespace
#   ----   ----                        ---------
#   Group  system:authenticated:oauth  
#+end_example

#+begin_src bash
oc get clusterrolebinding.rbac self-provisioners -o yaml
#+end_src

#+begin_example
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   annotations:
#     rbac.authorization.kubernetes.io/autoupdate: "true"
#   creationTimestamp: "2020-02-19T18:01:12Z"
#   name: self-provisioners
#   resourceVersion: "5664"
#   selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/self-provisioners
#   uid: d08ca25d-5341-11ea-857f-06399a073c9e
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: self-provisioner
# subjects:
# - apiGroup: rbac.authorization.k8s.io
#   kind: Group
#   name: system:authenticated:oauth
#+end_example

*** Removing Self Provisioning Projects

#+begin_src bash
oc patch clusterrolebinding.rbac self-provisioners -p '{"subjects": null}'
#+end_src

#+begin_example
# clusterrolebinding.rbac.authorization.k8s.io/self-provisioners patched
#+end_example

#+begin_src bash
oc patch clusterrolebinding.rbac self-provisioners -p '{ "metadata": { "annotations": { "rbac.authorization.kubernetes.io/autoupdate": "false" } } }'
#+end_src

#+begin_example
# clusterrolebinding.rbac.authorization.k8s.io/self-provisioners patched
#+end_example

#+begin_src bash
oc get clusterrolebinding.rbac self-provisioners -o yaml
#+end_src

#+begin_example
# apiVersion: rbac.authorization.k8s.io/v1
# kind: ClusterRoleBinding
# metadata:
#   annotations:
#     rbac.authorization.kubernetes.io/autoupdate: "false"
#   creationTimestamp: "2020-02-19T18:01:12Z"
#   name: self-provisioners
#   resourceVersion: "889255"
#   selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/self-provisioners
#   uid: d08ca25d-5341-11ea-857f-06399a073c9e
# roleRef:
#   apiGroup: rbac.authorization.k8s.io
#   kind: ClusterRole
#   name: self-provisioner
#+end_example

#+begin_src bash
oc login -u fancyuser1 -p openshift
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 64 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "default".
#+end_example

#+begin_src bash
oc new-project fancyuserproject
#+end_src

#+begin_example
# Error from server (Forbidden): You may not request a new project via this API.
#+end_example

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 64 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "default".
#+end_example

*** Customizing the request message

#+begin_src bash
oc patch --type=merge project.config.openshift.io cluster -p '{"spec":{"projectRequestMessage":"Please visit https://ticket.example.com to request a project"}}'
#+end_src

#+begin_example
# project.config.openshift.io/cluster patched
#+end_example

#+begin_src bash
oc login -u fancyuser1 -p openshift
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 64 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "default".
#+end_example

#+begin_src bash
oc new-project fancyuserproject
#+end_src

#+begin_example
# Error from server (Forbidden): Please visit https://ticket.example.com to request a project
#+end_example

*** Clean Up

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 64 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "default".
#+end_example

#+begin_src bash
oc patch clusterrolebinding.rbac self-provisioners -p '{"subjects":[{"apiGroup":"rbac.authorization.k8s.io","kind":"Group","name":"system:authenticated:oauth"}]}'
oc patch clusterrolebinding.rbac self-provisioners -p '{"metadata":{"annotations":{"rbac.authorization.kubernetes.io/autoupdate":"true"}}}'
oc patch --type=json project.config.openshift.io cluster -p '[{"op": "remove", "path": "/spec/projectRequestMessage"}]'
#+end_src

#+begin_example
# clusterrolebinding.rbac.authorization.k8s.io/self-provisioners patched
# clusterrolebinding.rbac.authorization.k8s.io/self-provisioners patched
# clusterrolebinding.rbac.authorization.k8s.io/self-provisioners patched
# clusterrolebinding.rbac.authorization.k8s.io/self-provisioners patched
# project.config.openshift.io/cluster patched project.config.openshift.io/cluster patched
#+end_example

** Cluster Resource Quotas
*** Setting quota per user

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 64 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "default".
#+end_example

#+begin_src bash
oc create clusterquota for-user-normaluser1 \
    --project-annotation-selector openshift.io/requester=normaluser1 \
    --hard pods=10
#+end_src

#+begin_example
# clusterresourcequota.quota.openshift.io/for-user-normaluser1 created
#+end_example


#+begin_src bash
oc get clusterresourcequotas for-user-normaluser1 -o yaml
#+end_src

#+begin_example
# apiVersion: quota.openshift.io/v1
# kind: ClusterResourceQuota
# metadata:
#   creationTimestamp: "2020-02-21T15:10:35Z"
#   generation: 1
#   name: for-user-normaluser1
#   resourceVersion: "893453"
#   selfLink: /apis/quota.openshift.io/v1/clusterresourcequotas/for-user-normaluser1
#   uid: 4f85f5b1-54bc-11ea-99d2-0a84b9320ef6
# spec:
#   quota:
#     hard:
#       pods: "10"
#   selector:
#     annotations:
#       openshift.io/requester: normaluser1
#     labels: null
#+end_example

#+begin_src bash
oc login -u normaluser1 -p openshift
#+end_src

#+begin_example
# Login successful.
# 
# You don't have any projects. You can try to create a new project, by running
# 
#     oc new-project <projectname>
# 
#+end_example

#+begin_src bash
oc get projects
#+end_src

#+begin_example
# No resources found.
#+end_example

#+begin_src bash
oc new-project welcome1
oc new-project welcome2
#+end_src

#+begin_example
# Now using project "welcome1" on server "https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443".
# 
# You can add applications to this project with the 'new-app' command. For example, try:
# 
#     oc new-app django-psql-example
# 
# to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:
# 
#     kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
# 
# [lab-user@clientvm 0 ~/test master|✚1…6]$ Now using project "welcome2" on server "https://api.cluster-munich-e7ab.sandbox1596.opentlc.com:6443".
# 
# You can add applications to this project with the 'new-app' command. For example, try:
# 
#     oc new-app django-psql-example
# 
# to build a new example application in Python. Or use kubectl to deploy a simple Kubernetes application:
# 
#     kubectl create deployment hello-node --image=gcr.io/hello-minikube-zero-install/hello-node
# 
#+end_example

#+begin_src bash
oc new-app -n welcome1 --name=php1 quay.io/redhatworkshops/welcome-php:latest
oc new-app -n welcome2 --name=php2 quay.io/redhatworkshops/welcome-php:latest
#+end_src

#+begin_example
# --> Found container image a87cda2 (14 months old) from quay.io for "quay.io/redhatworkshops/welcome-php:latest"
# 
#     temp.builder.openshift.io/test/welcome-php-1:7d1b0ed6 
#     ----------------------------------------------------- 
#     PHP 7.1 available as container is a base platform for building and running various PHP 7.1 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.
# 
#     Tags: builder, php, php71, rh-php71
# 
#     * An image stream tag will be created as "php1:latest" that will track this image
#     * This image will be deployed in deployment config "php1"
#     * Ports 8080/tcp, 8443/tcp will be load balanced by service "php1"
#       * Other containers can access this service through the hostname "php1"
# 
# --> Creating resources ...
#     imagestream.image.openshift.io "php1" created
#     deploymentconfig.apps.openshift.io "php1" created
#     service "php1" created
# --> Success
#     Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
#      'oc expose svc/php1' 
#     Run 'oc status' to view your app.
# [lab-user@clientvm 0 ~/test master|✚1…6]$ --> Found container image a87cda2 (14 months old) from quay.io for "quay.io/redhatworkshops/welcome-php:latest"
# 
#     temp.builder.openshift.io/test/welcome-php-1:7d1b0ed6 
#     ----------------------------------------------------- 
#     PHP 7.1 available as container is a base platform for building and running various PHP 7.1 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.
# 
#     Tags: builder, php, php71, rh-php71
# 
#     * An image stream tag will be created as "php2:latest" that will track this image
#     * This image will be deployed in deployment config "php2"
#     * Ports 8080/tcp, 8443/tcp will be load balanced by service "php2"
#       * Other containers can access this service through the hostname "php2"
# 
# --> Creating resources ...
#     imagestream.image.openshift.io "php2" created
#     deploymentconfig.apps.openshift.io "php2" created
#     service "php2" created
# --> Success
#     Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
#      'oc expose svc/php2' 
#     Run 'oc status' to view your app.
#+end_example

#+begin_src bash
oc get pods -n welcome1 -l app=php1
oc get pods -n welcome2 -l app=php2
#+end_src

#+begin_example
# NAME           READY   STATUS              RESTARTS   AGE
# php1-1-x8s9v   0/1     ContainerCreating   0          5s
# NAME           READY   STATUS              RESTARTS   AGE
# php2-1-mf8n7   0/1     ContainerCreating   0          6s
#+end_example

#+begin_src bash
oc get pods -n welcome1 -l app=php1
oc get pods -n welcome2 -l app=php2
#+end_src

#+begin_example
# NAME           READY   STATUS    RESTARTS   AGE
# php1-1-x8s9v   1/1     Running   0          35s
# NAME           READY   STATUS    RESTARTS   AGE
# php2-1-mf8n7   1/1     Running   0          37s
#+end_example

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 66 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "welcome2".
#+end_example

#+begin_src bash
oc describe clusterresourcequotas for-user-normaluser1
#+end_src

#+begin_example
# Name:		for-user-normaluser1
# Created:	3 minutes ago
# Labels:		<none>
# Annotations:	<none>
# Namespace Selector: ["welcome1" "welcome2"]
# Label Selector: 
# AnnotationSelector: map[openshift.io/requester:normaluser1]
# Resource	Used	Hard
# --------	----	----
# pods		2	10
#+end_example

#+begin_src bash
oc get ns welcome1 -o yaml
#+end_src

#+begin_example
# apiVersion: v1
# kind: Namespace
# metadata:
#   annotations:
#     openshift.io/description: ""
#     openshift.io/display-name: ""
#     openshift.io/requester: normaluser1
#     openshift.io/sa.scc.mcs: s0:c28,c17
#     openshift.io/sa.scc.supplemental-groups: 1000790000/10000
#     openshift.io/sa.scc.uid-range: 1000790000/10000
#   creationTimestamp: "2020-02-21T15:11:53Z"
#   name: welcome1
#   resourceVersion: "894151"
#   selfLink: /api/v1/namespaces/welcome1
#   uid: 7e259394-54bc-11ea-9d6d-06399a073c9e
# spec:
#   finalizers:
#   - kubernetes
# status:
#   phase: Active
#+end_example

#+begin_src bash
oc login -u normaluser1 -p openshift
oc scale dc/php1 -n welcome1 --replicas=5
oc scale dc/php2 -n welcome2 --replicas=6
#+end_src

#+begin_example
# Login successful.
# 
# You have access to the following projects and can switch between them with 'oc project <projectname>':
# 
#     welcome1
#   * welcome2
# 
# Using project "welcome2".
# [lab-user@clientvm 0 ~/test master|✚1…6]$ deploymentconfig.apps.openshift.io/php1 scaled
# [lab-user@clientvm 0 ~/test master|✚1…6]$ deploymentconfig.apps.openshift.io/php2 scaled
#+end_example

#+begin_src bash
oc get pods --no-headers -n welcome1 -l app=php1 | wc -l
oc get pods --no-headers -n welcome2 -l app=php2 | wc -l
#+end_src

#+begin_example
# 5
# 5
#+end_example

#+begin_src bash
oc get events -n welcome1 | grep "Error creating" | head -1
oc get events -n welcome2 | grep "Error creating" | head -1
#+end_src

#+begin_example
# 52s         Warning   FailedCreate                  replicationcontroller/php2-1   Error creating: pods "php2-1-wdz6q" is forbidden: exceeded quota: for-user-normaluser1, requested: pods=1, used: pods=10, limited: pods=10
#+end_example

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 66 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "welcome2".
#+end_example

#+begin_src bash
oc describe clusterresourcequotas for-user-normaluser1
#+end_src

#+begin_example
# Name:		for-user-normaluser1
# Created:	7 minutes ago
# Labels:		<none>
# Annotations:	<none>
# Namespace Selector: ["welcome1" "welcome2"]
# Label Selector: 
# AnnotationSelector: map[openshift.io/requester:normaluser1]
# Resource	Used	Hard
# --------	----	----
# pods		10	10
#+end_example

*** Setting quota by label

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 66 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "welcome2".
#+end_example

#+begin_src bash
oc create clusterresourcequota for-pricelist \
    --project-label-selector=appstack=pricelist \
    --hard=pods=5
#+end_src

#+begin_example
# clusterresourcequota.quota.openshift.io/for-pricelist created
#+end_example

#+begin_src bash
oc adm new-project pricelist-frontend
oc adm new-project pricelist-backend
#+end_src

#+begin_example
# Created project pricelist-frontend
# [lab-user@clientvm 0 ~/test master|✚1…5]$ Created project pricelist-backend
#+end_example

#+begin_src bash
oc adm policy add-role-to-user edit normaluser1 -n pricelist-frontend
oc adm policy add-role-to-user edit normaluser1 -n pricelist-backend
#+end_src

#+begin_example
# clusterrole.rbac.authorization.k8s.io/edit added: "normaluser1"
# clusterrole.rbac.authorization.k8s.io/edit added: "normaluser1"
#+end_example

#+begin_src bash
oc label ns pricelist-frontend appstack=pricelist
oc label ns pricelist-backend appstack=pricelist
#+end_src

#+begin_example
# namespace/pricelist-frontend labeled
# namespace/pricelist-backend labeled
#+end_example

#+begin_src bash
oc describe clusterresourcequotas for-pricelist
#+end_src

#+begin_example
# Name:		for-pricelist
# Created:	About a minute ago
# Labels:		<none>
# Annotations:	<none>
# Namespace Selector: ["pricelist-frontend" "pricelist-backend"]
# Label Selector: appstack=pricelist
# AnnotationSelector: map[]
# Resource	Used	Hard
# --------	----	----
# pods		0	5
#+end_example

#+begin_src bash
oc login -u normaluser1 -p openshift
#+end_src

#+begin_example
# Login successful.
# 
# You have access to the following projects and can switch between them with 'oc project <projectname>':
# 
#     pricelist-backend
#     pricelist-frontend
#     welcome1
#   * welcome2
# 
# Using project "welcome2".
#+end_example

#+begin_src bash
oc new-app -n pricelist-frontend --name frontend quay.io/redhatworkshops/pricelist:frontend
oc new-app -n pricelist-backend --name backend quay.io/redhatworkshops/pricelist:backend
#+end_src

#+begin_example
# --> Found container image 0bf1412 (11 months old) from quay.io for "quay.io/redhatworkshops/pricelist:frontend"
# 
#     temp.builder.openshift.io/myproject/pricelist-allowed-1:2e531170 
#     ---------------------------------------------------------------- 
#     PHP 5.6 available as container is a base platform for building and running various PHP 5.6 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.
# 
#     Tags: builder, php, php56, rh-php56
# 
#     * An image stream tag will be created as "frontend:frontend" that will track this image
#     * This image will be deployed in deployment config "frontend"
#     * Ports 8080/tcp, 8443/tcp will be load balanced by service "frontend"
#       * Other containers can access this service through the hostname "frontend"
# 
# --> Creating resources ...
#     imagestream.image.openshift.io "frontend" created
#     deploymentconfig.apps.openshift.io "frontend" created
#     service "frontend" created
# --> Success
#     Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
#      'oc expose svc/frontend' 
#     Run 'oc status' to view your app.
# [lab-user@clientvm 0 ~/test master|✚1…6]$ --> Found container image 0bf1412 (11 months old) from quay.io for "quay.io/redhatworkshops/pricelist:backend"
# 
#     temp.builder.openshift.io/myproject/pricelist-allowed-1:2e531170 
#     ---------------------------------------------------------------- 
#     PHP 5.6 available as container is a base platform for building and running various PHP 5.6 applications and frameworks. PHP is an HTML-embedded scripting language. PHP attempts to make it easy for developers to write dynamically generated web pages. PHP also offers built-in database integration for several commercial and non-commercial database management systems, so writing a database-enabled webpage with PHP is fairly simple. The most common use of PHP coding is probably as a replacement for CGI scripts.
# 
#     Tags: builder, php, php56, rh-php56
# 
#     * An image stream tag will be created as "backend:backend" that will track this image
#     * This image will be deployed in deployment config "backend"
#     * Ports 8080/tcp, 8443/tcp will be load balanced by service "backend"
#       * Other containers can access this service through the hostname "backend"
# 
# --> Creating resources ...
#     imagestream.image.openshift.io "backend" created
#     deploymentconfig.apps.openshift.io "backend" created
#     service "backend" created
# --> Success
#     Application is not exposed. You can expose services to the outside world by executing one or more of the commands below:
#      'oc expose svc/backend' 
#     Run 'oc status' to view your app.
#+end_example

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 68 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "welcome2".
#+end_example

#+begin_src bash
oc describe clusterresourcequotas for-pricelist
#+end_src

#+begin_example
# Name:		for-pricelist
# Created:	2 minutes ago
# Labels:		<none>
# Annotations:	<none>
# Namespace Selector: ["pricelist-frontend" "pricelist-backend"]
# Label Selector: appstack=pricelist
# AnnotationSelector: map[]
# Resource	Used	Hard
# --------	----	----
# pods		4	5
#+end_example

!!! I see 4 instead of 2 --> see documentation

#+begin_src bash
oc login -u normaluser1 -p openshift
#+end_src

#+begin_example
# Login successful.
# 
# You have access to the following projects and can switch between them with 'oc project <projectname>':
# 
#     pricelist-backend
#     pricelist-frontend
#     welcome1
#   * welcome2
# 
# Using project "welcome2".
#+end_example

#+begin_src bash
oc scale -n pricelist-frontend dc/frontend --replicas=3
oc scale -n pricelist-backend dc/backend --replicas=3
#+end_src

#+begin_example
# deploymentconfig.apps.openshift.io/frontend scaled
#  deploymentconfig.apps.openshift.io/backend scaled
#+end_example

#+begin_src bash
oc get events -n pricelist-frontend | grep "Error creating" | head -1
oc get events -n pricelist-backend | grep "Error creating" | head -1
#+end_src

#+begin_example
# 21s         Warning   FailedCreate                  replicationcontroller/backend-1   Error creating: pods "backend-1-88ptp" is forbidden: exceeded quota: for-pricelist, requested: pods=1, used: pods=5, limited: pods=5
#+end_example

*** Clean Up

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 68 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "welcome2".
#+end_example

#+begin_src bash
oc delete clusterresourcequotas for-pricelist for-user-normaluser1
#+end_src

#+begin_example
# clusterresourcequota.quota.openshift.io "for-pricelist" deleted
# clusterresourcequota.quota.openshift.io "for-user-normaluser1" deleted
#+end_example

#+begin_src bash
oc delete projects pricelist-backend pricelist-frontend welcome1 welcome2
#+end_src

#+begin_example
# project.project.openshift.io "pricelist-backend" deleted
# project.project.openshift.io "pricelist-frontend" deleted
# project.project.openshift.io "welcome1" deleted
# project.project.openshift.io "welcome2" deleted
#+end_example

#+begin_src bash
oc login -u kubeadmin -p RmEAm-FN5EH-XhpAH-YYbf8
#+end_src

#+begin_example
# Login successful.
# 
# You have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects'
# 
# Using project "default".
#+end_example

** Cluster Metering

https://coreos.com/blog/introducing-operator-framework-metering

#+begin_src bash
oc projects | grep openshift-metering
#+end_src

*** Add Metering Operator to cluster

Set the name to openshift-metering. No other namespace is supported. Label the
namespace with openshift.io/cluster-monitoring=true, and click Create.

*** Install the Metering stack

#+begin_example
apiVersion: metering.openshift.io/v1
kind: MeteringConfig
metadata:
  name: "operator-metering"
spec:
  unsupportedFeatures:
    enableHDFS: true

  storage:
    type: "hive"
    hive:
      type: "hdfs"
      hdfs:
        # Leave this value as-is.
        namenode: "hdfs-namenode-0.hdfs-namenode:9820"
#+end_example

*** Verify the installation

#+begin_src bash
oc -n openshift-metering get pods
#+end_src

#+begin_example
# NAME                                READY   STATUS     RESTARTS   AGE
# hdfs-datanode-0                     0/1     Init:0/2   0          12s
# hdfs-namenode-0                     0/1     Init:0/1   0          11s
# metering-operator-b979d6896-c44hq   2/2     Running    0          6m53s
#+end_example

#+begin_src bash
oc -n openshift-metering get pods
#+end_src

#+begin_example
# NAME                                  READY   STATUS    RESTARTS   AGE
# hdfs-datanode-0                       1/1     Running   0          2m41s
# hdfs-namenode-0                       1/1     Running   0          2m40s
# hive-metastore-0                      1/2     Running   0          2m14s
# hive-server-0                         2/3     Running   0          2m14s
# metering-operator-b979d6896-c44hq     2/2     Running   0          9m22s
# presto-coordinator-0                  2/2     Running   0          108s
# reporting-operator-5fbb884699-pgxw2   1/2     Running   0          77s
#+end_example

#+begin_src bash
oc get reportdatasources -n openshift-metering | grep -v raw
#+end_src

#+begin_example
# NAME                                         EARLIEST METRIC   NEWEST METRIC   IMPORT START   IMPORT END   LAST IMPORT TIME   AGE
# node-allocatable-cpu-cores                                                                                                    63s
# node-allocatable-memory-bytes                                                                                                 63s
# node-capacity-cpu-cores                                                                                                       63s
# node-capacity-memory-bytes                                                                                                    63s
# persistentvolumeclaim-capacity-bytes                                                                                          63s
# persistentvolumeclaim-phase                                                                                                   63s
# persistentvolumeclaim-request-bytes                                                                                           63s
# persistentvolumeclaim-usage-bytes                                                                                             63s
# pod-limit-cpu-cores                                                                                                           63s
# pod-limit-memory-bytes                                                                                                        63s
# pod-persistentvolumeclaim-request-info                                                                                        63s
# pod-request-cpu-cores                                                                                                         63s
# pod-request-memory-bytes                                                                                                      63s
# pod-usage-cpu-cores                                                                                                           63s
# pod-usage-memory-bytes                                                                                                        63s
#+end_example

#+begin_src bash
oc get reportdatasources -n openshift-metering | grep -v raw
#+end_src

#+begin_example
# NAME                                         EARLIEST METRIC        NEWEST METRIC          IMPORT START           IMPORT END             LAST IMPORT TIME       AGE
# node-allocatable-cpu-cores                   2020-02-21T13:41:00Z   2020-02-21T14:16:00Z   2020-02-21T13:41:00Z   2020-02-21T14:16:00Z   2020-02-21T15:43:00Z   2m49s
# node-allocatable-memory-bytes                2020-02-21T13:42:00Z   2020-02-21T14:17:00Z   2020-02-21T13:42:00Z   2020-02-21T14:17:00Z   2020-02-21T15:43:00Z   2m49s
# node-capacity-cpu-cores                      2020-02-21T13:42:00Z   2020-02-21T14:23:00Z   2020-02-21T13:42:00Z   2020-02-21T14:23:00Z   2020-02-21T15:43:02Z   2m49s
# node-capacity-memory-bytes                   2020-02-21T13:42:00Z   2020-02-21T14:17:00Z   2020-02-21T13:42:00Z   2020-02-21T14:17:00Z   2020-02-21T15:42:58Z   2m49s
# persistentvolumeclaim-capacity-bytes         2020-02-21T13:42:00Z   2020-02-21T14:11:00Z   2020-02-21T13:42:00Z   2020-02-21T14:11:00Z   2020-02-21T15:42:52Z   2m49s
# persistentvolumeclaim-phase                  2020-02-21T13:42:00Z   2020-02-21T14:05:00Z   2020-02-21T13:42:00Z   2020-02-21T14:05:00Z   2020-02-21T15:42:50Z   2m49s
# persistentvolumeclaim-request-bytes          2020-02-21T13:41:00Z   2020-02-21T14:16:00Z   2020-02-21T13:41:00Z   2020-02-21T14:16:00Z   2020-02-21T15:42:54Z   2m49s
# persistentvolumeclaim-usage-bytes            2020-02-21T13:41:00Z   2020-02-21T14:10:00Z   2020-02-21T13:41:00Z   2020-02-21T14:10:00Z   2020-02-21T15:42:58Z   2m49s
# pod-limit-cpu-cores                          2020-02-21T13:41:00Z   2020-02-21T14:22:00Z   2020-02-21T13:41:00Z   2020-02-21T14:22:00Z   2020-02-21T15:42:51Z   2m49s
# pod-limit-memory-bytes                       2020-02-21T13:42:00Z   2020-02-21T14:11:00Z   2020-02-21T13:42:00Z   2020-02-21T14:11:00Z   2020-02-21T15:42:54Z   2m49s
# pod-persistentvolumeclaim-request-info       2020-02-21T13:41:00Z   2020-02-21T14:16:00Z   2020-02-21T13:41:00Z   2020-02-21T14:16:00Z   2020-02-21T15:42:55Z   2m49s
# pod-request-cpu-cores                        2020-02-21T13:42:00Z   2020-02-21T14:05:00Z   2020-02-21T13:42:00Z   2020-02-21T14:05:00Z   2020-02-21T15:42:54Z   2m49s
# pod-request-memory-bytes                     2020-02-21T13:41:00Z   2020-02-21T14:04:00Z   2020-02-21T13:41:00Z   2020-02-21T14:04:00Z   2020-02-21T15:42:50Z   2m49s
# pod-usage-cpu-cores                          2020-02-21T13:42:00Z   2020-02-21T14:05:00Z   2020-02-21T13:42:00Z   2020-02-21T14:05:00Z   2020-02-21T15:42:53Z   2m49s
# pod-usage-memory-bytes                       2020-02-21T13:42:00Z   2020-02-21T14:05:00Z   2020-02-21T13:42:00Z   2020-02-21T14:05:00Z   2020-02-21T15:42:57Z   2m49s
#+end_example

*** Writing Reports

#+begin_src bash
oc get reportqueries -n openshift-metering | grep -v raw
#+end_src

#+begin_example
# NAME                                         AGE
# cluster-cpu-capacity                         4m48s
# cluster-cpu-usage                            4m48s
# cluster-cpu-utilization                      4m48s
# cluster-memory-capacity                      4m48s
# cluster-memory-usage                         4m48s
# cluster-memory-utilization                   4m48s
# cluster-persistentvolumeclaim-request        4m47s
# namespace-cpu-request                        4m47s
# namespace-cpu-usage                          4m47s
# namespace-cpu-utilization                    4m47s
# namespace-memory-request                     4m47s
# namespace-memory-usage                       4m47s
# namespace-memory-utilization                 4m47s
# namespace-persistentvolumeclaim-request      4m47s
# namespace-persistentvolumeclaim-usage        4m47s
# node-cpu-allocatable                         4m47s
# node-cpu-capacity                            4m48s
# node-cpu-utilization                         4m47s
# node-memory-allocatable                      4m47s
# node-memory-capacity                         4m47s
# node-memory-utilization                      4m47s
# persistentvolumeclaim-capacity               4m47s
# persistentvolumeclaim-request                4m47s
# persistentvolumeclaim-usage                  4m47s
# pod-cpu-request                              4m47s
# pod-cpu-usage                                4m47s
# pod-memory-request                           4m47s
# pod-memory-usage                             4m47s
#+end_example

*** Create Report with a Schedule

#+begin_example
apiVersion: metering.openshift.io/v1
kind: Report
metadata:
  name: cluster-cpu-usage-hourly
spec:
  query: "cluster-cpu-usage"
  schedule:
    period: "hourly"
#+end_example


#+begin_src bash
oc get reports -n openshift-metering
#+end_src

#+begin_example
# NAME                       QUERY               SCHEDULE   RUNNING                  FAILED   LAST REPORT TIME   AGE
# cluster-cpu-usage-hourly   cluster-cpu-usage   hourly     ReportingPeriodWaiting                               12s
#+end_example

*** Create One-Time Report

#+begin_example
apiVersion: metering.openshift.io/v1
kind: Report
metadata:
  name: namespace-cpu-request-2020
  namespace: openshift-metering
spec:
  query: namespace-cpu-request
  reportingEnd: '2020-12-30T23:59:59Z'
  reportingStart: '2020-01-01T00:00:00Z'
  runImmediately: true
#+end_example

#+begin_src bash
oc get reports -n openshift-metering
#+end_src

#+begin_example
# NAME                         QUERY                   SCHEDULE   RUNNING                  FAILED   LAST REPORT TIME   AGE
# cluster-cpu-usage-hourly     cluster-cpu-usage       hourly     ReportingPeriodWaiting                               109s
# namespace-cpu-request-2020   namespace-cpu-request              RunImmediately                                       21s
#+end_example
#+begin_src bash
oc get reports -n openshift-metering
#+end_src

